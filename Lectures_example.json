[
  {
    "index": 1,
    "level": 1,
    "start_page": 1,
    "end_page": 52,
    "title": "1. Introduction to Neural Networks",
    "content": "Neural Networks Charith Chitraranjan Outline • Brief look at biological neurons and their networks • Basic artificial neurons and neural networks • Why study neural networks • A simple pattern recognition example • Perceptron and Learning • Performance of neural networks in popular machine learning problems Neurons and Their Networks • What is a neural network? - A network of interconnected nerve cells called neurons. • The nervous systems (especially the brains) of animals are composed of neural networks. • Adult human brain has about a trillion neurons Biological Neuron • What is a neuron? - The basic unit of a nervous system. It's a type of cell. • What does a neuron do? - Receive input from other neurons at the synapses and if required conditions are satisfied, produce an output (a spike train) through the axon. Biological Neuron A neuron receives electrical signals through the axons terminals of other neurons. These signals have to cross a gap (called the synaptic cleft) at the synapses before entering the dendrites that carry the signal into the cell body (soma). What portion of the signal passes onto the dendrites will depend on the strength of the particular synapse. In modeling neurons, the input signal at each synapse is multiplied by its strength (weight ), w. Therefore, the resultant signal received by the cell body is the weighted sum of the pre-synaptic signals. If the signal present at the cell body is large enough to cause sufficient depolarization of the neural membrane, the neuron will fire and a voltage spike called an action potential will be transmitted through the output axon. There are two types of synapses; excitatory and inhibitory. Excitatory synapses cause membrane depolarization and contribute to fire the neuron. Such synapses are modeled with positive weights Inhibitory synapses cause membrane hyperpolarization and inhibit the firing of the neuron. Such synapses are modeled with negative weights Biological Neuron A neuron receives electrical signals through the axons terminals of other neurons. These signals have to cross a gap (called the synaptic cleft) at the synapses before entering the dendrites that carry the signal into the cell body (soma). What portion of the signal passes onto the dendrites will depend on the strength of the particular synapse. In modeling neurons, the input signal at each synapse is multiplied by its strength (weight ), w. Therefore, the resultant signal received by the cell body is the weighted sum of the pre-synaptic signals. If the signal present at the cell body is large enough to cause sufficient depolarization of the neural membrane, the neuron will fire and a voltage spike called an action potential will be transmitted through the output axon. There are two types of synapses; excitatory and inhibitory. Excitatory synapses cause membrane depolarization and contribute to fire the neuron. Such synapses are modeled with positive weights Inhibitory synapses cause membrane hyperpolarization and inhibit the firing of the neuron. Such synapses are modeled with negative weights Models of Artificial Neurons • Binary threshold Neurons • Rectified linear Neurons (aka Linear threshold neurons) • Sigmoid Neurons Binary Threshold Neuron (McCulloch-Pitts Model) In a network, a neuron typically receives input from many other neurons and its output may be fed into many other neurons as well. A very simple model of a neuron. 𝑂𝑢𝑡𝑝𝑢𝑡= ൝ 𝑖𝑓𝑧≥ 𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒 𝑧= ෍ 𝑥𝑖𝑤𝑖 Simple Artificial Neuron: Rectified Linear Neuron otherwise Output f(z) Simple Artificial Neuron: Sigmoid Neuron These use the sigmoid (logistic) function to produce the output. It is a continuous output function with nice derivatives that make learning easier. output 0.5 A Simple Artificial Neural Network Source: http://mechanicalforex.com/2011/06/neural-networks-in-trading-how- to-design-a-network-for-financial-forecasting-in-eight-simple-steps.html A simple feed-forward neural network with one hidden layer Why study/use neural networks • To help understand how the brain works - Mammalian brains are complex and contain trillions of neurons. Thus difficult to understand. • Simplified artificial neural networks can be used to solve practical machine learning problems. E.g., - Visual object recognition - Recognition of hand-written digits - Speech recognition - And other problems in the general areas of classification, prediction and clustering. A simple Pattern Recognition Example • Assume a 3x3 dot-matrix display as shown below that can display several patterns including several English characters such as, • Suppose out of everything it can display, we want to recognize the characters T and L. • Suppose we mark each cell as xi for i=1, 2,...,9 as follows with xi = 1 if the cell is shaded and 0 otherwise. • We can use the following artificial neural network to solve this problem. Weights on the edges are the multiplicative synaptic weights Number within each neuron is its spiking threshold. In this case, all are 1 and they are binary threshold neurons. 0.2 0.2 0.2 0.2 0.2 Outputs 1 for input T Outputs 1 for input L 0.25 0.25 0.25 0.25 Perceptron Output Perceptron Output and where Perceptron Output and where Perceptron Learning • In supervised learning, we want a perceptron to learn to separate data into two classes. • Let P ={x(1),..., x(L)} and N ={x(1),..., x(M)} be two classes (sets) of data that we want the perceptron to learn to separate. - Also assume that the data is linearly separable. E.g., 2D and 3D data shown below. Perceptron Learning • We want to find a set of weights that satisfy the following. • When the classes P and N are linearly separable, the perceptron leaning algorithm can be used to learn weights that satisfy the above. For 𝑛-dim 𝑒𝑛𝑠𝑖𝑜𝑛𝑎𝑙 data, ∀𝑥(𝑘) ∈𝑃, ෍ 𝑖=0 𝑤𝑖𝑥𝑖 (𝑘) ≥0 𝑎𝑛𝑑 ∀𝑥(𝑘) ∈𝑁, ෍ 𝑖=0 𝑤𝑖𝑥𝑖 (𝑘) < 0 Perceptron Learning Algorithm In this algorithm, - if a data point in P is misclassified, the weight vector is rotated towards that data point. - if a data point in N is misclassified, the weight vector is rotated away from that data point. If the data is linearly separable, the algorithm will find a weight vector that produces zero error in a finite number of iterations. chosen randomly each For randomly Initializw and and w=[w Perceptron Learning Algorithm 0 + w  Suppose 𝒙(k) is in the positive class but is misclassified as negative (under the separating line) Perceptron Learning Algorithm 0 + w  Suppose 𝒙(k) is in the negative class but is misclassified as positive (above the separating line) The pocket Algorithm • If the data is not linearly separable, the perceptron leaning algorithm does not converge to a solution. • In such cases, we want to find a weight vector ws that minimizes the number of misclassified data points. • A variant of the perceptron learning algorithm, known as the pocket algorithm produces an approximation to ws. - It uses the perceptron learning algorithm to update the weight vector - At any given time, it attempts to keep the best* weight vector found so far and updates it if a better one is found. * One that produces the least number of misclassified data points in the last run. The pocket Algorithm Reference: Raul Rojas, Neural Networks: A Systematic Introduction, Springer-Verlag, 1996. Performance of Artificial Neural Networks on Machine Learning Problems • ImageNet: Object Recognition - 1000 different object classes in 1.3 million high- resolution images from the web. Task is to correctly identify these objects. - Best system in the 2010 competition got 47% error for its first choice and 25% error for its top 5 choices. - A very deep neural net (Krizhevsky et. al. 2012) got less that 40% error for its first choice and less than 20% for its top 5 choices Source: Based on the Slides Prepared By: Geoffrey Hinton, Nitish Srivastava, Kevin Swersky. https://class.coursera.org/neuralnets-2012-001/class/index Some examples from an earlier version of the net Source: Based on the Slides Prepared By: Geoffrey Hinton, Nitish Srivastava, Kevin Swersky. https://class.coursera.org/neuralnets-2012-001/class/index Performance of Artificial Neural Networks on Machine Learning Problems Speech Recognition • A speech recognition system has several stages: - Pre-processing: Convert the sound wave into a vector of acoustic coefficients. Extract a new vector about every 10 milli- seconds. - The acoustic model: Use a few adjacent vectors of acoustic coefficients to place bets on which part of which phoneme is being spoken. - Decoding: Find the sequence of bets that does the best job of fitting the acoustic data and also fitting a model of the kinds of things people typically say. • Deep neural networks pioneered by George Dahl and Abdel-rahman Mohamed are now replacing the previous machine learning method for the acoustic model. Source: Based on the Slides Prepared By: Geoffrey Hinton, Nitish Srivastava, Kevin Swersky. https://class.coursera.org/neuralnets-2012-001/class/index Word error rates from MSR, IBM,. & Google (Hinton et. al. IEEE Signal Processing Magazine, Nov 2012) The task Hours of training data Deep neural network Gaussian Mixture Model GMM with more data Switchboard (Microsoft Research) 18.5% 27.4% 18.6% (2000 hrs) English broadcast news (IBM) 17.5% 18.8% Google voice search (android 4.1) 5,870 12.3% (and falling) 16.0% (>>5,870 hrs) References • Raul Rojas, Neural Networks: A Systematic Introduction, Springer-Verlag, 1996. • Ian Goodfellow, Yoshua Bengio, Aaron Courville: Deep Learning, MIT Press, 2016. Back Propagation for Supervised Learning with Neural Networks Charith Chitraranjan Outline • A simple multilayer feed forward neural network. • An objective for learning the weights • Gradient decent • Back propagation: An example • Modular approach to back propagation A Feed Forward Neural Network We need to find appropriate values for the weights w1, ..., w14 so that the outputs z1 and z2 behave desirably. Each neuron computes h(wx). I.e., h(weighted sum of its inputs). h is some transfer function. E.g., the sigmoid function w1 x w10 w14 Adjusting the Weights Consider the same network in the previous slide. Assume we have a set of labeled training examples (Xi,di 1,di 2) with i=1,..., N. - Xi is the input vector and di 1, di 2 are the actual(desired) outputs for the ith example. Table below shows part of the Iris data as an example ( only 3 out of the 4 features in the original data set). Data point Input (X) Desired output Sepal Length (x1) Sepal Width (x2) Petal Length (x3) Setosa (d1) Virginica(d2) 4.9 1.4 4.6 3.1 1.5 4.7 3.2 1.3 5.4 3.9 1.7 4.6 3.4 1.4 Adjusting the Weights • Consider the same example as before. • We can compute a prediction error (E) that depends on the mismatch between the desired outputs di 1,di 2 and predicted outputs 1,zi - E.g., Squared difference • We can adjust the weights such that E reaches its minimum. Adjusting the Weights • Eventually, error (E) is a function of the weights and the inputs. - Why not just express E, in terms of w's and x's and minimize it ? • For a network of decent size and non-linear transfer functions, expressing E analytically in terms of weights and minimizing it becomes tedious and almost impossible. • Neural networks with just input and output layers (no hidden layers) are very limited in what they can do and they need carefully crafted features, which are tedious to get. • Neural networks with many hidden layers (deep neural nets), can learn features without much human intervention. • So, we use numerical methods to adjust the weights. - Gradient decent is a popular choice Gradient Decent E = 1 w E = • Let the current values be w1=a and w2=b. • We want to reduce the error. So, we need to change w1 and w2 along some direction that has a negative gradient (down hill). Gradient Decent Gradient Decent E = 1 w E = • Let the current values be w1=a and w2=b. • We want to reduce the error. So, we need to change w1 and w2 along some direction that has a negative gradient (down hill). • is the direction of the maximum gradient. So, we move in the opposite direction specified by . Gradient Decent E = 1 w E = • Let the current values be w1=a and w2=b. • We want to reduce the error. So, we need to change w1 and w2 along some direction that has a negative gradient (down hill). • is the direction of the maximum gradient. So, we move in the opposite direction specified by . Gradient Decent: Update step • Let f(w1,...wj ,...wN) be a function of N variables. • Then, in the kth iteration, each variable wj will be updated as follows ,..., ,..., point at the evaluated w.r.t derivative partial the and rate learning the where Gradient Decent: Update step • Let f(w1,...wj ,...wN) be a function of N variables. • Then, in the kth iteration, each variable wj will be updated as follows ,..., ,..., point at the evaluated w.r.t derivative partial the and rate learning the where Gradient Decent • To perform gradient decent, we need the partial derivatives of the error w.r.t. weights. I.e., • We find these partial derivatives using the back propagation algorithm. - Propagate the error from the output layer back to the input layer through the hidden layers. Chain Rule of Differentiation Back propagation relies on the chain rule. • Let z be a function of the two variables u and v - I.e., • Let u and v each be a function of the two variables x and y. - I.e., • Then from the chain rule, the partial derivatives of z w.r.t x and y are, z = Back Propagation Error fn • Suppose we have a labeled training dataset containing N entries of the form with i=1,..., N, where x1 and x2 are the input attributes and d is the actual/desired output. • In order to do a gradient decent to find the weights such that the predicted outputs z2's are as close as possible to the actual outputs d's, we need the following derivatives. Neuron 1 Neuron 2 Back Propagation )1( Back Propagation (i) (i) (i) and output output (1) and Substitute is, neuron sigmoid derivative the general i.e., threshold zero and activation sigmoid have neurons the Assume Back Propagation Similarly Back Propagation: A Modular Approach lz1 lz2 l th layer We can assume that the last module computes the error. Then, ZL+1=E. Therefore message forward the Is the backward message Same Example in Modular Form 4 = 𝑓3(𝑧1 3) = 𝐸= 1 2 𝑧1 3 -𝑑2 Module 1: f1 4 = 𝐸 Module 2: f2 Module 3 (error) 𝑙= 𝜕𝐸 𝜕𝑍𝑗 4 = 𝜕𝐸 𝜕𝑍1 4 = 𝜕𝐸 𝜕𝐸= 1 𝑙= ෍ 𝑙+1 𝜕𝑍𝑘 𝜕𝑍𝑗 3 = 𝛿1 4 𝜕𝑍1 𝜕𝑍1 3 = 𝑧1 3 -𝑑 2 = 𝛿1 3 𝜕𝑍1 𝜕𝑍1 2 = 𝑧1 3 -𝑑𝑍1 3(1 -𝑍1 3)𝑤1 1 = 𝛿1 2 𝜕𝑍1 𝜕𝑍1 = 𝑧1 3 -𝑑𝑍1 3(1 -𝑍1 3)𝑤1 2𝑍1 2 1 -𝑍1 2 𝑤1 1 = 𝛿1 2 𝜕𝑍1 𝜕𝑍2 = 𝑧1 3 -𝑑𝑍1 3(1 -𝑍1 3)𝑤1 2𝑍1 2(1 -𝑍1 2)𝑤2 Same Example in Modular Form... 𝜕𝑤12 = 𝛿1 3 𝜕𝑍13 𝜕𝑤12 = 𝑧1 3 -𝑑 𝑍1 3(1 -𝑍1 3) 𝑧1 Module 1: f1 4 = 𝐸 Module 2: f2 Module 3 (error) 𝜕𝑤𝑚 𝑙= ෍ 𝑙+1 𝜕𝑍𝑘 𝑙+1 𝜕𝑤𝑚 𝜕𝑤11 = 𝛿1 2 𝜕𝑍12 𝜕𝑤11= 𝑧1 3 -𝑑𝑍1 3(1 -𝑍1 3)𝑤1 2 𝑍1 2 1 -𝑍1 2 𝑧1 𝜕𝑤21 = 𝛿1 2 𝜕𝑍12 𝜕𝑤21= 𝑧1 3 -𝑑𝑍1 3(1 -𝑍1 3)𝑤1 2 𝑍1 2 1 -𝑍1 2 𝑧2 𝐸= 1 2 ෍ 𝑖=1 (𝑖) -𝑑(𝑖) 𝜕𝑤1 = ෍ 𝑖=1 𝜕𝑧2 (𝑖) 𝜕𝑧2 (𝑖) 𝜕𝑤1 = ෍ 𝑖=1 (𝑖) -𝑑(𝑖) 𝜕𝑧2 (𝑖) 𝜕𝑤1 𝜕𝑤1 = ෍ 𝑖=1 (𝑖) -𝑑(𝑖) 𝜕𝑧2 (𝑖) 𝜕𝑦2 (𝑖) 𝜕𝑦2 (𝑖) 𝜕𝑤1 = ෍ 𝑖=1 (𝑖) -𝑑(𝑖) 𝜕𝑧2 (𝑖) 𝜕𝑦2 (𝑖) 𝜕𝑦2 (𝑖) 𝜕𝑧1 (𝑖) 𝜕𝑧1 (𝑖) 𝜕𝑤1 𝜕𝑤1 = ෍ 𝑖=1 (𝑖) -𝑑(𝑖) 𝜕𝑧2 (𝑖) 𝜕𝑦2 (𝑖) 𝜕𝑦2 (𝑖) 𝜕𝑧1 (𝑖) 𝜕𝑧1 (𝑖) 𝜕𝑦1 (𝑖) 𝜕𝑦1 (𝑖) 𝜕𝑤1 𝜕𝑤1 = ෍ 𝑖=1 (𝑖) -𝑑(𝑖) 𝑧2 (𝑖)(1 -𝑧2 (𝑖))𝑤3𝑧1 (𝑖)(1 -𝑧1 (𝑖))𝑥1 (𝑖) 𝑆𝑖𝑚𝑖𝑙𝑎𝑟𝑙𝑦, 𝜕𝐸 𝜕𝑤2 = ෍ 𝑖=1 (𝑖) -𝑑(𝑖) 𝑧2 (𝑖)(1 -𝑧2 (𝑖))𝑤3𝑧1 (𝑖)(1 -𝑧1 (𝑖))𝑥2 (𝑖) Useful Links • https://www.youtube.com/watch?v=q0pm3Br IUFo • https://www.youtube.com/watch?v=- YRB0eFxeQA&list=PLE6Wd9FR-- EfW8dtjAuPoTuPcqmOV53Fu&index=10"
  },
  {
    "index": 2,
    "level": 1,
    "start_page": 53,
    "end_page": 70,
    "title": "2. Loss Functions Regularization",
    "content": "Loss Functions and Regularization Charith Chitraranjan Outline • Error or loss functions for classification problems • Regularization Error Functions: Binary Classification • Two classes (0,1) in the data. • Let tn be the target value (actual class) for the nth data sample. - tn is either 1 or 0 Let the prediction of the network for the nth data sample, denoted yn, represent the probability of class label 1 given the inputs of sample n, i.e, our objective is to make yn = P(tn =1|inputsn) Inputs Output (y) Error Functions: Binary Classification • If the output neuron has a sigmoid activation function, ▪ . ▪it produces output values between 0 and 1. So, qualifies to be a probability • We train the model such that yn gets as close as possible to the desired probability P(tn =1|inputsn) • We need to specify a suitable error/loss function and adjust the weights of the network to minimize that function. exp( Binary Classification: Squared Error (SE sample error Squared • This is simple, but doesn't work very well in practice • It is susceptible to outliers Red and blue represent two classes of points Binary Classification: Cross Entropy with Logistic Loss Cross Entropy between two probability distributions p and q are defined as below - H(p,q) is low when p and q are similar and is high when p and q are dissimilar. E.g., let X be a random variable representing the outcome of a coin toss. So, the possible values are X=H and X=T. Suppose there are two coins. One is fair and the other is biased, and let p and q be their probability distribution functions respectively with the following values. If both coins were fair, log[ )3.0 log 5.0 7.0 log 5.0 log( log( 3.0 ,7.0 5.0 then 5.0 log 5.0 5.0 log 5.0 5.0 then Binary Classification: Cross Entropy with Logistic Loss • Cross Entropy between two probability distributions p and q are defined as below - H(p,q) is low when p and q are similar and is high when p and q are dissimilar. • In our case we'll let p be the distribution of the actual class label (t) and q be that of the predicted class label (y) log[ log( log( (CEL sample for logistic entropy wi cross Binary Classification: Cross Entropy with Logistic Loss Line of separation with cross-entropy logistic loss Line of separation with squared error Multi-Class Classification: Cross Entropy with Logistic Loss If the classes are independent, Inputs log( log( samples data independen for CEL log( log( (CEL sample for activation logistic entropy wi cross Multi-Class Classification: Cross Entropy with Softmax • If a given sample should belong to one and only one class, then we would have to add a softmax layer Softmax Layer exp( exp( Multi-Class Classification: Cross Entropy with Softmax log( samples data independen for activation Softmax entropy wi Cross Regularization • Any strategy that makes a model perform better not just on training data but also on previously unseen test data. - Reducing the generalization error instead of training error. - Avoid overfitting (and underfitting too) Image source: www.geeksforgeeks.org Appropriate fit 𝜃0 + 𝜃1𝑥+ 𝜃2𝑥2 + 𝜃3𝑥3 +𝜃4 𝑥4 Regularization - Fitting for Classification Image source: www.geeksforgeeks.org Regularization • Let J be the error to be minimized and is a function of parameters (weights) w. - without reqularization we just try to minimize J(w) - with regularization, we try to minimize J(w)+λΩ(w) where λ is a constant and Ω(w) is a parameter norm penalty. E.g. Lp norm. Regularization - Exampls of Norm Penanlties • L2 norm Minimize J(w)+λΣ(wi)2 E.g., for linear regression Regularization gives numerical stability to linear regression when the data is poorly conditioned (many dimension and few data points) contours contours λΣ(wi)2 is the penalty here Regularization - Exampls of Norm Penanlties • L1 norm Minimize J(w)+λΣ|wi| E.g., for linear regression contours L2 Vs L1 Regularization • Both L2 and L1 regularization both try to make the model simpler by bringing some parameters (not so important ones) closer to • L1 regularization brings parameters to zero more aggressively making the model sparser by completely eliminating (zero) unimportant model parameters. Some useful Video Lectures • https://youtu.be/hrIad1RVFV0?list=PLE6Wd9F R--EdyJ5lbFl8UuGjecvVw66F6&t=2116 • https://www.youtube.com/watch?v=PKXpaLUi gA8 • https://www.youtube.com/watch?v=KvtGD37 Rm5I&list=PLLssT5z_DsK- h9vYZkQkYNWcItqhlRJLN&index=40"
  },
  {
    "index": 3,
    "level": 1,
    "start_page": 71,
    "end_page": 93,
    "title": "3. Convolutional Neural Nets",
    "content": "Convolutional Neural Networks Charith Chitraranjan Outline • Convolutional Neural Networks for Image Data - Convolutional layer - Pooling layer • Why use convolutional nets • Derivatives for back propagation Convolutional Layer For an Image • Basic Idea: You take an image and slide a filter over it. - The filter computes a weighted sum of the pixels it covers. • The feature map is the output of the filter as applied to the image. - It has the same number of pixels as the image. - The image is usually padded with zeros to allow the filter slide over the edges of the image Feature Map Image Application of a Filter w11 w12 w21 w22 x11 x1n xij xn1 xmn Filter Image Application of a Filter w11 w12 w21 w22 x11 x1n xij xn1 xmn Feature map Image image, in the cell placed filter the ,j+v- i+u- (i,j) z11 z1n zij zn1 zmn Application of a Filter For the image, light grey = 0 and dark grey = 1 For the filter a red dot means a weight of 1. No dot means a weight of 0. For the feature map, higher the value, the darker the cell is. Feature map in values and colors Image Filter Application of a Filter • An image typically has multiple channels, e.g., RGB. Therefore, it's not a 2D matrix but a 3D volume. If the image has D channels the depth of the volume is D. • Hence, the filter too is a 3D volume with the same depth (D) as the image. image, the cell it' filter whe the Output ,j+v- i+u- uvk (i,j) Application of a Filter • Typically, multiple individual filters are applied to an image. - Results in a separate feature map for each filter. - If Nf is the number of filters used, there'll be Nf feature maps generated. Application of a Filter filter weights the are and map feature cell value the that Note by, given image the cell it' when filter Output (i,j) (i,j) uvkp ijp ,j+v- i+u- uvkp ijp ijp Feature map p Application of a Filter filter weights the are and map feature cell value the that Note by, given image the cell it' when filter Output (i,j) (i,j) uvkp ijp ,j+v- i+u- uvkp ijp ijp Feature map p Pooling Layer • After a convolutional layer (filtering), the feature maps are sub-sampled. x11 x1n xij xn1 xmn z11 zqr Pooling Layer x11p x1np xijp xn1p xmnp z11p zqrp area this Let max map feature For ijp i,j qrp • Max operation is commonly used in the pooling layer: Max pooler Convolutional Neural Network • A convolutional NN is a collection of alternating convolutional and pooling layers. • Typically at the end, a fully connected feed forward NN with a hidden layer is present. CNN Features CNN Features Convolutional nets Vs. fully connected nets Consider two layers with nm neurons each. • A fully connected network would have (nm)2 number of weights to learn • A convolutional layer with a filter of size sr would involve only sr weights. - Typically sr <<< nm - Remember, it's the same filter that is slid over the image. • Convolutional layers allow us to build deep networks (many layers) for object recognition. - These do better than other methods. (They won the image net competition) Back Propagation: A Modular Approach lz1 lz2 Then Let l th layer We can assume that the last module computes the error. Then, ZL+1=E. Therefore message forward the Convolutional Layer ijk qrp Derivatives for Convolutional Layer l • From the equations in the previous slide ,r+v- q+u- qrp uvkp qrp qrp uvkp ijk ijk ,r+v- q+u- uvkp qrp Then Let (same equation as in slide 10) Derivatives for Convolutional Layer l otherwise qrp ijk uvkp qrp ijk qrp qrp ijk qrp qrp ijk ijk ijk ,r+v- q+u- uvkp qrp Therefore r+v- when and q+u- when r+v- and q+u- where and with Derivatives for Max Pooling Layer l otherwise argmax if ,1 max{ and where and with xyk ijk ijk qrp ijk qrp qrp ijk qrp qrp ijk ijp ijp xyp qrp Prominent CNN architectures • Resnet (Kaiming He et al., 2015) - 152 layers, Winner Imagenet 2015. • GoogleNet (Szegedy et al., 2015) - 22 Layers Winner Imagenet 2014. • VGG (Simonyan et. al., 2014) - 19 Layers, 2nd place, Imagenet 2014. Reference • Machine learning lecture 10 - Convolutional Neural Networks, by Nando de Freitas, at Oxford University - Slides available at, https://www.cs.ox.ac.uk/people/nando.defreitas/ machinelearning/lecture9.pdf - Video available at, https://www.youtube.com/watch?v=bEUX_56Lojc"
  },
  {
    "index": 4,
    "level": 1,
    "start_page": 94,
    "end_page": 123,
    "title": "4. Improving Deep Neural Networks",
    "content": "Improving the Performance of Deep Neural Netoworks Charith Chitraranjan Outline • Normalization • Dropout • Early Stopping • Data Augmentation Normalizing Inputs Use same  and  for the test data too. Why Normalizing Inputs? Normalization for Intermediate Layers • Can we normalize the output of some layer to make it easier to learn the parameters (weights) of the next layer? • Yes, Batch normalization • Normalize the weighted sums (Y's) of layer l before forwarding them to layer l+1. • Note that we apply normalization before the application of the activation function. It's still an ongoing debate whether to do it before or after activation function. Before the activation seems to be popular at the moment. size minibatch norm Layer l Batch Normalization • Normalizing everything to 0 mean and a stabdard deviation of 1 is not always desirable. So, they are scaled. parameters learnable are norm norm Dropout • Randomly drop neurons (and its connections) from a neural network during training. • Each neuron is retained with a probability p. I.e., droped with probability (1-p). Srivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex,Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout:A simple way to prevent neural networks from overfit-ting.J. Mach. Learn. Res., 15(1):1929-1958, January2014. Dropout • When the network is used in testing, the weight on each outgoing connection of a neuron is multiplied by the probability it was present (p). Early Stoping • During training, monitor the error on a validation set and stop training as soon as the validation error starts to increase. • Stop earlier than the point at which the trainign error is minimum Data Augmentation • Transform original data to generate synthetic data • Produces more data. • Not as good as collecting more actual data but less expensive. • Train the models with augmented data. • Prduces more generalized models. References • S. Ioffe and C. Szegedy. Batch normalization: Accelerating deepnetwork training by reducing internal covariate shift. InICML, 2015. • https://www.youtube.com/watch?v=FDCfw- YqWTE&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=9 • https://www.youtube.com/watch?v=tNIpEZLv_eg&list=PLkDaE6sCZn6 Hn0vK8co82zjQtt3T2Nkqc&index=27 • https://www.youtube.com/watch?v=em6dfRxYkYU&list=PLkDaE6sCZ n6Hn0vK8co82zjQtt3T2Nkqc&index=28 Improved Architectures for Deep Neural Networks Charith Chitraranjan Outline • Problems with typical Deep CNNs • Residual networks (ResNets) • Densely connected convolutional networks (DenseNets) Problems with Deep CNNs • Exploding/vanishing gradient problem • Addressed to a large extent by, • normalized initialization of weights (Gloret, 2010) and • batch normalization(Ioffe, 2015) • Degradation problem • As a network gets deeper beyond some number of layers, accuracy saturates and then degrades(He, 2016). • This is not caused by overfitting as the training accuracy degrades too. Degradation Problem... • The training accuracy of a deeper model shouldn't decrease due to the addition of the layers. • Assume you add extra layers to some optimal shallower network. Then, each of the the extra layers can learn to become the identity function and the accuracy will be the same. • E.g., let's say we get the best possible training accuracy with 𝑙 layers. Then, if we add more layers, then for any 𝑖= 𝑙+ 1, ... 𝐿 , 𝑓𝑖 can learn to be the identity function, 𝑓𝑖𝑍𝑖= 𝑍𝑖. This means input is connected to output with one-one mapping with unit weights. • But this problem exists with typical deep neural networks/CNNs - additional layers may not learn what they can learn to improve accuracy. Residual Networks (ResNet) • Idea is to have identity mapping with shortcuts between every few layers. Input Input Typical network ResNet Image source: (He, 2016) ResNet: Basic building block • Let 𝐻(𝑋) be the true mapping function that a typical NN without identity mappings would have to learn with the two weight layers shown. • Output of the two layers = 𝐻(𝑋) • With identity mappings, the two weight layers would have to learn a mapping function 𝐹(𝑋) such that, • 𝐹𝑋= 𝐻𝑋-𝑋 (because now the output, 𝐻𝑋= 𝐹𝑋+ 𝑋)) • 𝐹𝑋 is the residual with reference to the identity • The hypothesis is that it would be easier to learn the residual 𝐹𝑋 than trying to learn 𝐻𝑋. Basic building block of ResNet (He, 2016) ResNet: Hypothesis • The hypothesis is that it would be easier to learn the residual 𝐹𝑋 than trying to learn 𝐻𝑋. • If the mapping to be learned by these layer is indeed an identity function, then, • the degradation problem indicates that the layers have a hard time learning this identity function. • with identify mapping already provided, the weights should be driven to 0 making 𝐹𝑋= 0 for all 𝑋 because the residual is 0. Basic building block of ResNet (He, 2016) ResNet: Hypothesis... • In real-world problems, the optimal function to be learned, 𝐻𝑋 may not be identity mapping. • But if 𝐻𝑋 is close to identity, the assumption is that the residual, 𝐹𝑋 is easier to learn as perturbations with respect to the identity function than to learn 𝐻𝑋 as a new function from scratch. • This architecture helps to reduce the vanishing gradient problem too as it provides direct paths for the gradient to flow back. Basic building block of ResNet (He, 2016) ResNet: Basic building block... • The output (before the second relu) 𝑌= 𝐹𝑋+X • The above works if the dimensions of 𝐹 and 𝑋 are the same. • If not, 𝑌= 𝐹𝑋+ 𝑊𝑆X, where 𝑊𝑆 is a matrix to perform a linear projection for matching dimensions. Basic building block of ResNet (He, 2016) ResNet: Architecture... • He et al. (2016) have experimented with 𝐹 having 2 or 3 layers, although more are possible • 𝐹 having only a single layer has not provided an observable advantage. Input Input Typical (plain) network ResNet Image source: (He, 2016) ResNet: Some experimental results • Training on ImageNet. Thin curves denote training error, and bold curves denote validation error of the center crops. Left: plain networks of 18 and 34 layers. Right: ResNets of 18 and 34 layers. In this plot, the residual networks have no extra parameter compared to their plain counterparts (He, 2016). ResNet: Some experimental results... • Error rates(%,10 crop testing) on ImageNet validation. VGG-16 is based on tests by He at al. ResNet 50/101/152 are of option B that only uses projections forincreasing dimensions (He, 2016). Densely connected convolutional networks (DenseNet): Architecture • Connect all layers directly with each other in a feed- forward manner (Huang, 2017) • Each layer gets input from each of the preceding layers and provides its output as input to each of the subsequent layers. • Size of the feature maps should be matched. DenseNet: Advantages • Has a smaller number of parameters. • Alleviates the vanishing gradient problem as there are direct connections for the gradients to flow back. DenseNet: Why it has less parameters • In the traditional architecture, a layer must preserve information coming from the preceding layer as well as add new information that it computes and pass all this information to the next layer through itself. • Needs a large number of feature-maps (channels) → large number of parameters. • DenseNet architecture, separates the information that must be preserved from the preceding layers and the information that needs to be added by a given layer. • Information from preceding layers arrive through direct paths → a smaller number of feature-maps (channels) → a smaller number of parameters despite having dense connections. • In this example, each layer outputs 4 feature maps. DenseNet: Mapping function • Let 𝐻𝑙. be the mapping function computed by layer 𝑙, which can be a composite function that performs batch normalization (BN), ReLU, and convolution (usually 3x3). • Let 𝑋𝑙 be the output of layer 𝑙. • Then, 𝑋𝑙= 𝐻𝑙([𝑋0, 𝑋1,..., 𝑋𝑙-1]), where [𝑋0, 𝑋1,..., 𝑋𝑙-1] is a concatenation of outputs (feature maps) from all the layers before 𝑙. Not an addition as in ResNet ReLU Conv [𝑋0, 𝑋1,..., 𝑋𝑙-1] DenseNet: Multiple dense blocks • There are transitional layers between dense blocks • In the DenseNet implementation (Huang, 2017), a set of transitional layers consists of a BN layer (not shown), a 1x1 convolutional layer and a 2x2 average pooling layer. • Concatenation of input within a dense block doesn't allow changing the size of the feature maps. So, they all have the same size. • But reducing the size of feature maps is an essential part of a CNN. • The above architecture with transitional pooling layers allows the reduction of feature-map size. E.g., A network with 3 dense blocks (Huang, 2017). DenseNet: Some experimental results • Comparison of the DenseNets and ResNets top-1 error rates (single-crop testing) on the ImageNet validation dataset as a function of learned parameters(left) and FLOPs (floating-point operations) during test-time (right) (Huang, 2017). References • Glorot, X., & Bengio, Y. (2010, March). Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics (pp. 249-256). JMLR Workshop and Conference Proceedings. • He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778). • Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4700-4708). • Ioffe, S., & Szegedy, C. (2015, June). Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning (pp. 448-456). pmlr."
  },
  {
    "index": 5,
    "level": 1,
    "start_page": 124,
    "end_page": 152,
    "title": "5. Optimization and Hyperparameter Tuning",
    "content": "Optimization for Neural Networks Charith Chitraranjan Outline • Gradient decent - With Momentum - RMSprop - Adam • Hyperparameter tuning A Feed Forward Neural Network We need to find appropriate values for the weights w1, ..., w14 so that the outputs z1 and z2 behave desirably. Each neuron computes h(wx). I.e., h(weighted sum of its inputs). h is some transfer function. E.g., the sigmoid function w1 x w10 w14 Adjusting the Weights • Consider the same example as before. • We can compute a prediction error (E) that depends on the mismatch between the desired outputs di 1,di 2 and predicted outputs 1,zi - E.g., Squared difference • We can adjust the weights such that E reaches its minimum. Adjusting the Weights • Eventually, error (E) is a function of the weights and the inputs. - Why not just express E, in terms of w's and x's and minimize it ? • For a network of decent size and non-linear transfer functions, expressing E analytically in terms of weights and minimizing it becomes tedious and almost impossible. • Neural networks with just input and output layers (no hidden layers) are very limited in what they can do and they need carefully crafted features, which are tedious to get. • Neural networks with many hidden layers (deep neural nets), can learn features without much human intervention. • So, we use numerical methods to adjust the weights. - Gradient decent is a popular choice Gradient Decent E = 1 w E = • Let the current values be w1=a and w2=b. • We want to reduce the error. So, we need to change w1 and w2 along some direction that has a negative gradient (down hill). Gradient Decent Gradient Decent E = 1 w E = • Let the current values be w1=a and w2=b. • We want to reduce the error. So, we need to change w1 and w2 along some direction that has a negative gradient (down hill). • is the direction of the maximum gradient. So, we move in the opposite direction specified by . Gradient Decent E = 1 w E = • Let the current values be w1=a and w2=b. • We want to reduce the error. So, we need to change w1 and w2 along some direction that has a negative gradient (down hill). • is the direction of the maximum gradient. So, we move in the opposite direction specified by . Gradient Decent: Update step • Let f(w1,...wj ,...wN) be a function of N variables. • Then, in the kth iteration, each variable wj will be updated as follows. ,..., ,..., point at the evaluated w.r.t derivative partial the and rate learning the where Gradient Decent: Update step • Let f(w1,...wj ,...wN) be a function of N variables. • Then, in the kth iteration, each variable wj will be updated as follows ,..., ,..., point at the evaluated w.r.t derivative partial the and rate learning the where Gradient Decent for Learning Parameters in NNs • For some error function 𝐸, the weight update is, • With the error functions we use (E.g., squared error or cross- entropy), all training data must be explored before the error can be computed. • This is called batch gradient decent. However, for large dataset size N, this is going to be time consuming. sample data for error the where Stochastic Gradient Decent for Learning Parameters in NNs • One solution is to update the parameters after exploring each training example (typically, the training examples are randomly ordered). • This usually converges faster to a reasonable solution and is less susceptible to global optima. • However, it may not converge to the best solution, but hover around it. - This is may be good enough in some cases though. • To have the best of both options, we often use a hybrid method called mini batch gradient decent. each For Mini-batch Gradient Decent for Learning Parameters in NNs • Idea: update the parameters after exploring every m samples, where m<<N. For Gradient Descent with Momentum Gradient descent may need many iterations due to oscillations - learning rate will need to be kept small to prevent overshoot Solution: compute a moving average for the gradient to cancel/smooth out the oscillations. (𝑘) = 𝛽1𝑣𝑗 (𝑘-1) + 1 -𝛽1 𝜕𝐸𝑚 𝜕𝑤𝑗 (𝑘)= 𝑤𝑗 (𝑘-1) -𝛼𝑣𝑗 (𝑘), where 𝛼 is the learning rate. This allows a higher learning rate to enable faster convergence. Image adapted from: https://optimization.cbe.cornell.edu/index.php?title=Momentum Exponential Moving Average (EMA) 𝑣(𝑛) = 𝛽𝑣(𝑛-1) + 1 -𝛽𝑇(𝑛) 𝑣(0) = 0 𝑣(1) = 𝛽𝑣(0) + 1 -𝛽𝑇(1) 𝑣(2) = 𝛽𝑣(1) + 1 -𝛽𝑇(2) = 𝛽(𝛽𝑣(0) + 1 -𝛽𝑇(1)) + 1 -𝛽𝑇(2) = 𝛽2𝑣(0)+ 1 -𝛽𝑇(2)+ 𝛽1 -𝛽𝑇(1) Image source: https://medium.com/m learning- ai/exponentially- weighted-average- 5eed00181a09 Exponential Moving Average (EMA) 𝑣(2) = 𝛽𝑣(1) + 1 -𝛽𝑇(2) = 𝛽(𝛽𝑣(0) + 1 -𝛽𝑇(1)) + 1 -𝛽𝑇(2) = 𝛽2𝑣(0)+ 1 -𝛽𝑇(2)+ 𝛽1 -𝛽𝑇(1) 𝑣(3) = 𝛽𝑣(2) + 1 -𝛽𝑇(3) = 𝛽𝛽2𝑣(0)+ 1 -𝛽𝑇(2)+ 𝛽1 -𝛽𝑇(1) + 1 -𝛽𝑇(3) = 𝛽3𝑣(0) + 1 -𝛽𝑇(3) + 𝛽1 -𝛽𝑇(2) + 𝛽2 1 -𝛽𝑇(1) Image source: https://medium.com/m learning- ai/exponentially- weighted-average- 5eed00181a09 Exponential Moving Average (EMA) 𝑣(𝑛) = 𝛽𝑣(𝑛-1) + 1 -𝛽𝑇(𝑛) = 𝛽𝑛𝑣(0) + 1 -𝛽𝑇(𝑛) + 𝛽1 -𝛽𝑇(𝑛-1) + 𝛽2 1 -𝛽𝑇(𝑛-2) + ⋯+ 𝛽𝑛-1 1 -𝛽𝑇(1) Image source: https://medium.com/m learning- ai/exponentially- weighted-average- 5eed00181a09 Exponential Moving Average (EMA) 𝑣(𝑛) = 𝛽𝑣(𝑛-1) + 1 -𝛽𝑇(𝑛) = 𝛽𝑛𝑣(0) + 1 -𝛽𝑇(𝑛) + 𝛽1 -𝛽𝑇(𝑛-1) + 𝛽2 1 -𝛽𝑇(𝑛-2) + ⋯+ 𝛽𝑛-1 1 -𝛽𝑇(1) Approximately, 1-𝛽 number of values in the past are averaged. Image source: https://medium.com/m learning- ai/exponentially- weighted-average- 5eed00181a09 EMA: Bias correction 𝑣(𝑛) = 𝛽𝑣(𝑛-1) + 1 -𝛽𝑇(𝑛) 𝑣(0) = 0 𝑣(1) = 𝛽𝑣(0) + 1 -𝛽𝑇(1) For 𝛽= 0.9, 𝑣(1) = 0.9 × 0 + 0.1 × 𝑇(1), small value Image source: https://medium.com/m learning- ai/exponentially- weighted-average- 5eed00181a09 Bias correction: 𝑣(𝑛) 1-𝛽𝑛 Gradient Descent with Momentum • Gradient descent with bias corrected momentum (𝑘) = 𝛽1𝑣𝑗 (𝑘-1) + 1 -𝛽1 𝜕𝐸𝑚 𝜕𝑤𝑗 (𝑘) = 𝑤𝑗 (𝑘-1) -𝛼 1-𝛽1 𝑘, where 𝛼 is the learning rate. RMSprop Compute the EMA of the squares of the gradients (𝑘) = 𝛽2𝑠𝑗 (𝑘-1) + 1 -𝛽2 𝜕𝐸𝑚 𝜕𝑤𝑗 (𝑘)= 𝑤𝑗 (𝑘-1) - (𝑘) 1-𝛽2𝑘 +𝜀 𝜕𝐸𝑚 𝜕𝑤𝑗, where 𝛼 is the learning rate. This too allows higher learning rate to enable faster convergence. Image adapted from: https://optimization.cbe.cornell.edu/index.php?title=Momentum Adam Optimizer • Adam - Adaptive moment estimation • Combines both momentum and RMSprop (𝑘)= 𝑤𝑗 (𝑘-1) - (𝑘) 1-𝛽2𝑘 +𝜀 (𝑘) 1-𝛽1 where 𝛼 is the learning rate. Default values: 𝛽1 = 0.9, 𝛽2 = 0.99, 𝜀= 10-8 • Adam enables faster training. Learning Rate Decay Idea: Reduce the learning rate as epochs progress. - Helps to move through the search space fast initially and then fine tune towards the (local) optimum. Possible options: 1 + 𝑑𝑐𝑜𝑛𝑠𝑡× 𝑒𝑝ℎ𝑜𝑐# 𝛼= 𝜆𝑒𝑝ℎ𝑜𝑐#𝛼0, where 0 < 𝜆< 1. Hyperparameter Tuning • Potential hyperparameters to tune - Learning rate (𝛼). - Parameters of Adam (𝛽1, 𝛽2, and 𝜀) - usually default values are used for these. - Mini batch size. - Number of layers and number of neurons in each of them. - Learning rate decay parameters. - Etc. Hyperparameter Tuning: Grid Vs. Random Search 𝐻𝑃1. E.g., 𝛼 𝐻𝑃2. E.g., 𝜀 Grid search If one or more parameters has little effect on the performance, this can be waste of time. Random search Attempts a variety of values for each parameter. 𝐻𝑃1. E.g., 𝛼 𝐻𝑃2. E.g., 𝜀 Hyperparameter Tuning: Coarse to Fine Granularity • Initially, search a larger area coarsely. • Identify areas that produced better results. • Do a more fine-grained search in those areas. 𝐻𝑃1. E.g., 𝛼 𝐻𝑃2. E.g., 𝜀 𝐻𝑃1. E.g., 𝛼 𝐻𝑃2. E.g., 𝜀 Hyperparameter Tuning: Coarse to Fine Granularity • Use the training dataset or a validation dataset to assess model performance during hyperparameter tuning - leave the test data out of it. 𝐻𝑃1. E.g., 𝛼 𝐻𝑃2. E.g., 𝜀 𝐻𝑃1. E.g., 𝛼 𝐻𝑃2. E.g., 𝜀 References • https://www.youtube.com/playlist?list=PLkDa E6sCZn6Hn0vK8co82zjQtt3T2Nkqc • https://optimization.cbe.cornell.edu/index.ph p?title=Momentum • https://optimization.cbe.cornell.edu/index.ph p?title=RMSProp • https://www.cs.cmu.edu/~10606/gradient.pdf"
  },
  {
    "index": 6,
    "level": 1,
    "start_page": 153,
    "end_page": 262,
    "title": "6. Computer Vision",
    "content": "CS3631: Deep Neural Networks Deep neural networks for computer vision. - [Overview of object detection/ recognition/ applications] Important information • Textbook: Rick Szeliski, Computer Vision: Algorithms and Applications online at: http://szeliski.org/Book/ • Projects at CSE • Bio-Health Informatics https://sites.google.com/cse.mrt.ac.lk/biohealth • Eco-Sustainable Informatics https://sites.google.com/view/eco-sustainable/home Deep Neural Networks for Computer Vision - [Object Detection/ Recognition/ Applications] • LO1: Explain the concepts and terminology in problem solving with deep learning. • LO2: Explore the theoretical basis for major algorithms and approaches in deep learning. • LO3: Apply deep learning algorithms and tools to solve practical problems. • LO4: Enhance deep learning architectures for real-world applications. What is computer vision? Object detection (Sliding window, region proposals, selective search) Segmentation Object recognition Architectures: Region-based CNN (RCNN),. Fast RCNN, Faster RCNN, Mask-RCNN, YOLO. ( You only look once) Applications What is Computer Vision? Computer Vision Human Vision Identifying Different Objects [Animals] Dogs Fox Cat Rabbit Deer Pig Owl Can computers match human perception? • Yes and no (mainly no) • computers can be better at “easy” things • humans are better at “hard” things • The human visual system has no problem interpreting the subtle variations and shading and correctly segmenting the object from its background. • But huge progress • Accelerating in the last five years due to deep learning • What is considered “hard” keeps changing But humans can tell a lot about a scene from a little information... Human perception has its shortcomings Optical illusion Ambiguity Seeing / Thinking10 Images and Digital Representation • Goal of computer vision: perceive the “story” behind the picture • Compute properties of the world • 3D shape • Names of people or objects • What happened? How Image Classification Happens Identifying Different Objects Issues in Computer Vision Why is computer vision difficult? Viewpoint variation Illumination Scale Credit: Flickr user michaelpaul Credit: Flickr user michaelpaul Why is computer vision difficult? Intra-class variation Background clutter Motion (Source: S. Lazebnik) Occlusion16 Goal of Computer Vision • Improve photos Super-resolution (source: 2d3) Low-light photography (credit: Hasinoff et al., SIGGRAPH ASIA. 2016) Depth of field on cell phone camera (source: Google Research Blog) Removing objects (Google Magic Eraser) “A photo of a dog riding a bike in Times Square. It is wearing sunglasses and a beach hat” - Imagen Hair removal of medical skin images Applications Gaming Tumor identification 3D shape creation Autonomous vehicles Movie making Space exploration Applications Biometric Face recognition and analysis Detect faces and people Forensic Image Classification: A core task in Computer Vision (assume given a set of possible labels) {dog, cat, truck, plane, ...} cat This image by Nikita is licensed under CC-BY 2.0 Lecture 9 April 26, 2022 Computer Vision Tasks Image Classification with Localization Computer Vision Tasks Classification Semantic Segmentation Object Detection Instance Segmentation CAT GRASS, CAT, TREE, SKY DOG, DOG, CAT DOG, DOG, CAT. No spatial extent Multiple Object No objects, just pixels Lecture 9 Semantic Segmentation Label each pixel in the image with a category label Do not differentiate instances, It only care about pixels. (Disadvantage in sematic segmentation) Panoptic segmentation, where both objects and stuff (e.g., grass, sky) get labeled Semantic Segmentation: The Problem GRASS, CAT, TREE, SKY. , ... Paired training data: for each training image, each pixel is labeled with a semantic category. At test time, classify each pixel of a new image. Lecture 9 April 26, 2022 Fei-Fei Li, Jiajun Wu, Ruohan Gao Overview: Fundamental methods in object detection, localization, segmentation Traditional methods • Sliding Window Technique • Region Proposal Technique - Selective Search • Histogram of Oriented Gradients Deep learning methods • Region-based Convolutional Neural Networks (R-CNN) • Fast R-CNN • Faster R-CNN • Single Shot Detector (SSD) • You Only Look Once (YOLO) • Mask R-CNN Semantic Segmentation Idea: Sliding Window Full image Lecture 9 April 26, 2022 Fei-Fei Li, Jiajun Wu, Ruohan Gao Impossible to classify without context Q: how do we include context? Full image Q: how do we model this? Semantic Segmentation Idea: Sliding Window Full image Extract patch Classify center pixel with CNN Cow Grass Cow Problem: Very inefficient! Not reusing shared features between overlapping patches Semantic Segmentation Idea: Convolution An intuitive idea: encode the entire image with conv net, and do semantic segmentation on top. Problem: classification architectures often reduce feature spatial sizes to go deeper, but semantic segmentation requires the output size to be the same as input size. Convolutional layers apply a convolution operation to the input, using small filters (kernels) that scan across the input data. Each filter produces a feature map that highlights specific patterns, such as edges or textures, in the input. Fully connected layer, every neuron is connected to every neuron in the previous layer. This dense connectivity allows the network to learn complex relationships between input features. Semantic Segmentation Idea: Fully Convolutional Input: 3 x H x W Convolution s: D x H x Conv Conv Conv Conv Scores: C x H x W argmax Predictions: H x W Design a network with only convolutional layers without downsampling operators to make predictions for pixels all at once! Problem: convolutions at original image resolution will be very expensive ... Semantic Segmentation Idea: Fully Convolutional Input: 3 x H x W Predictions: H x W Design network as a bunch of convolutional layers, with downsampling and upsampling inside the network! High-res: D1 x H/2 x W/2 Med-res: D2 x H/4 x W/4 Med-res: D2 x H/4 x W/4 Low-res: D3 x H/4 x W/4 Downsampling: Pooling, strided convolution Upsampling: ??? C x H x W High-res: D x H/2 x W/2 Unpooling or strided transposed convolution Semantic Segmentation Idea: Fully Convolutional Design network as a bunch of convolutional layers, with downsampling and upsampling inside the network! Med-res: D2 x H/4 x W/4 Med-res: D2 x H/4 x W/4 Low-res: D3 x H/4 x W/4 Downsampling: Pooling, strided convolution Upsampling: Unpooling or strided transposed convolution 2-D cross correlation with padding In-Network upsampling: “Unpooling” Input: 2 x 2 Output: 4 x 4 Nearest Neighbor Input: 2 x 2 Output: 4 x 4 “Bed of Nails” In-Network upsampling: “Max Unpooling” Object Detection and Segmentation Segmentation is NOT easy - Some objects maybe within another object - Color does not always help segment an object - There is no single best strategy to group objects One solution: - Generate a hierarchy of segmentations at different scales using a variety of strategies. - Rank and combine the bounding boxes of these segments to generate candidate regions for object recognition. Next: Object Detection, Localization and Instance Segmentation Traditional methods • Sliding Window Technique • Region Proposal Technique - Selective Search Deep learning methods • Region-based Convolutional Neural Networks (R-CNN) • Fast R-CNN • Faster R-CNN • Single Shot Detector (SSD) • You Only Look Once (YOLO) • Mask R-CNN OBJECT DETECTION. Object Recognition Overview: Fundamental methods in object detection, localization and Instance Segmentation Traditional methods ✓Sliding Window Technique • Region Proposal Technique - Selective Search Deep learning methods • Region-based Convolutional Neural Networks (R-CNN) • Fast R-CNN • Faster R-CNN • Single Shot Detector (SSD) • You Only Look Once (YOLO) • Mask R-CNN Brief History Classification, Localization, Detection, Segmentation Class Scores Cat: 0.9 Dog: 0.05 Car: 0.01 ... Vector: Fully Connected: 4096 to 1000 Box Coordinates Fully Connected: 4096 to 4 Softmax Loss L2 Loss Loss Correct label: Cat Multitask Loss Object Detection: Single Object (Classification + Localization) x, y (x, y, w, h) Treat localization as a regression problem! Correct box: (x', y', w', h') CAT: (x, y, w, h) DOG: (x, y, w, h) DOG: (x, y, w, h) CAT: (x, y, w, h) DUCK: (x, y, w, h) DUCK: (x, y, w, h) numbers numbers Many numbers! Each image needs a different number of outputs! Object Detection: Multiple Objects Dog? NO Cat? NO Background? YES Apply a CNN to many different crops of the image, CNN classifies each crop as object or background Object Detection: Multiple Objects Sliding Window Approach • Problem: Need to apply CNN to huge number of locations, scales, and aspect ratios, very computationally expensive! Region Proposals: Selective Search Find “blobby” image regions that are likely to contain objects Relatively fast to run; e.g. Selective Search gives 2000 region proposals in a few seconds on CPU Region-based CNN (R-CNN) R-CNN. Lecture 9 “Slow” R-CNN Input image ConvNet ConvNet ConvNet SVMs SVMs SVMs Warped image regions Regions of Interest (RoI) from a proposal method (~2k) Forward each region through ConvNet Classify regions with SVMs Bbox reg Bbox reg Bbox reg Predict “corrections” to the RoI: 4 numbers: (dx, dy, dw, dh) Lecture 9 Problem: Very slow! Need to do ~2k independent forward passes for each image! Idea: Pass the image through convnet before cropping! Crop the conv feature instead! R-CNN Problem: Very slow! Need to do ~2k independent forward passes for each image! Idea: Pass the image through convnet before cropping! Crop the conv feature instead! Best Bounding Box Identification Issue: What if we output two boxes both pointing to the same object? - Use non-maximum suppression (NMS) to filter out redundant overlapping bounding boxes and retain only the most relevant ones. - Use when the object is the same for all of the bounding boxes Cannot use NMS Intersection Over Union (IoU) Precision: from all of our predictions, how many of them are correct? 1/3 Recall: from all the ground truths, how many of them predicted correctly? ½ Fast R-CNN Fast R-CNN - runs the CNN only once on the entire image to generate a convolutional feature map, This reduces the computational cost and improves the detection speed. Disadvantage: the selective search process is slow R-CNN - Feeds 2000 region proposals to the CNN separately Fast R-CNN Idea: Pass the image through convnet before cropping! Crop the conv feature instead! Why Fast R-CNN is fast than RCNN? Classification Bounding Box Differentiability: supports effective training through backpropagation. Backpropagation computes gradients of the loss function with respect to the network's weights. This process is essential for optimizing the network during training. Optimal Training Process - Why RoI Faster R-CNN: Lecture 9 Make CNN do proposals! Insert Region Proposal Network (RPN) to predict proposals from features. R-CNN and Fast R-CNN rely on the Selective Search algorithm to generate region proposals, which is slow. This bottleneck was addressed Faster R-CNN Otherwise same as Fast R-CNN: Crop features for each proposal, classify each one Faster R-CNN: Faster R-CNN is a Two-stage object detector First stage: Run once per image Backbone network Region proposal network Second stage: Run once per region Crop features: RoI pool / align Predict object class Prediction bbox offset Lecture 9 Do we really need the second stage? Object Recognition Does not support real-time object detection Object Detection Object Recognition References: R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 580-587. IEEE, 2014. Object Detection and Recognition Single-Stage Object Detectors: YOLO / SSD. / RetinaNet Divide image into grid 7 x 7 Image a set of base boxes centered at each grid cell Here B = 3 Input image 3 x H x W Within each grid cell: Regress from each of the B base boxes to a final box with 5 numbers: (dx, dy, dh, dw, confidence) Predict scores for each of C classes (including background as a class) Looks a lot like RPN, but category-specific! Output: 7 x 7 x (5 * B + C) YOLO:. You Only Look Once 1. Extremely fast 2. Reason Globally on the Entire Image 3. Learn Generalizable Representations YOLO : Process 1. Mark a SxS grid over the image, normally a 7x7 grid 2. For each bounding box, predict Its center (x,y) Height and width (h,w) Confidence = P(object)x IoU 3. Each grid cell also predicts a conditional class probability :Class probability map for each grid cell that reflects the most likely object class that exists in a grid cell 4. Combine the above information to detect objects 5. Threshold detections and NMS YOLO:. Object Localization (dx, dy, dw, dh, confidence) These numbers are just for understanding. There are more calculations in the actual processing YOLO: Training and Testing x-train y-train CNN Output x-test During the training phase: If there is an object in the cell, P(object) =1 Otherwise, P(object) = confidence=0 If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object. YOLO: Architecture YOLO: Training YOLO: Training YOLO: Loss Function Pc: class probability (1/0) whether the grid cell contain an object or not/ x: Normalized x coordinate of the center of the bounding box y: Normalized y coordinate of the center of the bounding box w: Normalized width of the bounding box h: Normalized height of the bounding box C: confidence score probability that the box contains an object This normalization ensures that the value is between 0 and 1, making it independent of the image size. L = Lcls + Lloc Without these constants; • Localization and class prediction errors are equally weighted - this is not ideal. • Since most grid cells don't have objects, their confidence predictions are 0 and they dominate the gradient - this creates model instability. Dropout and extensive data augmentation is used to avoid overfitting. YOLO: Multiple Object Detection Blue Yellow Dog bicycle YOLO: Summary - A Singular Approach: YOLO reframes object detection, transitioning from image pixels directly to bounding box coordinates and class probabilities in one seamless step. - Grid-Based Detection: The algorithm segments images into grids, predicting bounding boxes and their confidence scores within each segment. - Intersection Over Union (IOU): This metric ensures that YOLO's predicted bounding boxes align perfectly with the actual objects. YOLO version 1: Limitations Each grid cell can contain only one class Each grid cell can contain only B bounding boxes Bounding box prediction struggles with unusual ratios Same error in small box is worse than in large box Many grid cells do not contain an image o Training confidence scores pushed to zero o Coordinate loss contribution is low YOLO: History YOLO 10 (2024) : Latest Version https://arxiv.org/pdf/2405.14458 YOLO 10 : Strengths and Weaknesses Strengths: Popular object detection algorithm Fast inference/ efficient → use in real-time object detection End- to-end training Background error is low Simple architecture and require minimal training data → easy to implement and adapt to new tasks. Weaknesses: Performance is lower than state-of-art Makes more localization errors Struggle with small objects Inability to perform fine-grained object classification Real-time end-to-end object detection 1. NMS-free training 2. Spatial-channel decoupled down- sampling 3. Rank-guided block design Applications: video surveillance, self-driving cars, and augmented reality. Object Detection: Many variables ... Backbone Network VGG16 ResNet-101 Inception V2 Inception V3 Inception ResNet MobileNet “Meta-Architecture” Two-stage: Faster R-CNN Single-stage: YOLO / SSD. Hybrid: R-FCN Image Size # Region Proposals Takeaways Faster R-CNN is slower but more accurate SSD is much faster but not as accurate Bigger / Deeper backbones work better Instance Segmentation Classification Semantic Segmentation Object Detection Instance Segmentation CAT GRASS, CAT, TREE, SKY DOG, DOG, CAT DOG, DOG, CAT. No spatial extent Multiple Object No objects, just pixels Object Detection: Faster R-CNN Lecture 9 Instance Segmentation: Mask R-CNN Mask Prediction Add a small mask network that operates on each RoI and predicts a 28x28 binary mask Mask R-CNN RoI Align Con Classification Scores: C Box coordinates (per class): 4 * CNN +RPN. Con Predict a mask for each of C classes C x 28 x 28 256 x 14 x 14 256 x 14 x 14 Mask R-CNN: Very Good Results! Lecture 9 Mask R-CNN for Pose Lecture 9 Summary: Fundamental methods in object detection Traditional methods • Sliding Window Technique • Region Proposal Technique - Selective Search Deep learning methods • Region-based Convolutional Neural Networks (R-CNN) • Fast R-CNN • Faster R-CNN • Single Shot MultiBox Detector (SSD) • You Only Look Once (YOLO) • Mask R-CNN Applications Gaming Tumor identification 3D shape creation Autonomous vehicles Movie making Space exploration Applications Biometric Face recognition and analysis Detect faces and people Forensic Autonomous Vehicles Medical Image/ Signal Processing Bio-Health Informatics https://sites.google.com/ cse.mrt.ac.lk/biohealth Medical Applications: Chest X-ray Classification Classification https://chestxpert.live/home Factory Automation OCR - Optical Character Recognition Biometrics & Face Recognition Gaming & Activity Recognition Movie Maker: Visual Effects Virtual reality Augmented reality Artificial Intelligence 3D technology Face/ Action manipulation Visual Search Space Exploration Remote Sensing Eco-Sustainable Informatics Pose Estimation (Human Activity Recognition) Human Computer Interaction Sports and Fitness Patient care and Rehabilitation Gaming and Animation Autonomous Vehicles Security and Surveillance Robotics Virtual Reality and Augmented Reality Video Analysis Packing process Unpacking process Video Analysis using RNN Beyond 2D Object Detection... Lecture 9 - Object Detection + Captioning = Dense Captioning Johnson, Karpathy, and Fei-Fei, “DenseCap: Fully Convolutional Localization Networks for Dense Captioning”, CVPR 2016 Figure copyright IEEE, 2016. Reproduced for educational purposes. Object Detection + Captioning = Dense Captioning Dense Video Captioning Ranjay Krishna et al., “Dense-Captioning Events in Videos”, ICCV 2017 Figure copyright IEEE, 2017. Reproduced with permission. Objects + Relationships = Scene Graphs Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen et al. \"Visual genome: Connecting language and vision using crowdsourced dense image annotations.\" International Journal of Computer Vision 123, no. 1 (2017): 32-73. Scene Graph Prediction Xu, Zhu, Choy, and Fei-Fei, “Scene Graph Generation by Iterative Message Passing”, CVPR 2017 Figure copyright IEEE, 2018. Reproduced for educational purposes. Lecture 9 - 3D Object Detection 2D Object Detection: 2D bounding box (x, y, w, h) 3D Object Detection: 3D oriented bounding box (x, y, z, w, h, l, r, p, y) Simplified bbox: no roll & pitch Much harder problem than 2D object detection! Lecture 9 - 3D Object Detection: Monocular Camera Same idea as Faster RCNN, but proposals are in 3D 3D bounding box proposal, regress 3D box parameters + class score Chen, Xiaozhi, Kaustav Kundu, Ziyu Zhang, Huimin Ma, Sanja Fidler, and Raquel Urtasun. \"Monocular 3d object detection for autonomous driving.\" CVPR 2016. Faster R-CNN 3D Shape Prediction: Mesh R-CNN Gkioxari et al., Mesh RCNN, ICCV. 2019 Projects at CSE • Bio-Health Informatics https://sites.google.com/cse. mrt.ac.lk/biohealth Projects at CSE • Eco-Sustainable Informatics https://sites.google.com/view/e co-sustainable/home Current state of the art • You just saw many examples of current systems. • Many of these are less than 5 years old • Computer vision is an active research area, and rapidly changing • Many new apps in the next 5 years • Deep learning and generative methods powering many modern applications • Many startups across a dizzying array of areas • Generative AI, robotics, autonomous vehicles, medical imaging, construction, inspection, VR/AR, ... Negative Impacts & Privacy & Ethics"
  },
  {
    "index": 7,
    "level": 1,
    "start_page": 263,
    "end_page": 300,
    "title": "7. Recurrent Neural Networks Part 1",
    "content": "Recurrent Neural Networks for Sequence Modeling Dr. Sandareka Wickramanayake Outline • Motivation • Fundamentals • Applications of RNNs • Issues with Backpropagating Gradients in RNNs Sequences • Collection of elements where: • Elements can be repeated • Order matters • Of variable (potentially infinite) length Why Care About Sequences? • “Why do we care about sequences?” • “I will go home after I finish my work.” • “Why”, “do”, “we”, “care”, “about”, “sequences” • “W”, “H”, “Y”, “D”, “O”, “W”, “E. ”, ... Sequences are Everywhere Videos “Sequences really seem to be everywhere! We should learn how to model them. What is the best way to do that? Stay tuned!” Sentences, words Speech Image Programs Medical records So Far You Have Discussed Multi-Layer Perceptrons Convolutional Neural Networks • Feed Forward Neural Networks Modeling Sequences • Collection of elements where: • Elements can be repeated • Order matters • Of variable (potentially infinite) length • Feed-forward neural networks don't do well with sequential data. • Accept single/static input samples, e.g., images Sequence Modeling • “Modeling word probabilities is really difficult” • Simplest model • 𝑝𝑀𝑜𝑑𝑒𝑙𝑖𝑛𝑔× 𝑝𝑤𝑜𝑟𝑑× 𝑝𝑝𝑟𝑜𝑏𝑎𝑏𝑖𝑙𝑖𝑡𝑖𝑒𝑠× 𝑝𝑖𝑠× 𝑝𝑟𝑒𝑎𝑙𝑙𝑦 × 𝑝𝑑𝑖𝑓𝑓𝑖𝑐𝑢𝑙𝑡 • Can output - “The the the the the” • Assume independence of words • Context is missing 𝑝𝑥= 𝑥'𝑠 𝑝𝑟𝑜𝑏𝑎𝑏𝑖𝑙𝑖𝑡𝑦 𝑖𝑛 𝑎 𝑐𝑜𝑝𝑢𝑠 Sequence Modeling • “Modeling word probabilities is really difficult” • Fixing a small context: N-grams • Only condition on N previous words • Doesn't consider words that are more than N words away • The data table is still very, very large • Modeling • Modeling word • Modeling word probabilities • Modeling word probabilities is • Modeling word probabilities is really • Modeling word probabilities is really difficult Sequence Modeling • “Modeling word probabilities is really difficult” • Fixing a small context: N-grams • Only condition on N previous words • Doesn't consider words that are more than N words away • The data table is still very, very large Recurrent Neural Networks (RNNs) ℎ𝑡= tanh(𝑾𝒉ℎ𝑡-1 + 𝑾𝒙𝑥𝑡) • Persistent state variable ℎ stores information from the context observed so far. Recurrent Neural Networks (RNNs) 𝑃(𝑦𝑡+1) = Softmax(𝑾𝒚ℎ𝑡) RNNs predict the target 𝑦 (the next word) from the state ℎ. Softmax ensures we obtain a distribution over all possible words. Recurrent Neural Networks (RNNs) Input the next word in the sentence 𝑥1. Recurrent Neural Networks (RNNs) Recurrent Neural Networks (RNNs) Recurrent Neural Networks (RNNs) 𝑦𝑡+1 RNN Weights are shared over time steps RNNs rolled out over time RNNs: Loss - Cross Entropy • Next word prediction is essentially a classification task where the number of classes is the size of the vocabulary. • As such we use the cross-entropy loss: • For one word • For one sentence • With parameters 𝜃= {𝑊𝑥, 𝑊ℎ, 𝑊𝑦} ℒ(𝑦, ො𝑦)𝑡= -𝑦𝑡log ො𝑦𝑡 ℒ𝑦, ො𝑦= -෍ 𝑡=1 𝑦𝑡log ො𝑦𝑡 𝑦𝑡+1 RNNs: Training Example • Character-level language model • Vocabulary [h, e, l, o] • Example training sequence - “hello” RNNs: Training Example • Character-level language model • Vocabulary [h, e, l, o] • Example training sequence - “hello” RNNs: Training Example • Character-level language model • Vocabulary [h, e, l, o] • Example training sequence - “hello” RNNs: Training Example • Character-level language model • Vocabulary [h, e, l, o] • Example training sequence - “hello” RNNs: Inference Example • Character-level language model • Vocabulary [h, e, l, o] • At test time sample characters one at a time, feedback to the model. RNNs: Inference Example • Character-level language model • Vocabulary [h, e, l, o] • At test time sample characters one at a time, feedback to the model. RNNs: Inference Example • Character-level language model • Vocabulary [h, e, l, o] • At test time sample characters one at a time, feedback to the model. RNNs: Inference Example • Character-level language model • Vocabulary [h, e, l, o] • At test time sample characters one at a time, feedback to the model. RNNs: Training • Given a corpus of text data (e.g., sentences), train the parameters of the RNN. • Principle: unfold the computational graph and use backpropagation. • Called Back-propagation Through Time (BPTT) algorithm. • Forward through the entire sequence to compute loss, then backward through the entire sequence to compute the gradient. RNNs: Training • Can then apply any general-purpose gradient-based techniques. • Conceptually: first compute the gradients of the internal nodes, then compute the gradients of the parameters. Recurrent Neural Networks: Process Sequences One-to-Many • One-to-Many • Produce output sequence from a single input vector. • E.g., Image captioning (Image ➔ Sequence of words) Recurrent Neural Networks: Process Sequences • Many-to-One • Encode input sequence in a single vector. • E.g., Sentiment Analysis (Sequence of words ➔ Sentiment) Many-to-One Recurrent Neural Networks: Process Sequences • Many-to-Many • Encode input sequence to an output sequence. • E.g., Machine Translation Sequence of words ➔ Sequence of words Many-to-Many Vanilla RNNs: Advantages • Possibility of processing input of any length. • Computation considers historical information. • Captures dependencies within a short range at least. • Model size not increasing with the size of the input. • Shared functions and parameters: significantly reduce the capacity and are good for generalization in learning. Vanilla RNNs: Disadvantages • Computation is slow. • Cannot consider any future input for the current state. • Vanishing gradient problem which causes difficulty accessing information from a long time ago... Finally, Tim was planning to visit France on the final week of his journey. He was quite excited to try the local delicacies and had lots of recommendations for good restaurants and exhibitions. His first stop was, of course, the capital where he would meet his long-time Friend Jean-Pierre. In order to arrive for breakfast, he took the early 5 AM train from London to ... Vanilla RNN Gradient Flow ℎ𝑡= tanh 𝑾𝒉 𝑾𝒙 ℎ𝑡-1 ℎ𝑡= tanh 𝑊ℎ𝑡-1 ℎ𝑡= tanh(𝑾𝒉ℎ𝑡-1 + 𝑾𝒙𝑥𝑡) Vanilla RNN Gradient Flow ℎ𝑡= tanh 𝑾𝒉 𝑾𝒙 ℎ𝑡-1 ℎ𝑡= tanh 𝑊ℎ𝑡-1 ℎ𝑡= tanh(𝑾𝒉ℎ𝑡-1 + 𝑾𝒙𝑥𝑡) Backpropagation from ℎ𝑡 to ℎ𝑡-1 multiplies by 𝑾 Vanilla RNN Gradient Flow Computing gradient of ℎ0 involves many factors of 𝑾 (and repeated tanh) ℎ𝑡= tanh(𝑾𝒉ℎ𝑡-1 + 𝑾𝒙𝑥𝑡) 𝑃(𝑦𝑡+1) = Softmax(𝑾𝒚ℎ𝑡) ℒ𝑦, ො𝑦= - 𝑦𝑡log ො𝑦𝑡 𝜕𝑾𝒉 = 𝜕ℒ 𝜕ො𝑦 𝜕ො𝑦 𝜕ℎ𝑡 𝜕ℎ𝑡 𝜕𝑾𝒉 𝜕𝑾𝒉 = ෍ 𝑘=1 𝑡𝜕ℒ 𝜕ො𝑦 𝜕ො𝑦 𝜕ℎ𝑡 𝜕ℎ𝑡 𝜕ℎ𝑘 𝜕ℎ𝑘 𝜕𝑾𝒉 𝜕ℎ𝑡 𝜕𝑾𝒉 = ෍ 𝑘=1 𝑡𝜕ℎ𝑡 𝜕ℎ𝑘 𝜕ℎ𝑘 𝜕𝑾𝒉 Vanilla RNN Gradient Flow Computing gradient of ℎ0 involves many factors of 𝑾 (and repeated tanh) Largest singular value > 1: Exploding gradients Largest singular value < 1: Vanishing gradients Gradient Issues of RNNs - Solutions • Exploding Gradient • Truncated Backpropagation - Stop backpropagating after a certain point, which is usually not optimal because not all of the weights get updated. • Run forward and backward through chunks of the sequence instead of the whole sequence. • Carry hidden states forward in time forever, but only backpropagate for some smaller number of steps. • Penalize or artificially reduce the gradient. • Gradient Clipping - Put a maximum limit on a gradient. Gradient Issues of RNNs - Solutions • Vanishing Gradient • Weight Initialization - Initialize weights so that the potential for vanishing gradient is minimized • Long Short-Term Memory Network (LSTM)"
  },
  {
    "index": 8,
    "level": 1,
    "start_page": 301,
    "end_page": 333,
    "title": "8. Recurrent Neural Networks Part 2",
    "content": "Recurrent Neural Networks for Sequence Modeling II Dr. Sandareka Wickramanayake Outline • RNN - Recap • LSTM • LSTM. Variants • Beam Search Acknowledgement • Understanding LSTM Networks by Cristopher Olah (https://colah.github.io/posts/2015-08-Understanding-LSTMs/) • How LSTM networks solve the problem of vanishing gradients by Nir Arbel (https://medium.datadriveninvestor.com/how-do- lstm-networks-solve-the-problem-of-vanishing-gradients- a6784971a577) RNN - Recap • Sequential data analysis must consider past inputs along with the current input. • Looking into the infinite past requires recursion. • RNNs retain information about the past through recurrent hidden states. RNN - Recap 𝑃(𝑦𝑡+1) = Softmax(𝑾𝒚ℎ𝑡) ℎ𝑡= tanh(𝑾𝒉ℎ𝑡-1 + 𝑾𝒙𝑥𝑡) ℒ𝑦, ො𝑦= -෍ 𝑡=1 𝑦𝑡log ො𝑦𝑡 Task - Text generation • ℎ𝑡= the state of the network State summarizes information from the entire past. Need to define initial state ℎ0 RNN - Recap RNNs • ℎ𝑡= the state of the network All columns are identical. An input at 𝒕= 𝟏 affects the outputs forever. The state can be arbitrarily complex. 𝒕= 𝟏 Time RNNs The state can be arbitrarily complex. 𝒕= 𝟏 Time 𝒕= 𝟏 Time RNNs Variants One-to-Many Many-to-One Many-to-Many RNNs In Class Activity Bi-Directional RNNs • In many applications output at time 𝑡 may depend on the whole input sequence • E.g., Speech recognition: correct interpretation of the current sound may depend on the next few phonemes, potentially even the next few words • Bidirectional RNNs are introduced to address this • Bidirectional RNNs - RNN with both forward and backward recursion • Explicitly models the fact that just as the future can be predicted from the past, the past can be deduced from the future RNNs Gradient Issues ℎ𝑡= tanh(𝑾𝒉ℎ𝑡-1 + 𝑾𝒙𝑥𝑡) 𝑃(𝑦𝑡+1) = Softmax(𝑾𝒚ℎ𝑡) ℒ𝑦, ො𝑦= - 𝑦𝑡log ො𝑦𝑡 ℎ𝑡= tanh 𝑾𝒉 𝑾𝒙 ℎ𝑡-1 ℎ𝑡= tanh 𝑾ℎ𝑡-1 RNNs Gradient Issues 𝜕𝑾= ෍ 𝑘=1 𝑇𝜕ℒ𝑘 𝜕ℒ𝑘 𝜕𝑾= 𝜕ℒ𝑘 𝜕ℎ𝑘 𝜕ℎ𝑘 𝜕ℎ𝑘-1 ... 𝜕ℎ2 𝜕ℎ1 𝜕ℎ1 𝜕ℒ𝑘 𝜕𝑾= 𝜕ℒ𝑘 𝜕ℎ𝑘 𝑡=2 𝜕ℎ𝑡 𝜕ℎ𝑡-1 𝜕ℎ1 RNNs Gradient Issues Since ℎ𝑡= tanh(𝑾𝒉ℎ𝑡-1 + 𝑾𝒙𝑥𝑡) 𝜕ℒ𝑘 𝜕𝑾= 𝜕ℒ𝑘 𝜕ℎ𝑘 𝑡=2 𝑡𝑎𝑛ℎ-1(𝑾𝒉ℎ𝑡-1 + 𝑾𝒙𝑥𝑡)𝑾𝒉 𝜕ℎ1 Causes for vanishing or exploding gradients. 𝑾= 𝑾 -𝛼𝜕ℒ Long Short-Term Memory (LSTM) • Vanilla RNN operates in a “multiplicative” way (repeated tanh). • Two recurrent cell designs were proposed and widely adopted: • Long Short-Term Memory (LSTM) • Gated Recurrent Unit (GRU) • Both designs process information in an “additive” way with gates to control information flow. • Sigmoid gate outputs numbers between 0 and 1, describing how much of each component should be let through. The repeating module in a standard RNN contains a single layer. The repeating module in an LSTM contains four interacting layers. LSTM • The key to LSTMs is the cell state. • Stores information of the past → long-term memory • Passes along time steps with minor linear interactions → “additive” • Results in an uninterrupted gradient flow → errors in the past pertain and impact learning in the future Gradient Flow LSTM • The LSTM can remove or add information to the cell state, which is achieved using gates. • Gates - A way to optionally let information through. • Input gate → controls the intake of new information • Forget gate → determines what part of the cell state to be updated • Output gate → determines what part of the cell state to output 𝑆𝑖𝑔𝑚𝑜𝑖𝑑𝜎= 1 + 𝑒-𝑧 Gate Step-by-step LSTM Walk Through • Saman studies at UOM. His sister studies at UOC. She studies law. • Step 1 - Decide what information we're going to throw away from the cell state. • Forget gate • Looks at ℎ𝑡-1 and 𝑥𝑡 and outputs a number between 0 and 1 for each number in 𝐶𝑡-1. A 1 represents “completely keep this” while a 0 represents “completely get rid of this.” • E.g., When LSTM sees “sister” it needs to forget about “Saman”. Step-by-step LSTM Walk Through • Saman studies at UOM. His sister studies at UOC. She studies law. • Step 2 - Decide what new information we're going to store in the cell state. • Input gate • Decides which values we'll update based on ℎ𝑡-1 and 𝑥𝑡. • Tanh gate • A vector of new candidate values, ሚ𝐶𝑡, that could be added to the state. • E.g., LSTM needs to add the gender of the “sister” to replace the old one it is forgetting. Step-by-step LSTM Walk Through • Saman studies at UOM. His sister studies at UOC. She studies law. • Step 3 - Update the old cell state • The new cell state 𝐶𝑡 is comprised of information from the past 𝑓𝑡∗𝐶𝑡-1 and valuable new information 𝑖𝑡∗ ෩𝐶𝑡. • E.g., LSTM drops the information about “Saman's” gender and adds new information about “sister 's” gender. Step-by-step LSTM Walk Through • Saman is studying at UOM. His sister is studying at UOC. She studies law. • Step 3 - Update the hidden state • New hidden state, ℎ𝑡= 𝑜𝑡 ∗tanh(𝐶𝑡) • tanh function filters the new cell state to characterize stored information • Significant information in 𝐶𝑡 → ±1 • Minor details → 0 • E.g., Since LSTM just saw “sister”, it wants to output whether the subject is singular or plural. Gradient Calculation in LSTM 𝜕𝑾= ෍ 𝑘=1 𝑇𝜕ℒ𝑘 𝜕ℒ𝑘 𝜕𝑾= 𝜕ℒ𝑘 𝜕ℎ𝑘 𝜕ℎ𝑘 𝜕𝐶𝑘 𝑡=2 𝜕𝐶𝑡 𝜕𝐶𝑡-1 𝜕𝐶1 𝐶𝑡= 𝐶𝑡-1 ⊗𝑓𝑡 ⊕ ሚ𝐶𝑡⊗ 𝑖𝑡 𝜕𝐶𝑡 𝜕𝐶𝑡-1 = 𝐴𝑡 + 𝐵𝑡 + 𝐷𝑡 + 𝐸𝑡 Variants of LSTM • Gated Recurrent Unit (GRU) • Combines the forget and input gates into a single “update gate.” • Merges the cell state and hidden state. • Simpler than LSTM models. • Similar performance compared to LSTM with fewer parameters and faster convergence. Variants of LSTM • Gated Recurrent Unit (GRU) • 𝑧𝑡 (Update gate) - Controls the composition of the new state • 𝑟𝑡 (Reset gate) - Determines how much old information is needed in the alternative state ෩ℎ𝑡 • ෨ℎ𝑡 (Alternative state) - Contains new information • ℎ𝑡 (New state) - Replace selected old information with new information in the new state Beam Search Vocabulary <Start> Arrived The Green Witch Mage Who Came END Attention • What is a discrete environment? • The man in the red jacket takes a .............. dog for a walk. Focus on a Specific Part! The Problem of Longer Sequences Hello World Hidden State Bonjour monde <eos> Jane went to Africa last September and enjoyed the culture and met many wonderful people; she came back raving about how wonderful her trip was and is tempting me to go too. Jane est allée en Afrique en septembre dernier et a apprécié la culture et a rencontré de nombreuses personnes formidables ; elle est revenue ravie de la beauté de son voyage et me donne envie d'y aller aussi. RNN - Tries to memorize the whole English sentence and then translate it into French. Human translator - Translate one chuck of the sentence at a time. Works for short sentences but not for longer sentences.! Attention Jane est allée Hidden State Jane Afrique septembre aussi 𝛼𝑛1 Context of the first word 0 ≤ 𝛼𝑖 𝑗 ≤1 𝑗= 1 𝐶𝑗= ෍ 𝑗 × ℎ𝑖 𝑗 = Amount of attention 𝑦𝑗 should pay to ℎ𝑖 Attention Jane est allée Afrique septembre aussi 𝛼𝑛1 Jane went 𝛼𝑛2 Context of the second word Hidden State 𝑗= 1 𝐶𝑗= ෍ 𝑗 × ℎ𝑖 𝑗 = Amount of attention 𝑦𝑗 should pay to ℎ𝑖 𝛼𝑛1 Attention Jane est allée Afrique septembre aussi 𝛼𝑛1 Jane went 𝛼𝑛2 Hidden State 𝛼𝑛3 𝑗= 1 𝐶𝑗= ෍ 𝑗 × ℎ𝑖 𝑗 = Amount of attention 𝑦𝑗 should pay to ℎ𝑖 Computing Attention 𝛼𝑖 • 𝛼𝑖 𝑗 = Amount of attention 𝑦𝑗 should pay to ℎ𝑖 • 𝛼𝑖 exp(𝑧𝑖 σ𝑖exp(𝑧𝑖 𝑆𝑗-1 A small neural network, E.g., One hidden layer MLP Attention Visualization Attention for Image Captioning Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, ICML 2015 Problems of LSTM • Cannot handle very long sequences. • Cannot train in parallel • Solution - Transformers"
  },
  {
    "index": 9,
    "level": 1,
    "start_page": 334,
    "end_page": 378,
    "title": "9. Generative Models",
    "content": "Generative Models. Dr. Sandareka Wickramanayake Outline • Supervised vs Unsupervised Learning • Discriminative vs Generative Models • Autoencoders • Variational Autoencoders • Generative Adversarial Models Supervised vs Unsupervised Learning • Supervised Learning • Dataset (x,y) - x is data and y is label • Goal - Learn a function to map x into y (f(x) = y) • Examples - Classification, Regression, Object detection, Semantic segmentation, Image captioning Cat Classification Object Detection Supervised vs Unsupervised Learning • Supervised Learning • Dataset (x,y) - x is data and y is label • Goal - Learn a function to map x into y (f(x) = y) • Examples - Classification, Regression, Object detection, Semantic segmentation, Image captioning Semantic Segmentation Image Captioning A brown cat is on the tree. Supervised vs Unsupervised Learning • Unsupervised Learning • Dataset (x) - Just data, no labels • Goal - Learn some underlying hidden structure of data • Examples - Clustering, Dimensionality reduction, Density estimation Supervised vs Unsupervised Learning • Unsupervised Learning • Dataset (x) - Just data, no labels • Goal - Learn some underlying hidden structure of data • Examples - Clustering, Dimensionality reduction, Density estimation • Density estimation • The problem of reconstructing the probability density function using a set of given data points. • Suppose, we observe 𝑥1, ... , 𝑥𝑛. We want to recover the underlying probability density function generating our dataset. Supervised vs Unsupervised Learning • Unsupervised Learning • Dataset (x) - Just data, no labels • Goal - Learn some underlying hidden structure of data • Examples - Clustering, Dimensionality reduction, Density estimation • Density estimation • The problem of reconstructing the probability density function using a set of given data points. • Suppose, we observe 𝑥1, ... , 𝑥𝑛. We want to recover the underlying probability density function generating our dataset. Supervised vs Unsupervised Learning • Unsupervised Learning • Dataset (x) - Just data, no labels • Goal - Learn some underlying hidden structure of data • Examples - Clustering, Dimensionality reduction, Density estimation • Supervised Learning • Dataset (x,y) - x is data and y is label • Goal - Learn a function to map x into y (f(x) = y) • Examples - Classification, Regression, Object detection, Semantic segmentation, Image captioning Generative vs Discriminative Models Discriminative Model Generative Model Generative vs Discriminative Models • Logistic regression • Support Vector Machine • Traditional neural networks • Nearest neighbor • Conditional Random Fields • DNNs (e.g., CNNs, RNNs) • Naïve Bayes • Bayesian networks • Markov random fields • Hidden Markov Models Discriminative Model Generative Model Generative Modeling • Given training data, generate new samples from the same distribution. 𝑷𝒎𝒐𝒅𝒆𝒍(𝒙) Learning Sampling Training data ~ 𝑷𝒅𝒂𝒕𝒂(𝒙) Objectives: 1. Learn 𝑷𝒎𝒐𝒅𝒆𝒍𝒙 that approximates 𝑷𝒅𝒂𝒕𝒂(𝒙). 2. Sampling new 𝒙 from 𝑷𝒎𝒐𝒅𝒆𝒍(𝒙). Generative Modeling • Given training data, generate new samples from the same distribution. 𝑷𝒎𝒐𝒅𝒆𝒍(𝒙) Learning Sampling Training data ~ 𝑷𝒅𝒂𝒕𝒂(𝒙) Formulate as density estimation problems: - Explicit density estimation: explicitly define and solve for 𝑷𝒎𝒐𝒅𝒆𝒍(𝒙). - Implicit density estimation: learn model that can sample from 𝑷𝒎𝒐𝒅𝒆𝒍(𝒙). without explicitly defining it. Why Generative Models? • Realistic samples for artwork, super- resolution, colorization, etc. • Learn useful features for downstream tasks such as classification. • Getting insights from high-dimensional data (physics, medical imaging, etc.) • Modeling the physical world for simulation and planning (robotics and reinforcement learning applications) • Many more ... We Discuss Two Generative Models. • Variational Autoencoder (Explicit density) • Generative Adversarial Networks (Implicit density) We Discuss Two Generative Models. • Variational Autoencoder • Generative Adversarial Networks Autoencoders • Unsupervised approach for learning a lower-dimensional feature representation from unlabeled training data. Input data Latent Features Reconstructed Input data Decoder Encoder 𝑧 is usually smaller than 𝑥 Want features to capture meaningful factors of variation in data. Autoencoder is not a generative model! Autoencoders • Unsupervised approach for learning a lower-dimensional feature representation from unlabeled training data. Input data Latent Features Reconstructed Input data Decoder Encoder Originally: Linear + nonlinearity (sigmoid) Later: Deep fully-connected Later: ReLU CNN (Up convolution) Originally: Linear + nonlinearity (sigmoid) Later: Deep fully-connected Later: ReLU CNN Autoencoders • Unsupervised approach for learning a lower-dimensional feature representation from unlabeled training data. Input data Latent Features Reconstructed Input data Decoder Encoder • Train such that features can be used to reconstruct original data. • L2 Loss Function - 𝑥-ො𝑥2 • Doesn't use labels. Autoencoders • Unsupervised approach for learning a lower-dimensional feature representation from unlabeled training data. Source: https://ml.berkeley.edu/blog/posts/vq-vae/ Autoencoders • Impact of the latent space dimension Kingma, et al., 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114. Autoencoders - Image Segmentation Noh, et al. \"Learning deconvolution network for semantic segmentation.\" ICCV. 2015. Autoencoders • Denoising autoencoders Very good at removing noise. Source: https://www.andreaamico.eu/unsup- learning/2019/06/24/autoencoder.html Loss function Autoencoders • After training throw away the decoder. ➔ The encoder can be used to initialize a supervised model. • Transfer from a large and unlabeled dataset to a small labeled dataset. Input data Latent Features Fine-tune the encoder jointly with the classifier layer. Autoencoders • Autoencoders can reconstruct data and can learn features to initialize a supervised model. • Features capture factors of variation in training data. • But we can't generate new images from an autoencoder because we don't know the space of z. • How do we make autoencoder a generative model? Input data Latent Features Reconstructed Input data Variational Autoencoders [0,2] Variational Autoencoders • Assume training data 𝑥(𝑖) 𝑖=1 𝑁 is generated from the distribution of unobserved (latent) representation 𝑧. Sample 𝑧 from 𝑧|𝑥 ~ ℵ(𝜇, 𝜎) Decoder network 𝑃𝜃 (𝑥|𝑧) Encoder network 𝑄∅ (𝑧|𝑥) Maximize the likelihood of the original input being reconstructed. Make approximate posterior distribution close to prior. Sampled latent vector Variational Autoencoders • Reparameterization trick • 𝑧= 𝜇+ 𝜎⨀𝜀 where 𝜀 ~ Normal(0,1) Sample 𝑧 from 𝑧|𝑥 ~ ℵ(𝜇, 𝜎) Decoder network 𝑃𝜃 (𝑥|𝑧) Encoder network 𝑄∅ (𝑧|𝑥) Sampled latent vector Variational Autoencoders - Code Example Variational Autoencoders - Generated Images Degree of Smile Head Pose Disentangled Variational Autoencoders Higgins, Irina, et al. \"Early visual concept learning with unsupervised deep learning.“ ICLR 2017. Disentangled Variational Autoencoders Higgins, Irina, et al. \"Early visual concept learning with unsupervised deep learning.“ ICLR 2017. Generative Adversarial Networks (GANs) • Problem: • Want to sample from complex, high-dimensional training distribution? No direct way to do this! • Solution: • Sample from a simple distribution we can easily sample from, e.g., random noise. • Learn transformation to training distribution. • But what can we use to represent this complex transformation? • A Neural Network Generator Network Input: Random noise Output: Sample from the training distribution Generative Adversarial Networks (GANs) • Problem: • Want to sample from complex, high-dimensional training distribution? No direct way to do this! • Solution: • Sample from a simple distribution we can easily sample from, e.g., random noise. • Learn transformation to training distribution. • Problem : • But we don't know which sample z maps to which training image (can't learn by reconstructing training images). • Solution: • Use a Discriminator Network to tell whether the generated image is within data distribution or not. Generator Network Input: Random noise Output: Sample from the training distribution Discriminator Network Real/Fake Gradient Training GANs: Two-player game • Discriminator network: try to distinguish between real and fake images • Generator network: try to fool the discriminator by generating real- looking images Generator Network Input: Random noise Fake Images (from the generator) Discriminator Network Real/Fake Real Images (from the training set) Training GANs: Two-player game • Discriminator network: try to distinguish between real and fake images • Generator network: try to fool the discriminator by generating real- looking images. • Train jointly in the minimax game. Discriminator output for real data x Discriminator output for Generated fake data G(z) Discriminator outputs likelihood in (0,1) of real image Training GANs: Two-player game • Discriminator network: try to distinguish between real and fake images • Generator network: try to fool the discriminator by generating real- looking images. • Train jointly in the minimax game. • Discriminator wants to maximize objective such that D(x) is close to 1 (real) and D(G(z)) is close to 0 (fake). • Generator wants to minimize objective such that D(G(z)) is close to 1 (discriminator is fooled into thinking generated G(z) is real). Training GANs: Two-player game Training GANs: Two-player game Generator Objective Training GANs: Two-player game • Discriminator network: try to distinguish between real and fake images • Generator network: try to fool the discriminator by generating real- looking images Generator Network Input: Random noise Fake Images (from the generator) Discriminator Network Real/Fake Real Images (from the training set) After training, use the generator network to generate new images. GANs: Generated Samples Goodfellow, Ian, et al. \"Generative adversarial networks.\" Communications of the ACM, 2020. The nearest neighbor from the training set. These networks consisted of fully connected layers GANs: Convolutional Architectures • Generator is an upsampling network with fractionally-strided convolutions • Discriminator is a convolutional network. • Architecture guidelines for stable Deep Convolutional GANs • Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator). • Use batchnorm in both the generator and the discriminator. • Remove fully connected hidden layers for deeper architectures. • Use ReLU activation in the generator for all layers except for the output, which uses Tanh. • Use LeakyReLU activation in the discriminator for all layers. Radford et al. \"Unsupervised representation learning with deep convolutional generative adversarial networks.“ ICLR 2015. GANs: Convolutional Architectures Radford et al. \"Unsupervised representation learning with deep convolutional generative adversarial networks.“ ICLR 2015. GANs: Convolutional Architectures Radford et al. \"Unsupervised representation learning with deep convolutional generative adversarial networks.“ ICLR 2015. GANs: Convolutional Architectures Radford et al. \"Unsupervised representation learning with deep convolutional generative adversarial networks.“ ICLR 2015. Samples from the model Average Z vectors, do arithmetic Progressive GANs “PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION”, ICLR. 2018"
  },
  {
    "index": 10,
    "level": 1,
    "start_page": 379,
    "end_page": 394,
    "title": "10. Spiking Neural Networks",
    "content": "Spiking Neural Network Dr. Sandareka Wickramanayake Outline • Biological Neurons • Spiking Neural Networks. (SNNs) • How does SNN work? • SNN neuron models • SNN architectures • How to train an SNN? • Deep Learning in SNNs • Advantages and disadvantages of SNN Biological Neuron • The elementary unit of the brain. • Axon - Output of a neuron • Dendrite - Input • Synapse - Connection (The output of one neuron is connected to the input of another neuron.) • When a spike comes (an input comes) the potential of the neuron increases/decreases. • When the potential reaches a threshold, a spike is produced and the neuron potential drops. Biological Neuron Source - https://epsych.msstate.edu/biological/neuron /Part2/20.html Ions https://faculty.washington.edu/ Biological Neuron • The neuron fires when its potential reaches a threshold. • After an output spike, the potential falls below the resting potential. • After an output spike, channels remain open for a refractory period; during this time, input spikes have a very small impact on potential. • A single neuron may connect to >10K other neurons; ~100 billion neurons in human brains; ~500 trillion synapses. https://teachmephysiology.com/nervous- system/synapses/action-potential/ Biological Neurons and Artificial Neurons are Different! • Fundamentally different in: • General structure • Neural computations • Learning rule compared to the brain Meng et al., “Using a Data-Driven Approach to Predict Waves Generated by Gravity-Driven Mass Flows”, 2020 Spiking Neural Networks (SNNs) • Closely mimic the biological neurons • AN - operates with values changing continuously with time • SN - operates with discrete events that occur at certain points in time. • SN receives a series of spikes as input and produces a series of spikes as the output. • A series of spikes - Spike trains. Deng et al. Neural Networks, 2020 Spiking Neural Networks (SNNs) How does an SNN work? • At each point in time each neuron has some value. • The value in a neuron can change based on the mathematical model of a neuron • e.g, if a neuron receives a spike from the upstream neuron, the value might increase or decrease. • If the value in a neuron exceeds some threshold, the neuron will send a single impulse to each downstream neuron connected to the initial one. • After this, the value of the neuron will instantly drop below its average. • Over time the value of the neuron will smoothly return to its average. How to Model a Neuron? https://snntorch.readthedocs.io/en/latest/tutorials/tutorial_2.html How to Model a Neuron? • Leaky Integrate-and-Fire (LIF) • The most popular model. • When input spikes show up, the potential is incremented/decremented based on the synaptic weight for that input. • When inputs are absent, a leak is subtracted. • The increments/decrements are step functions (unlike the smooth curves in this figure). SNN Architectures • Feedforward NN • Data is transmitted strictly in one direction • Recurrent NN • Connections between neurons form a directed graph along a temporal sequence. • Hybrid NN • Consists of both feedforward and recurrent connections. How to Train an SNN • Still a research problem • Training SNN might be a difficult task • Not differentiable → Cannot use gradient descent. • Unsupervised • Spike-timing-dependent plasticity (STDP) • Growing Spiking Neural Networks • Supervised • SpikeProp • Remote Supervised Method (ReSuMe) How to Train an SNN • Spike-Timing-Dependent Plasticity (STDP) Rule • Initially, all synaptic connections have equal weights. • The magnitude of change in synaptic weight • Depends on the timing of spikes. • If the presynaptic spike arrives at the postsynaptic neuron before the postsynaptic neuron fires (it causes the firing), the synapse is potentiated. • If the presynaptic spike arrives at the postsynaptic neuron after it is fired, the synapse is depressed. Deep Learning with SNN • Theoretically possible • Spiking Deep Neural Networks are not as good as traditional Deep Neural Networks Advantages and Disadvantages of SNNs • Advantages • SNN is dynamic • An SNN can still train when it is already working; • Better for speech and dynamic image recognition. • SNNs usually have fewer neurons than the traditional ANNs; • SNNs can work very fast since the neurons will send impulses not a continuous value; • SNNs have increased the productivity of information processing and noise immunity since they use the temporal presentation of information. • Disadvantages • Hard to train. • Performance is not comparable to ANNs YET."
  },
  {
    "index": 11,
    "level": 1,
    "start_page": 395,
    "end_page": 423,
    "title": "11. Transformers",
    "content": "Transformers. Dr Sandareka Wickramanayake sandarekaw@cse.mrt.ac.lk Acknowledgement • Attention is All You Need • How Transformers Work: A Detailed Exploration of Transformer Architecture RNN - Recap • Sequential data analysis must consider past inputs along with the current input. • Looking into the infinite past requires recursion. • RNNs retain information about the past through recurrent hidden states. Encoder-Decoder Architecture • Creating context and using the final context of the first RNN. Attention • What is a discrete environment? • The man in the red jacket takes a .............. dog for a walk. Focus on a Specific Part! The Problem of Longer Sequences Hello World Hidden State Bonjour monde <eos> Jane went to Africa last September and enjoyed the culture and met many wonderful people; she came back raving about how wonderful her trip was and is tempting me to go too. Jane est allée en Afrique en septembre dernier et a apprécié la culture et a rencontré de nombreuses personnes formidables ; elle est revenue ravie de la beauté de son voyage et me donne envie d'y aller aussi. RNN - Tries to memorize the whole English sentence and then translate it into French. Human translator - Translate one chuck of the sentence at a time. Works for short sentences but not for longer sentences.! Attention Jane est allée Hidden State Jane Afrique septembre aussi 𝛼𝑛1 Context of the first word 0 ≤ 𝛼𝑖 𝑗 ≤1 𝑗= 1 𝐶𝑗= ෍ 𝑗 × ℎ𝑖 𝑗 = Amount of attention 𝑦𝑗 should pay to ℎ𝑖 Attention Jane est allée Afrique septembre aussi 𝛼𝑛1 Jane went 𝛼𝑛2 Context of the second word Hidden State 𝑗= 1 𝐶𝑗= ෍ 𝑗 × ℎ𝑖 𝑗 = Amount of attention 𝑦𝑗 should pay to ℎ𝑖 𝛼𝑛1 Attention Jane est allée Afrique septembre aussi 𝛼𝑛1 Jane went 𝛼𝑛2 Hidden State 𝛼𝑛3 𝑗= 1 𝐶𝑗= ෍ 𝑗 × ℎ𝑖 𝑗 = Amount of attention 𝑦𝑗 should pay to ℎ𝑖 Computing Attention 𝛼𝑖 • 𝛼𝑖 𝑗 = Amount of attention 𝑦𝑗 should pay to ℎ𝑖 • 𝛼𝑖 exp(𝑧𝑖 σ𝑖exp(𝑧𝑖 𝑆𝑗-1 A small neural network, E.g., One hidden layer MLP Attention Visualization Transformers • This architecture aims to solve sequence-to- sequence tasks while handling long-range dependencies with ease. • It relies entirely on self-attention to compute representations of its input and output WITHOUT. using sequence-aligned RNNs or convolution • Hence, transformers can be more efficient to implement at scale. Transformers • Transformers are made up of stacks of transformer blocks, each of which is a multilayer network made by combining: • Self-attention layers • Linear layers • Feedforward Networks Self-Attention • Models relationships between all words in a sentence, regardless of their respective position. • Allows extracting information from arbitrarily large contexts without the need to pass it through intermediate recurrent connections as in RNNs • Parallel computation • Adhere to long-term dependencies • Creates a rich representation which captures the interdependencies between the words. Self-Attention • Computation of 𝑦2 is based on a set of comparisons between the input 𝑥2 and other elements 𝑥1 and 𝑥3 and to 𝑥2 itself. • The steps of Self-Attention calculation • Compute the 𝑠𝑐𝑜𝑟𝑒𝑥𝑖, 𝑥𝑗= 𝑥𝑖. 𝑥𝑗 ∀ 𝑗 • Apply SoftMax Normalization to create a vector of weights, 𝛼𝑖𝑗 • Calculate 𝑦𝑖= σ𝑗𝛼𝑖𝑗𝑥𝑗 Self-Attention Layer Self-Attention in Transformers • Each input plays three different roles in the attention process • Query: The current focus of attention when being compared to all the other input elements • Key: An input element being compared to the current focus of attention • Value: As used to compute the output for the current focus of attention Self-Attention Layer Self-Attention in Transformers • Each input plays three different roles in the attention process • Query: The current focus of attention when being compared to all the other input elements • Key: An input element being compared to the current focus of attention • Value: As used to compute the output for the current focus of attention • To capture these three different roles, Transformers introduce weight matrices 𝑊𝑄, 𝑊𝐾 and 𝑊𝑉 where 𝑊𝑄, 𝑊𝐾, 𝑊𝑉 ∈ 𝑅𝑑×𝑑 • Query : 𝑞𝑖= 𝑊𝑄𝑥𝑖, Key : 𝑘𝑖= 𝑊𝐾𝑥𝑖, Value : 𝑣𝑖= 𝑊𝑉𝑥𝑖 • 𝑠𝑐𝑜𝑟𝑒𝑥𝑖, 𝑥𝑗= 𝑞𝑖. 𝑘𝑗 • 𝑠𝑐𝑜𝑟𝑒𝑥𝑖, 𝑥𝑗= 𝑞𝑖.𝑘𝑗 Self-Attention in Transformers 1. Generate key, query, and value vectors 2. Compare Key/Query pairs 3. Apply SoftMax 4. Weight and sum Value vectors Self-Attention in Transformers • Transformer block consists of • Self-Attention Layer • Feedforward Layer • Residual Connections • Normalizing Layers Transformer Block Transformers Blocks Self-Attention Layer Layer Normalize Feedforward Layer Layer Normalize Residual Connection Residual Connection • Hard for a single transformer block to capture all kinds of parallel relations among the input words. • Solution - Multi-head self- attention layers • Sets of self-attention layers, called heads, that reside in parallel layers at the same depth in a model, each with its own set of parameters Multi-head Attention Multi-head Attention Layer 𝑄, 𝑊1 𝐾, 𝑊1 𝑉 Head 1 𝑄, 𝑊2 𝐾, 𝑊2 𝑉 Head 2 𝑄, 𝑊3 𝐾, 𝑊3 𝑉 Head 3 𝑄, 𝑊4 𝐾, 𝑊4 𝑉 Head 4 Head 1 Head 2 Head 3 Head 4 Concatenate outputs Embed to 𝑑 • Given these distinct sets of parameters, each head can learn different aspects of the relationships that exist among inputs at the same level of abstraction Multi-head Attention Multi-head Attention Layer 𝑄, 𝑊1 𝐾, 𝑊1 𝑉 Head 1 𝑄, 𝑊2 𝐾, 𝑊2 𝑉 Head 2 𝑄, 𝑊3 𝐾, 𝑊3 𝑉 Head 3 𝑄, 𝑊4 𝐾, 𝑊4 𝑉 Head 4 Head 1 Head 2 Head 3 Head 4 Concatenate outputs Embed to 𝑑 Positional Embeddings • How does a transformer model the position of each token in the input sequence? • Positional Embedding • Modify the input embeddings by combining them with positional embeddings specific to each position in an input sequence Positional Embeddings • How do we get them? • Start with randomly initialized embeddings corresponding to each • possible input position up to some maximum length • Learn these positional embeddings during training • To produce an input embedding that captures positional information, we add the word embedding for each input to its corresponding positional embedding ҧ𝑥1 ҧ𝑥2 ҧ𝑥𝑛 Transformer Blocks Position Embedding Word Embedding Composite Embedding Transformer - Encoder • An encoder block consists of • A Multi-head Attention Layer • A Feedforward Layer • Residual Connections • Normalizing Layers • Generates text sequences • A decoder block consists of • 2 Multi-head Attention Layers • A Feedforward Layer • Residual Connections • Normalizing Layers • Operates in an autoregressive manner Transformer - Decoder Models Based on Transformers • Bidirectional Encoder Representations from Transformers (BERT) • Compute contextualized representations of the tokens in an input sequence that are generally useful across a range of downstream applications. • Encoder-only transformer model Models Based on Transformers • Bidirectional Encoder Representations from Transformers (BERT) • Pre-training and fine-tuning • Pre-training: BERT is pre-trained on large text corpora (e.g., Wikipedia, BookCorpus) using two unsupervised tasks: • Masked Language Model (MLM): Randomly masks words in a sentence, and the model learns to predict the masked words. • Next Sentence Prediction (NSP): Learns the relationship between sentence pairs by predicting whether a given sentence follows the first one. • Fine-tuning: BERT can be fine-tuned on specific downstream tasks by adding task-specific layers and training on small task-specific datasets after pre-training. • BERT-based models - RoBERTa, ALBERT, XLNET,. ColBERT Models Based on Transformers • Generative Pre-trained Transformer (GPT) • A series of language generation models developed by OpenAI. • Decoder-only transformer model • Unidirectional and Auto-regressive modelling: GPT generates text by predicting one word at a time, conditioning only on previous words. • Pre-training & Fine-tuning: • Pre-training: Pre-trained on massive text datasets unsupervised, learning to predict the next word in the sequence. • Fine-tuning: After pre-training, GPT can be fine-tuned on specific tasks (e.g., question answering, summarization) with task-specific data. • Applications: Text generation, chatbots, language translation, and summarization. Models Based on Transformers • Text-to-Text Transformer (T5) • Reframes every NLP task as a text-to-text task (inputs and outputs are text) • Enables a single model to handle a wide range of tasks. • Uses an MLM task for pre-training but is fine-tuned in a text-to-text fashion. • T5 is pre-trained on a massive dataset, ensuring it captures diverse and complex language patterns. • Applications • Summarization • Translation • Text classification • Question answering."
  },
  {
    "index": 12,
    "level": 1,
    "start_page": 424,
    "end_page": 428,
    "title": "12. Limitations of Deep Learning and Future Directions",
    "content": "Limitations of Deep Learning and Future Directions. Dr. Sandareka Wickramanayake Deep Neural Networks have Limitations! • Despite the impressive performance, Deep Neural Networks have many limitations. • Some of the prominent limitations are • Lack explainability • Require a large amount of data • Prone to adversarial attacks • Require high computational resources • Privacy issues Limitations in Deep Learning - Group Activity • Introduction • You should do this assignment in the groups you formed in the first lecture of sequential modelling. • Each group works on a specific topic. • The topics and links to related resources are in the Moodle document “Limitations in DL-Group Activity-Details.” • Your tasks • Prepare a short presentation for the topic assigned to you. - This week • Answer a set of questions (A quiz). - Next week Limitations in Deep Learning - Group Activity • Presentation • Create a Google presentation - A maximum of 10 slides. • Should include the following • Introduction to the problem • Why the problem is important (The implications of the problem) • What are the existing solutions • One group member should submit the PDF version to Moodle by the deadline. • This is the version that will be graded. • Also, include a view-only link to your presentation in the given Google document in the following format. • Group_<Number>-<Topic>-<Link to the Google presentation> • Deadline for presentation submission - 24th Oct 2024, 11.59 PM Limitations in Deep Learning - Group Activity • Quiz • You are required to answer the quiz in the next class. • Refer to the presentations submitted by other groups and resources given, discuss the answers in the group and formulate the answers. • One member of the group should submit the answers. • Deadline - 30th Oct 2024, 3.15 - 5.15 PM."
  }
]