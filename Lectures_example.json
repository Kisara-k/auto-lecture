[
  {
    "index": 0,
    "level": 1,
    "start_page": 1,
    "end_page": 44,
    "title": "00 Introduction",
    "content": "CS3621: Data Mining Introduction. Slides based on Data Mining: Concepts and Techniques, 3rd ed. Jiawei Han, Micheline Kamber, and Jian Pei University of Illinois at Urbana-Champaign & Simon Fraser University ©2011 Han, Kamber & Pei. All rights reserved. Course Outline Module Code CS 3621 Module Title Data Mining Credits 3.0 Hours/ Week Lectures Pre - requisites Introduction to Data Science GPA/NGPA GPA. Lab/Assignm ents •ILO1: recall the basic fundamental concepts involved in the process of discovering useful, possibly unexpected, patterns in large data sets •ILO2: select appropriate techniques for data mining, considering the given problem •ILO3: analyse the computational challenges in implementing modern data mining systems. •ILO4: evaluate and implement appropriate solution for a given data mining problem from a wide array of application domains and communicate. Outline Syllabus T1. Introduction to Data Mining and Applications [ILO1] T2. Pattern Mining [ILO 1,2,3] T3. Time Series Mining and Forecasting [ILO 1,2,3] T4. Clustering and Cluster Evaluation [ILO 1,2,3] T5. Outlier Analysis [ILO 1,2,3] T6. Introduction to NoSQL Databases [ILO 1,3] T7. Data Warehouses and Data Lakes [ILO 3,4] T8. Datacubes and Online Analytical Processing [ILO 3,4] T9. Introduction to Information Retrieval [ILO 1,4] T10. Data Mining and Society [ILO 1,4] Final Evaluation Category % on Final Grade Quizzes Assignments Mid Semester Quiz Final Exam Lecture Panel 1. Dr. Thanuja Ambegoda 2. Dr. Sapumal Ahangama 3. Dr. Buddhika Karunaratne Introduction Why Data Mining? What Is Data Mining? A Multi-Dimensional View of Data Mining What Kind of Data Can Be Mined? What Kinds of Patterns Can Be Mined? What Technology Are Used? What Kind of Applications Are Targeted? Major Issues in Data Mining A Brief History of Data Mining and Data Mining Society Summary Why Data Mining? The Explosive Growth of Data: from terabytes to petabytes ◼Data collection and data availability ◼Automated data collection tools, database systems, Web, computerized society ◼Major sources of abundant data ◼Business: Web, e-commerce, transactions, stocks, ... ◼Science: Remote sensing, bioinformatics, scientific simulation, ... ◼Society and everyone: news, digital cameras, YouTube We are drowning in data, but starving for knowledge! “Necessity is the mother of invention”-Data mining-Automated analysis of massive data sets Evolution of Sciences Before 1600, empirical science 1600-1950s, theoretical science Each discipline has grown a theoretical component. Theoretical models often motivate experiments and generalize our understanding. 1950s-1990s, computational science Over the last 50 years, most disciplines have grown a third, computational branch (e.g. empirical, theoretical, and computational ecology, or physics, or linguistics.) Computational Science traditionally meant simulation. It grew out of our inability to find closed-form solutions for complex mathematical models. 1990-now, data science The flood of data from new scientific instruments and simulations The ability to economically store and manage petabytes of data online The Internet and computing Grid that makes all these archives universally accessible Scientific info. management, acquisition, organization, query, and visualization tasks scale almost linearly with data volumes. Data mining is a major new challenge! Jim Gray and Alex Szalay, The World Wide Telescope: An Archetype for Online Science, Comm. ACM, 45(11): 50-54, Nov. 2002 Evolution of Database Technology 1960s: Data collection, database creation, IMS and network DBMS 1970s: Relational data model, relational DBMS implementation 1980s: RDBMS,. advanced data models (extended-relational, OO, deductive, etc.) Application-oriented DBMS (spatial, scientific, engineering, etc.) 1990s: Data mining, data warehousing, multimedia databases, and Web databases 2000s Stream data management and mining Data mining and its applications Web technology (XML, data integration) and global information systems Chapter 1. Introduction Why Data Mining? What Is Data Mining? A Multi-Dimensional View of Data Mining What Kind of Data Can Be Mined? What Kinds of Patterns Can Be Mined? What Technology Are Used? What Kind of Applications Are Targeted? Major Issues in Data Mining A Brief History of Data Mining and Data Mining Society Summary What Is Data Mining? ◼Data mining (knowledge discovery from data) ◼Extraction of interesting (non-trivial, implicit, previously unknown and potentially useful) patterns or knowledge from huge amount of data ◼Data mining: a misnomer? ◼Alternative names ◼Knowledge discovery (mining) in databases (KDD), knowledge extraction, data/pattern analysis, data archeology, data dredging, information harvesting, business intelligence, etc. ◼Watch out: Is everything “data mining”? ◼Simple search and query processing ◼(Deductive) expert systems Knowledge Discovery (KDD) Process This is a view from typical database systems and data warehousing communities Data mining plays an essential role in the knowledge discovery process Data Cleaning Data Integration Databases Data Warehouse Task-relevant Data Selection Data Mining Pattern Evaluation Example: A Web Mining Framework ◼Web mining usually involves ◼Data cleaning ◼Data integration from multiple sources ◼Warehousing the data ◼Data cube construction ◼Data selection for data mining ◼Data mining ◼Presentation of the mining results ◼Patterns and knowledge to be used or stored into knowledge-base Data Mining in Business Intelligence Increasing potential to support business decisions End User Business Analyst Data Analyst DBA Decision Making Data Presentation Visualization Techniques Data Mining Information Discovery Data Exploration Statistical Summary, Querying, and Reporting Data Preprocessing/Integration, Data Warehouses Data Sources Paper, Files, Web documents, Scientific experiments, Database Systems Example: Mining vs. Data Exploration ◼Business intelligence view ◼Warehouse, data cube, reporting but not much mining ◼Business objects vs. data mining tools ◼Supply chain example: tools ◼Data presentation ◼Exploration KDD Process: A Typical View from ML and Statistics Input Data Data Mining Data Pre- Processing Post- Processing This is a view from typical machine learning and statistics communities Data integration Normalization Feature selection Dimension reduction Pattern discovery Association & correlation Classification Clustering Outlier analysis ......... Pattern evaluation Pattern selection Pattern interpretation Pattern visualization Example: Medical Data Mining ◼Health care & medical data mining - often adopted such a view in statistics and machine learning ◼Preprocessing of the data (including feature extraction and dimension reduction) ◼Classification or/and clustering processes ◼Post-processing for presentation Chapter 1. Introduction Why Data Mining? What Is Data Mining? A Multi-Dimensional View of Data Mining What Kind of Data Can Be Mined? What Kinds of Patterns Can Be Mined? What Technology Are Used? What Kind of Applications Are Targeted? Major Issues in Data Mining A Brief History of Data Mining and Data Mining Society Summary Multi-Dimensional View of Data Mining Data to be mined ◼Database data (extended-relational, object-oriented, heterogeneous, legacy), data warehouse, transactional data, stream, spatiotemporal, time-series, sequence, text and web, multi-media, graphs & social and information networks Knowledge to be mined (or: Data mining functions) ◼Characterization, discrimination, association, classification, clustering, trend/deviation, outlier analysis, etc. ◼Descriptive vs. predictive data mining ◼Multiple/integrated functions and mining at multiple levels Techniques utilized ◼Data-intensive, data warehouse (OLAP), machine learning, statistics, pattern recognition, visualization, high-performance, etc. Applications adapted ◼Retail, telecommunication, banking, fraud analysis, bio-data mining, stock market analysis, text mining, Web mining, etc. Chapter 1. Introduction Why Data Mining? What Is Data Mining? A Multi-Dimensional View of Data Mining What Kind of Data Can Be Mined? What Kinds of Patterns Can Be Mined? What Technology Are Used? What Kind of Applications Are Targeted? Major Issues in Data Mining A Brief History of Data Mining and Data Mining Society Summary Data Mining: On What Kinds of Data? Database-oriented data sets and applications Relational database, data warehouse, transactional database Advanced data sets and advanced applications Data streams and sensor data Time-series data, temporal data, sequence data (incl. bio-sequences) Structure data, graphs, social networks and multi-linked data Object-relational databases Heterogeneous databases and legacy databases Spatial data and spatiotemporal data Multimedia database Text databases The World-Wide Web Chapter 1. Introduction Why Data Mining? What Is Data Mining? A Multi-Dimensional View of Data Mining What Kind of Data Can Be Mined? What Kinds of Patterns Can Be Mined? What Technology Are Used? What Kind of Applications Are Targeted? Major Issues in Data Mining A Brief History of Data Mining and Data Mining Society Summary Data Mining Function: (1) Generalization ◼Information integration and data warehouse construction ◼Data cleaning, transformation, integration, and multidimensional data model ◼Data cube technology ◼Scalable methods for computing (i.e., materializing) multidimensional aggregates ◼OLAP (online analytical processing) ◼Multidimensional concept description: Characterization and discrimination ◼Generalize, summarize, and contrast data characteristics, e.g., dry vs. wet region Data Mining Function: (2) Association and Correlation Analysis ◼Frequent patterns (or frequent itemsets) ◼What items are frequently purchased together in your Walmart? ◼Association, correlation vs. causality ◼A typical association rule ◼Diaper → Beer [0.5%, 75%] (support, confidence) ◼Are strongly associated items also strongly correlated? ◼How to mine such patterns and rules efficiently in large datasets? ◼How to use such patterns for classification, clustering, and other applications? Data Mining Function: (3) Classification Classification and label prediction ◼Construct models (functions) based on some training examples ◼Describe and distinguish classes or concepts for future prediction ◼E.g., classify countries based on (climate), or classify cars based on (gas mileage) ◼Predict some unknown class labels Typical methods ◼Decision trees, naïve Bayesian classification, support vector machines, neural networks, rule-based classification, pattern- based classification, logistic regression, ... Typical applications: ◼Credit card fraud detection, direct marketing, classifying stars, diseases, web-pages, ... blue Yellow Data Mining Function: (4) Cluster Analysis ◼Unsupervised learning (i.e., Class label is unknown) ◼Group data to form new categories (i.e., clusters), e.g., cluster houses to find distribution patterns ◼Principle: Maximizing intra-class similarity & minimizing interclass similarity ◼Many methods and applications Data Mining Function: (5) Outlier Analysis Outlier analysis ◼Outlier: A data object that does not comply with the general behavior of the data ◼Noise or exception? ― One person's garbage could be another person's treasure ◼Methods: by product of clustering or regression analysis, ... ◼Useful in fraud detection, rare events analysis Time and Ordering: Sequential Pattern, Trend and Evolution Analysis ◼Sequence, trend and evolution analysis ◼Trend, time-series, and deviation analysis: e.g., regression and value prediction ◼Sequential pattern mining ◼e.g., first buy digital camera, then buy large SD memory cards ◼Periodicity analysis ◼Motifs and biological sequence analysis ◼Approximate and consecutive motifs ◼Similarity-based analysis ◼Mining data streams ◼Ordered, time-varying, potentially infinite, data streams Structure and Network Analysis Graph mining ◼Finding frequent subgraphs (e.g., chemical compounds), trees (XML), substructures (web fragments) Information network analysis ◼Social networks: actors (objects, nodes) and relationships (edges) ◼e.g., author networks in CS, terrorist networks ◼Multiple heterogeneous networks ◼A person could be multiple information networks: friends, family, classmates, ... ◼Links carry a lot of semantic information: Link mining Web mining ◼Web is a big information network: from PageRank to Google ◼Analysis of Web information networks ◼Web community discovery, opinion mining, usage mining, ... Evaluation of Knowledge ◼Are all mined knowledge interesting? ◼One can mine tremendous amount of “patterns” and knowledge ◼Some may fit only certain dimension space (time, location, ...) ◼Some may not be representative, may be transient, ... ◼Evaluation of mined knowledge → directly mine only interesting knowledge? ◼Descriptive vs. predictive ◼Coverage ◼Typicality vs. novelty ◼Accuracy ◼Timeliness Chapter 1. Introduction Why Data Mining? What Is Data Mining? A Multi-Dimensional View of Data Mining What Kind of Data Can Be Mined? What Kinds of Patterns Can Be Mined? What Technology Are Used? What Kind of Applications Are Targeted? Major Issues in Data Mining A Brief History of Data Mining and Data Mining Society Summary Data Mining: Confluence of Multiple Disciplines Data Mining Machine Learning Statistics Applications Algorithm Pattern Recognition High-Performance Computing Visualization Database Technology Why Confluence of Multiple Disciplines? ◼Tremendous amount of data ◼Algorithms must be highly scalable to handle such as tera-bytes of data ◼High-dimensionality of data ◼Micro-array may have tens of thousands of dimensions ◼High complexity of data ◼Data streams and sensor data ◼Time-series data, temporal data, sequence data ◼Structure data, graphs, social networks and multi-linked data ◼Heterogeneous databases and legacy databases ◼Spatial, spatiotemporal, multimedia, text and Web data ◼Software programs, scientific simulations ◼New and sophisticated applications Chapter 1. Introduction Why Data Mining? What Is Data Mining? A Multi-Dimensional View of Data Mining What Kind of Data Can Be Mined? What Kinds of Patterns Can Be Mined? What Technology Are Used? What Kind of Applications Are Targeted? Major Issues in Data Mining A Brief History of Data Mining and Data Mining Society Summary Applications of Data Mining Web page analysis: from web page classification, clustering to PageRank & HITS algorithms Collaborative analysis & recommender systems Basket data analysis to targeted marketing Biological and medical data analysis: classification, cluster analysis (microarray data analysis), biological sequence analysis, biological network analysis Data mining and software engineering (e.g., IEEE Computer, Aug. 2009 issue) From major dedicated data mining systems/tools (e.g., SAS, MS SQL-. Server Analysis Manager, Oracle Data Mining Tools) to invisible data mining Chapter 1. Introduction Why Data Mining? What Is Data Mining? A Multi-Dimensional View of Data Mining What Kind of Data Can Be Mined? What Kinds of Patterns Can Be Mined? What Technology Are Used? What Kind of Applications Are Targeted? Major Issues in Data Mining A Brief History of Data Mining and Data Mining Society Summary Major Issues in Data Mining (1) Mining Methodology ◼Mining various and new kinds of knowledge ◼Mining knowledge in multi-dimensional space ◼Data mining: An interdisciplinary effort ◼Boosting the power of discovery in a networked environment ◼Handling noise, uncertainty, and incompleteness of data ◼Pattern evaluation and pattern- or constraint-guided mining User Interaction ◼Interactive mining ◼Incorporation of background knowledge ◼Presentation and visualization of data mining results Major Issues in Data Mining (2) Efficiency and Scalability ◼Efficiency and scalability of data mining algorithms ◼Parallel, distributed, stream, and incremental mining methods Diversity of data types ◼Handling complex types of data ◼Mining dynamic, networked, and global data repositories Data mining and society ◼Social impacts of data mining ◼Privacy-preserving data mining ◼Invisible data mining Chapter 1. Introduction Why Data Mining? What Is Data Mining? A Multi-Dimensional View of Data Mining What Kind of Data Can Be Mined? What Kinds of Patterns Can Be Mined? What Technology Are Used? What Kind of Applications Are Targeted? Major Issues in Data Mining A Brief History of Data Mining and Data Mining Society Summary A Brief History of Data Mining Society 1989 IJCAI. Workshop on Knowledge Discovery in Databases Knowledge Discovery in Databases (G. Piatetsky-Shapiro and W. Frawley, 1991) 1991-1994 Workshops on Knowledge Discovery in Databases Advances in Knowledge Discovery and Data Mining (U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, 1996) 1995-1998 International Conferences on Knowledge Discovery in Databases and Data Mining (KDD'95-98) Journal of Data Mining and Knowledge Discovery (1997) ACM SIGKDD. conferences since 1998 and SIGKDD. Explorations More conferences on data mining PAKDD. ( 1997), PKDD (1997), SIAM-Data Mining (2001), (IEEE) ICDM. ( 2001), etc. ACM Transactions on KDD starting in 2007 Conferences and Journals on Data Mining KDD Conferences ACM SIGKDD. Int. Conf. on Knowledge Discovery in Databases and Data Mining (KDD) SIAM. Data Mining Conf. (SDM) (IEEE). Int. Conf. on Data Mining (ICDM) European Conf. on Machine Learning and Principles and practices of Knowledge Discovery and Data Mining (ECML-PKDD). Pacific-Asia Conf. on Knowledge Discovery and Data Mining (PAKDD). Int. Conf. on Web Search and Data Mining (WSDM) Other related conferences DB conferences: ACM SIGMOD, VLDB, ICDE, EDBT, ICDT. , ... Web and IR conferences: WWW, SIGIR, WSDM ML. conferences: ICML, NIPS PR. conferences: CVPR, Journals Data Mining and Knowledge Discovery (DAMI or DMKD) IEEE. Trans. On Knowledge and Data Eng. (TKDE) KDD. Explorations ACM Trans. on KDD Where to Find References? DBLP, CiteSeer, Google Data mining and KDD (SIGKDD: CDROM). Conferences: ACM-SIGKDD, IEEE-ICDM, SIAM-DM, PKDD, PAKDD,. etc. Journal: Data Mining and Knowledge Discovery, KDD Explorations, ACM TKDD. Database systems (SIGMOD: ACM SIGMOD. Anthology-CD ROM). Conferences: ACM-SIGMOD, ACM-PODS, VLDB, IEEE-ICDE, EDBT, ICDT, DASFAA. Journals: IEEE-TKDE, ACM-TODS/TOIS, JIIS,. J. ACM, VLDB. J., Info. Sys., etc. AI & Machine Learning Conferences: Machine learning (ML), AAAI, IJCAI, COLT. ( Learning Theory), CVPR, NIPS,. etc. Journals: Machine Learning, Artificial Intelligence, Knowledge and Information Systems, IEEE-PAMI,. etc. Web and IR Conferences: SIGIR, WWW, CIKM,. etc. Journals: WWW: Internet and Web Information Systems, Statistics Conferences: Joint Stat. Meeting, etc. Journals: Annals of statistics, etc. Visualization Conference proceedings: CHI, ACM-. SIGGraph, etc. Journals: IEEE Trans. visualization and computer graphics, etc. Chapter 1. Introduction Why Data Mining? What Is Data Mining? A Multi-Dimensional View of Data Mining What Kind of Data Can Be Mined? What Kinds of Patterns Can Be Mined? What Technology Are Used? What Kind of Applications Are Targeted? Major Issues in Data Mining A Brief History of Data Mining and Data Mining Society Summary Summary Data mining: Discovering interesting patterns and knowledge from massive amount of data A natural evolution of database technology, in great demand, with wide applications A KDD process includes data cleaning, data integration, data selection, transformation, data mining, pattern evaluation, and knowledge presentation Mining can be performed in a variety of data Data mining functionalities: characterization, discrimination, association, classification, clustering, outlier and trend analysis, etc. Data mining technologies and applications Major issues in data mining"
  },
  {
    "index": 1,
    "level": 1,
    "start_page": 45,
    "end_page": 89,
    "title": "03 Time Series Part 1",
    "content": "Time Series Mining & Forecasting Dr. Thanuja Ambegoda Lecture outline Introduction to TS Basic visualization techniques Trend analysis and seasonality detection Cyclic pattern analysis Anomaly detection in TS Stationarity in TS Traditional TS forecasting methods Advanced TS forecasting methods Part 1. Introduction to time series analysis and mining Time series everywhere! U)0 - Sunspots Month Monthly Sunspot Numbers, 1749 - 1983 _.,_ , I 35000. :IIOOO. l>OO Half-hourly electricity demand in England and Wales - Dally minimum temperan.es Dllte Daily temperature in Melbourn between 1981 and 1990 What is a time series? A time series is an ordered sequence of values of a variable usually at equally spaced time intervals Typically, the mean and the standard deviation can vary in a time series Understanding time series data helps predict future events based on historical patterns Trend Volatility TS analysis, forecasting & mining Time series analysis Techniques for identifying and analysing temporal dependencies in sequential data Time series mining What has happened? Time series forecasting What's going to happen next? Applications Finance Sales forecasting Inventory analysis Stock market analysis Price estimation Weather Temperature Climate change Seasonal shifts Rain, wind, snow,... Industry Usage Anomaly Maintenance Healthcare Patient occupancy in hospitals Disease outbreaks Patient monitoring Insurance Sales forecasting Insurance beneﬁts awarded Diﬀerence between a time series and a random variable with known mean and standard deviation We might be able to build a distribution that explains all the data points in the time series. But does that mean the distribution can be sampled to produce the time series? Dependence on the past Memory of a TS The extent to which past values inﬂuence future values A few points having a similar trend But there might be no overall trend Actual values tend to depend on the value at the previous time point Autocorrelation Memory of a Time Series Short memory Short-term dependencies. Only recent past values inﬂuence the next value E.g. ﬁnancial data Long memory Long-term dependencies. Distant past values can impact the next value. E.g. climate data Stationary TS often have short memory Non-stationary TS often have long memory but require more complex models Time Series Components Time series components Level: The average value in the series. Trend: The increasing or decreasing value in the series. Cycle: (Long-term) phenomena that happen across seasonal periods, without a ﬁxed period (eg. economic recession) Seasonality: The repeating short-term cycle in the series. Noise: The random variation in the series. (unsystematic) Reference Trend Refers to the increasing or decreasing value in the series. Can be linear or nonlinear. Methods to detect: Moving Averages Polynomial Fitting Seasonal component Periodic ups and downs. Period can be a year, several years, months, weeks or days. 0 6000 01-01-1992 Retail Sales of Used Car Dealers in the United States 12 month seasonal period 23-06-1997 14-12-2002 05-06-2008 Month IE-M 26-11-2013 19-05-2019 How to measure/detect seasonality Autocorrelation plots FFT (Fast Fourier Transform) Seasonal decomposition Autocorrelation Plots Autocorrelation, often denoted as R(k), measures the correlation between a time series and its lagged version. A signiﬁcant spike at a particular lag indicates seasonality at that lag. If there's seasonality present at a particular lag k, the autocorrelation plot will show a significant spike at that lag. For example, if there's a yearly seasonality in monthly data, we would expect a significant autocorrelation at lag 12. Decomposing TS into components Often this is done to help improve understanding of the time series, but it can also be used to improve forecast accuracy. Additive models (linear) y(t) = Level + Trend + Seasonality + Noise Multiplicative models (nonlinear) y(t) = Level * Trend * Seasonality * Noise Real-world problems are messy and noisy. There may be additive and multiplicative components. There may be an increasing trend followed by a decreasing trend. There may be non-repeating cycles mixed in with the repeating seasonality components. Decomposing TS into components (I) \"'O 100- (1) 80- 60- Electrical equipment manufacturing (Euro area) Year series Data - Trend-cycle Original Trend-cycle component Seasonal component Residual component Seasonal and Trend Decomposition using LOESS (STL) LOESS:. Locally Weighted Scatterplot Smoothing Alternatively LOWESS,. which means Locally Weighted Regression Non-parametric smoothing technique. Fits smooth curves using locally weighted polynomial regression. Each point estimated based on subset of its neighbors. Close neighbors have more inﬂuence via weighted least squares. Ensures gradual transitions, yielding a smooth curve. Reference Steps in STL Trend extraction Applies LOESS. smoother to time series. Captures the central path or growth trajectory. Not inﬂuenced by seasonality. Detrending & Seasonal Extraction Remove trend from original series. Apply LOESS. to detrended series to capture seasonality. Seasonal pattern can be of any type: monthly, quarterly, etc. Extract Residual Component Residuals = Original Series - Trend - Seasonality. Captures randomness and potential anomalies. Helpful for model diagnostics. Seasonally adjusted data If the variation due to seasonality is not of primary interest, the seasonally adjusted series can be useful. For example, monthly unemployment data are usually seasonally adjusted in order to highlight variation due to the underlying state of the economy rather than the seasonal variation. TS decomposition using statsmodel package Naive, or classical, decomposition method. Requires you to know if the model is additive or multiplicative Statsmodels python package 700 .---&-'--(-)---,---*----+-,----.----, \"'O 600 / 500 a; 400 Vl 300 '8 200 l00=---=-______::,..:::,:__----=--..1. __ _,__-----'----'-------'---____.0----'--------'-----'--------'------' \"'O 1.10 u-v------w--------x-y----z{ ro 1.05 _g 1.00 \"iii 0.95 1 0.90 0. 85 .___ _ _.__ _ ____. __ _,__-----'----'-------'---_____J$----'--------'-----'--------'------' Month Cycles Oscillations that aren't ﬁxed in a seasonal pattern. Typically longer duration than seasonality. Can be caused by economic conditions, large-scale events, etc. Why detect cycles? Improve forecasting accuracy.Uncover underlying long-term patterns. Make informed strategic decisions (e.g., economic, business planning). Separate cyclical eﬀects from irregular noise. Detecting cycles Spectral analysis Transform time series into frequency domain using Fourier Transform. Identify dominant frequencies (apart from seasonal frequencies). The peaks in spectral density indicate cyclical components. Seasonality shows up as distinct peaks at regular intervals in the spectral density plot. Cyclical components will manifest as broader peaks or bands at lower frequencies, representing longer-term ﬂuctuations. Detecting cycles 2. Wavelet transform Decompose time series into diﬀerent frequency bands. Detect cycles of various lengths. Suitable for non-stationary time series Seasonality: Seasonal components will appear as consistent patterns at speciﬁc scales (frequencies) across the time domain. Cycles: Cyclical components will appear as patterns at broader scales that may vary over time. TS and regression Focused on identifying underlying trends and patterns Mathematically modeling / describing those patterns Predict/forecast future values TS and regression Regression can model the trend over time Linear regression for mean demand. Function of economic growth? Within year cycles - seasonal variability? Y = b1x1 + b2x2 + b3x3 + ... TS allows you to model the process without knowing the underlying causes Level Noise Non-constant Variance Trend Seasonality Outlier Signal and Noise Signal and noise C'ansignal o.s S00 Noise -0.2 S00 Ho isy slgna I o.s S00 Signal and noise: some deﬁnitions Statistical moments Mean and standard deviation Stationary vs non-stationary Trends in mean and/or standard deviation Stationary - doesn't depend on the time of observation Seasonality Periodic patterns Autocorrelation Degree to which the time series values in period t are related to time series values in period t-1, t-2, ... Stationarity of a Time Series Stationary TS TS whose properties do not depend on the time of observation Constant variance and level No seasonality No trend Stationarity check Visual inspection Seasonal-Trend decomposition Summary statistics at random partitions Statistical tests Stationarity check: Visual inspection Mean Variance Seasonality Stationarity check: Seasonal-Trend decomposition Statsmodels python package from pandas import read_csv from statsmodels.tsa.seasonal import seasonal_decompose df = read_csv( a1rhne-p ssengers csv', header=O) df.Month = pd.to_datetime(df.Month) df = df.set_index('Month') result = seasonal_decompose( df, model='multiplicative') result.plotQ ii 12 l - ,, l.. d L1tfl--- q --As 7 J Stationarity check: summary statistics Summary statistics at random partitions Stationarity check: statistical tests Augmented Dickey-Fuller test TS is non-stationary! Adjustments Fixing non-constant variance Trend removal Fixing non-constant variance + trend removal Fixing non-constant variance Air Passengers from 1949 to 1961 • Logarithmic - Removes extreme ( exponential) trends • Square root transformation - Removes a quadratic growth trend • Box-Cox transform - Supports both square root and log transform lil50 Time Fixing non-constant variance To reverse the boxcox transform: from scipy.special import boxcox, inv_boxcox from pandas import read_csv from numpy import log from numpy import sqrt from scipy.stats import boxcox import matplotlib.pyplot as pit series = read_csv('airline-passengers.csv', header=O, parse_dates=[O], index_col=O, squeeze=True) series_log = log(series) series_sqr = sqrt(series) series_boxcox, lam= boxcox(series) print('Lambda: %f % lam) pit.plot( series_log) plt.plot(series_sqr) pit.plot( series_boxcox) pit.show() .,.. ,.. IIO• ... l2• lll. Original Time Series Square Root Transform 62) .. ,..,, 1110 • ,o. ------ Box-Cox Transform lll l10 Trend removal eoo. Differencing !IIO· - First order : the time series value yi is replaced by oi - the difference between it and the previous value. - First-order ·y' - . - r,. 1 ll)O· . i - . i .n- . - Second-order i = Yi - Yi-1 Original Time Series !!ISO lllS2 1¥.>1 !!ISi 1!160 from pandas import datetime from matplotlib import pyplot After Differencing (First-order) series = read_csv('airline-passengers.csv', header=0, parse_dates=[0], index_col=0, squeeze=True) diff = series.diffQ pyplot.plot( series. values) pyplot.plot( diff. values) so. so. •100 • Fixing non-constant variance + trend removal from pandas import read_csv from pandas import Series import pandas as pd from scipy.stats import boxcox series = read_csv('airline-passengers.csv', header=0, parse_dates=[0], index_col=0, squeeze=True) series_boxcox, lam= boxcox(series) series_boxcox=pd.DataFrame(series_boxcox) series_boxcox_diff = series_boxcox.diff() pit.plot( series) plt.plot(series_boxcox_diff) ..... , ... Original Time Series Box Cox Transform+ Differencing"
  },
  {
    "index": 2,
    "level": 1,
    "start_page": 90,
    "end_page": 135,
    "title": "04 Time Series Part 2",
    "content": "Time Series Mining & Forecasting - 2 Dr. Thanuja Ambegoda Time Series Part. 2 - Lecture Outline Time series forecasting (ctd) Autocorrelation revisited Forecasting methods Evaluation of time series forecasting Time series mining Time series mining tasks Time series compression Time series similarity measures Motifs, matrix proﬁle, discord, anomalies Time series representations Correlation Correlation is a statistical measure that expresses the extent to which two variables are linearly related (meaning they change together at a constant rate) Autocorrelation • ✓ • . ,. • •• • • I... ::::, -- I... c.. :.:; co I... • • • • ::::, co • • temperature on day i-1 •-.--... 1 2 3 4 5 6 7 8 shift Autocorrelation plot (ACF) • ACF. describes the autocorrelation between an observation and another observation at a prior time step that includes direct and indirect dependence information. • How to calculate ACF? Autocorrelation For lag= I :Maxlag: 0 :s ACF(lag)= corr (Xt, X1agN) • Example:X=[I 2 3 4 5 6 7 8] X=[3 4 5 6 7 8] Xlag I = [2 3 4 5 6 7] X1ag2=[ I 2 3 \u0006 6] ACF(lag= I )=correlation(X,X1agl) ACF(lag=2)=correlation(X,X1ag2) Partial autocorrelation (PACF) plot • PACF describes direct relationship between an observation and its lag. • How to calculate PACF? For lag= I :Maxlag Fit Linear Regression: Xt = $o + $ -Xt-1 + $+ Xt-2 + ... + <1>1ag Xt-lag PACF(lag)= <1>1ag Partial Autocorrelation 02 5 -oos Time Series Models Traditional Time Series Models Univariate Models -ARIMA -SARIMAX. - Prophet - Neural Prophet Multivariate Models - Vector Auto regression Machine Learning Models - Neural Network Regressor - Catboost Regressor - Any reg ressor Simple forecasting methods Average method Naive method Seasonal naive method Drift method Homework: Read about simple forecasting methods These methods are covered for the sake of completeness of fundamental theory, and can be used as primary benchmarks. Time series forecasting Estimating future values of a time series Exponential smoothing Autoregressive methods ARMA ARIMA SARIMA. Exponential smoothing Simple Exponential Smoothing (SES) (Brown, 1956) is suitable for forecasting data with no clear trend or seasonal pattern. Prediction is a weighted linear sum of recent past observations Weights decline exponentially, making observations much earlier in the past less relevant Exponential smoothing Double Exponential Smoothing (Holt, 1957) Smoothing for level + trend Triple Exponential Smoothing (Holt-Winter, 1960) Smoothing for trend + seasonality + level Exponential smoothing - Python import pandas as pd import numpy as np import matplotlib.pyplot as pit from statsmodels.tsa.holtwinters import ExponentialSmoothing from statsmodels.tsa.holtwinters import SimpleExpSmoothing df = pd.read_csv('airline-passengers.csv', parse_dates=['Month'],index_col='Month') df.index.freq = 'MS' ts=df.iloc[:, 0] es I = SimpleExpSmoothing(ts).fit(smoothing_level=0.2) ts_es I = es 1.predict(start=ts.index[0], end=ts.index(-1 ]) es2 = ExponentialSmoothing(ts, trend='add').fit(smoothing_level=0.2,smoothing_trend=0.2) ts_ es2 = es2.predict( start=ts.index[0], end=ts.index[-1 ]) es3 = ExponentialSmoothing(ts, trend='add',seasonal='mul', seasonal_periods= 12).fit(smoothing_level=0.2,smoothing_trend=0.2,smoothing_seasonal=0.2) ts_es3 = es3.predict(start=ts.index[0], end=ts.index(-1 ]) Exponential smoothing - Python 600 _ - Raw Data - ES 500 • 400 - JlO 200 - Simple Exponential Smoothing a=0.2 Level only 6ll0 _ - Ra v Data - Ho 500 • 200 - 1.954 1.956 Double Exponential Smoothing a =0.2, % =0.2 Level+ Trend fiO0. - Ra Data - Holt-Wmters J)Q 200 • 100 • l950 l952 l956 l958 Triple Exponential Smoothing a =0.2, % =0.2, y=0.2, S= 12 Level+ Trend + Seasonality Autoregressive model (AR) In an autoregression model, we forecast the variable of interest using a linear combination of past values of the variable. The term autoregression indicates that it is a regression of the variable against itself. εt is the white noise. This is an autoregressive model of order p - AR(p) Autoregressive model • Y t = ct,o + ct> 1 Xt-1 +ct,2 Xt-2 • 0.288144 = ct,o + ct,1 X 0.000988026 +ct,2 X -0.538906 • 0.000988026 = ct,o + ct,1 X -0.538906 +ct,2 X 0.719208 • -0.224202 = ct,o + ct,1 X 0.3254 I 3+ct,2 X -0.204708 • 98 Equations, 3 Coefficients • Solution: Linear least squares Autoregressive model - Python import statsmodels.api as sm import matplotlib.pyplot as pit from statsmodels.tsa.ar _model import AutoReg train, test = x[ I :len(x)-7], x[len(x)-7:] model = AutoReg(train, lags=2) model_fit = model.fitQ print('Coefficients: %s' % model_fit.params) x_ar = model_fit.predict(start=O, end=len(x), dynamic=False) plt.plot(x[2:len(x)],label='Actual') plt.plot(x_ar, color='red',label='AR(2) Model') plt.legend(loc='best') Moving average model (MA) Rather than using past values of the forecast variable in a regression, a moving average model uses past forecast errors in a regression-like model. εt is the white noise. This is a moving average model of order q - MA(q) The MA component models the relationship between an observation and the white noise errors of previous observations Moving average model It is possible to write any stationary AR(p) model as an MA(oo) model. For example, using repeated substitution, we can demonstrate this for an AR(1) model: Yt = ¢1Yt-l + Ct = ¢1 ( ¢1Yt-2 + ct-1) + ct = <PiYt-2 + </>1ct-l + ct = ¢f Yt-3 + </>ict-2 + </>1ct-l + ct etc. Provided -1 < ¢1 < 1, the value of¢, will get smaller ask gets larger. So eventually we obtain Yt =ct+ </>1ct-l + </>ict-2 + </>ict-3 + · · ·, an MA(oo) process. Moving average model Parameter estimation: Maximum likelihood estimation Non-linear least squares estimation Moving average model - python from pandas ·mport read_csv from statsmodels.tsa.arima_model import ARMA from sklearn.metrics import mean_squared_error import matplotlib.pyplot as pit df = read_csv('HalmstadTempM.csv', header=O, parse_dates=[O], index_col=O) dftrain, dftest = df[l :len(df)-7], df[len(df)-7:] model = ARMA( dftrain, order=(O, 5)) #MA(S) model_fit = model.fit() pri,nt('Coefficients: %s' % model_fit.params) predictions = model_fit.predict( start=len( dftrain ), end=len( dftrain)+len(dftest)-1, dynamic=False) for i in range(len(predictions) ): print('pr dicted=%f, expected=%f % (predictions[i], dftest.values[i])) rmse = sqrt(mean_squared_error(dftest.values, predictions)) pr\"nt(iest RMSE: %.3 % rmse) pit.plot( dftest,label='Actu I') plt.plot(predictions, color=' red', label='MA( I) Forecast') plt.legend(loc='best') pit.show() Autoregressive moving average model (ARMA) Combines AR and MA models AR accounts for autocorrelations in the TS (deterministic) MA accounts for smoothing ﬂuctuations (stochastic) Model fitting: Maximum likelihood estimation (Box-Jenkins method) Autoregressive integrated moving average (ARIMA) ARIMA = ARMA. + trend removal Seasonal ARIMA (SARIMA) SARIMA = ARMA. + trend removal + seasonal adjustment • SARI MA. ( p, d, q)(P. D, Q)m • Example: SARI MA. ( 2, /,3)( /, 1,2) 12) ■ Autoregressive with 2 lags ■ Y'c = <f>o + <f>1 X't-1 + <f>2 X't-2 ■ Autoregressive with one Seasonal la s ( 12) ■ Differencing data in order of one • X'c=Xt-Xt-1 ■ Differencing data in order of one with Seasonal legs ( 12) • X't=Xt-Xt-12 ■ Moving Average with 3 lags ■ Y'c = µ+Et + 0, Et-I + 02 Et-2 + 03 Et-3 ■ Moving Average with 2 Seasonal lags ( 12) ■ Y,t = µ + Et+ 01 Et-12 + 02 Et-24 ----, Passengers JJO 19 9 Months -- Month Months -- SARIMA. - Python from pandas import read_csv from sklearn.metrics import mean_squared_error import matplotlib.pyplot as pit from pmdarima import auto_arima import pandas as pd df = read_csv('airline-passengers.csv', header=O, parse_dates=[O], index_col=O) train, test = df[ I :len( df)-7], df[len( df)-7:] # SARIMA. wtth automatic hyperparameter settJng model = auto_arima(train, start_p= I, start_q= I, max_p=3, max_q=3, m= 12, start_P=O, seasonal=True, d= I, D= I, trace=True, error _action='ignore', suppress_ warnings=True, stepwise=True) model.fit(train) forecast = model.predict(n_periods=len(test)) plt.plot(test.values,label='Actual') plt.plot(forecast, color='red' ,label='SARIMA'+. str( model.order )+str( model.seasonal_ order)) plt.legend(loc='best') Comparison of model assumptions Model Constant Variance ARMA ARIMA SARIMA. ✓ Constant Mean (No Trend) Seasonality­ Free Evaluation Residual plots: ACF, PACF. residual plots, histogram or density plot of residuals Information Criteria AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion) are used to compare and select models with diﬀerent parameters. They take into account both the likelihood of the model and the number of parameters used. The model with the lowest AIC or BIC is usually preferred. These criteria are often used to choose the order of ARIMA. models (i.e., the values of p, d, and q). Model selection (hyperparameter tuning) Visualization MA - Autocorrelation function (ACF) plot RA - Partial autocorrelation function (PACF) plot Grid search Test for diﬀerent model orders exhaustively A model that is more accurate and less complex is preferred More accurate - ﬁts the data Less complex - smaller number of parameters Based on the error metrics, residual analysis, and overﬁtting checks, select the model that seems most likely to generalize well to new data. This is typically the model with low error on the validation/test set, whose residuals are closest to white noise, and has lower AIC or BIC values. Time Series Mining Time series mining tasks Indexing (query by content) Clustering Classiﬁcation Prediction Summarization Anomaly detection Segmentation Indexing (query by content) Whole matching: a query time series is matched against a database of individual time series to identify the ones similar to the query Subsequence Matching: a short query subsequence time series is matched against longer time series by sliding it along the longer sequence, looking for the best matching location. Brute force technique (linear or sequential matching) Very slow Somewhat more eﬃcient method: Store a compressed version of the TS and use it for an initial comparison (lower bound) using linear scan Even more eﬃcient method: Use an index structure that clusters similar sequences together Indexing Two types Vector-based: Original sequences are compressed using dimensionality reduction Resulting sequences grouped in the new dimensions Hierarchical (e.g. R-tree) or non-hierarchical Performance deteriorates when dim > 5 Metric-based: Superior in performance Works for higher dimensionalities (dim < 30) Require only distances between objects Clusters objects using the distance between objects Summarization Text/graphical descriptions (summaries) Anomaly detection and motif discovery are special cases of summarization Some of popular approaches for visualizing massive time series datasets include TimeSearcher, Calendar-Based Visualization, Spiral and VizTree Read section 3.5 of this reference Time series compression TS compression helps reduce storage costs and allows eﬃcient indexing Delta encoding Delta of delta encoding Simple 8b Run Length Encoding (RLE) Reference Similarity measures Dissimilarity measures Distance measures Time series similarity measures Similarity is the inverse of distance They play an important role in time series mining tasks Euclidean distance and Lp norms Dynamic time warping Longest common subsequence similarity Probabilistic similarity measures General transformations Euclidean distance and Lp norms Dissimilarity between sequences C and Q D(C,Q) = Lp(C,Q) = p=2 reduces to Euclidean distance (p=1 is Manhattan distance) Widely used Only works when the two sequences are of the same length For two sequences of length n, consider each of them as a point in n dimensional Euclidean space Euclidean distance between normalized sequences To normalize C, replace C with c' where Dynamic Time Warping (DTW) Useful when the two sequences have a similar shape but don't line up perfectly (i.e. out of phase) in the x axis Euclidean distance doesn't capture the similarity well DTW allows acceleration/deceleration of the signal (warping) along the time (x) axis Example application: speech processing Dynamic Time Warping Uses bottom-up dynamic programming approach Basic implementation O(mn) With a warping window w, O(nw) DTW cost/distance: number of changes (edits) made to C to make it aligned with Q Reference Basic idea Consider X = x1, x2, ..., xn , and Y = y1, y2, ..., yn We are allowed to extend each sequence by repeating elements Euclidean distance now calculated between the extended sequences X' and Y' Matrix M, where mij = d(xi, yj) Motifs, anomalies, discords, matrix proﬁles Matrix Profile TL;DR Abdullah Mueen, Eamonn Keogh, et al. A novel data structure for time series data mining. Features domain agnostic, exact or approximate, fast, one hyper-parameter, space efficient, para I lel izable, streaming support, missing data, interactive constant in time, 2.0 C LO V) 0.5 LOO f 0.75 CO 0.50 Vl 0 0.25 0.00 Anomaly fllote that there is a tiny signal unnoticable to the naked eye in the thousandths, but the matrix profile picks it up. Indices The matrix profile (red) is composed of two arrays; distances and 1-NN indices. Large distances are anomalous events. Repeated patterns are found in the 1-NN indices. rma\u0004 by 1y1er ►1a Time series representations Refer section 4 of this reference References and additional reading Book chapter on Time Series Mining NeuralProphet"
  },
  {
    "index": 3,
    "level": 1,
    "start_page": 136,
    "end_page": 211,
    "title": "05 Clustering",
    "content": "Cluster Analysis Basic concepts and Methods Dr. Thanuja Ambegoda Senior Lecturer Department of Computer Science and Engineering University of Moratuwa Lecture outline Cluster Analysis: Basic Concepts Partitioning Methods Hierarchical Methods Density-Based Methods Grid-Based Methods Evaluation of Clustering. Summary 1. Cluster analysis: Basic concepts 2. Partitioning methods 3. Hierarchical methods What is cluster analysis Cluster: A collection of data objects that are, similar (or related) to one another within the same group dissimilar (or unrelated) to the objects in other groups Cluster analysis (or clustering, data segmentation, ...) Partitioning a set of data objects into groups (subsets) such that similar ones are grouped together while dissimilar ones are placed in diﬀerent groups. Unsupervised learning: no predefined classes (i.e., learning by observations not by examples) Typical applications As a stand-alone tool to get insight into data distribution As a preprocessing step for other algorithms Clustering for Data Understanding and Applications Biology: taxonomy of living things: kingdom, phylum, class, order, family, genus and species Information retrieval: document clustering Land use: Identification of areas of similar land use in an earth observation database Marketing: Help marketers discover distinct groups in their customer bases, and then use this knowledge to develop targeted marketing programs City-planning: Identifying groups of houses according to their house type, value, and geographical location Earthquake studies: Observed earthquake epicenters should be clustered along continent faults Clustering as a Preprocessing Tool (Utility) Summarization: Preprocessing for regression, PCA, classification, and association analysis Compression: Image processing: vector quantization Finding K-nearest Neighbors Localizing search to one or a small number of clusters Outlier detection Outliers are often viewed as those “far away” from any cluster Quality: What Is Good Clustering? A good clustering method will produce high quality clusters, i.e., high intra-class similarity: cohesive within clusters low inter-class similarity: distinctive between clusters The quality of a clustering method depends on the similarity measure used by the method its implementation, and Its ability to discover some or all of the hidden patterns Measure the Quality of Clustering Dissimilarity/Similarity metric Similarity is expressed in terms of a distance function, typically metric: d(i, j) The definitions of distance functions are usually rather diﬀerent for interval-scaled, boolean, categorical, ordinal, ratio, and vector variables. Weights should be associated with diﬀerent variables based on applications and data semantics Quality of clustering: There is usually a separate “quality” function that measures the “goodness” of a cluster. It is hard to define “similar enough” or “good enough” The answer is typically highly subjective Considerations for Cluster Analysis Partitioning criteria Single level vs. hierarchical partitioning (often, multi-level hierarchical partitioning is desirable) Separation of clusters Exclusive (e.g., one customer belongs to only one region) vs. non-exclusive (e.g., one document may belong to more than one class) Similarity measure Distance-based (e.g., Euclidian, road network, vector) vs. connectivity-based (e.g., density or contiguity) Clustering space Full space (often when low dimensional) vs. subspaces (often in high-dimensional clustering) Requirements and Challenges Scalability Clustering all the data instead of only on samples Ability to deal with diﬀerent types of attributes Numerical, binary, categorical, ordinal, linked, and mixture of these Constraint-based clustering User may give inputs on constraints Use domain knowledge to determine input parameters Interpretability and usability Others Discovery of clusters with arbitrary shape Ability to deal with noisy data Incremental clustering and insensitivity to input order High dimensionality Major Clustering Approaches (1) Partitioning approach: Construct various partitions and then evaluate them by some criterion, e.g., minimizing the sum of squared errors Typical methods: k-means, k-medoids, CLARANS. Hierarchical approach: Create a hierarchical decomposition of the set of data (or objects) using some criterion Typical methods: Diana, Agnes, BIRCH, CHAMELEON. Density-based approach: Based on connectivity and density functions Typical methods: DBSACN, OPTICS,. DenClue Grid-based approach: based on a multiple-level granularity structure Typical methods: STING,. WaveCluster, CLIQUE. Major Clustering Approaches (2) Model-based: A model is hypothesized for each of the clusters and tries to find the best fit of that model to each other Typical methods: EM, SOM, COBWEB. Frequent pattern-based: Based on the analysis of frequent patterns Typical methods: p-Cluster User-guided or constraint-based: Clustering by considering user-specified or application-specific constraints Typical methods: COD (obstacles), constrained clustering Link-based clustering: Objects are often linked together in various ways Massive links can be used to cluster objects: SimRank, LinkClus 1. Cluster analysis: Basic concepts 2. Partitioning methods 3. Hierarchical methods Partitioning Algorithms: Basic Concept Partitioning method: Partitioning a database D of n objects into a set of k clusters, such that the sum of squared distances is minimized (where ci is the centroid or medoid of cluster Ci) Given k, find a partition of k clusters that optimizes the chosen partitioning criterion Global optimal: exhaustively enumerate all partitions Heuristic methods: k-means and k-medoids algorithms k-means (MacQueenʼ67, Lloydʼ57/ʼ82): Each cluster is represented by the center of the cluster k-medoids or PAM (Partition around medoids) (Kaufman & Rousseeuwʼ87): Each cluster is represented by one of the objects in the cluster The K-Means Clustering Method Given k, the k-means algorithm is implemented in four steps: Partition objects into k non-empty subsets Compute seed points as the centroids of the clusters of the current partitioning (the centroid is the center, i.e., mean point, of the cluster) Assign each object to the cluster with the nearest seed point Go back to Step 2, stop when the assignment does not change An Example of K-Means Clustering Comments on the K-Means Method Strength: Eﬀicient: O(tkn), where n is # objects, k is # clusters, and t is # iterations. Normally, k, t << n. Comparing: PAM: O(k(n-k)2 ), CLARA: O(. ks2 + k(n-k)) Comment: Often terminates at a local optimal. Weakness Applicable only to objects in a continuous n-dimensional space Using the k-modes method for categorical data In comparison, k-medoids can be applied to a wide range of data Need to specify k, the number of clusters, in advance (there are ways to automatically determine the best k (see Hastie et al., 2009) Sensitive to noisy data and outliers Not suitable to discover clusters with non-convex shapes Variations of the K-Means Method Most of the variants of the k-means which diﬀer in Selection of the initial k means Dissimilarity calculations Strategies to calculate cluster means Handling categorical data: k-modes Replacing means of clusters with modes Using new dissimilarity measures to deal with categorical objects Using a frequency-based method to update modes of clusters A mixture of categorical and numerical data: k-prototype method What Is the Problem of the K-Means Method? The k-means algorithm is sensitive to outliers ! Since an object with an extremely large value may substantially distort the distribution of the data K-Medoids: Instead of taking the mean value of the object in a cluster as a reference point, medoids can be used, which is the most centrally located object in a cluster PAM: K-Medoids algorithm Total Cost = 20 K=2 Arbitrarily choose k object as initial medoids Assign each remaining object to nearest medoids Randomly select a nonmedoid object,Orandom Compute total cost of swapping Total Cost = 26 Swapping O and Oramdom If quality is improved. Loop until no change The K-Medoid Clustering Method K-Medoids Clustering: Find representative objects (medoids) in clusters PAM (Partitioning Around Medoids, Kaufmann & Rousseeuw 1987) Starts from an initial set of medoids and iteratively replaces one of the medoids by one of the non-medoids if it improves the total distance of the resulting clustering PAM works eﬀectively for small data sets, but does not scale well for large data sets (due to the computational complexity) Eﬀiciency improvement on PAM CLARA. ( Kaufmann & Rousseeuw, 1990): PAM on samples CLARANS. ( Ng & Han, 1994): Randomized re-sampling 1. Cluster analysis: Basic concepts 2. Partitioning methods 3. Hierarchical methods Hierarchical Clustering Use distance matrix as clustering criteria. This method does not require the number of clusters k as an input, but needs a termination condition AGNES. ( Agglomerative Nesting) Introduced in Kaufmann and Rousseeuw (1990) Implemented in statistical packages, e.g., Splus Use the single-link method and the dissimilarity matrix Merge nodes that have the least dissimilarity Go on in a non-descending fashion Eventually all nodes belong to the same cluster Dendrogram: Shows How Clusters are Merged Decompose data objects into several levels of nested partitioning (tree of clusters), called a dendrogram A clustering of the data objects is obtained by cutting the dendrogram at the desired level, then each connected component forms a cluster DIANA. ( Divisive Analysis) Introduced in Kaufmann and Rousseeuw (1990) Implemented in statistical analysis packages, e.g., Splus Inverse order of AGNES. Eventually each node forms a cluster on its own Distance between Clusters Single link: smallest distance between an element in one cluster and an element in the other, i.e., dist(Ki, Kj) = min(tip, tjq) Complete link: largest distance between an element in one cluster and an element in the other, i.e., dist(Ki, Kj) = max(tip, tjq) Average: avg distance between an element in one cluster and an element in the other, i.e., dist(Ki, Kj) = avg(tip, tjq) Centroid: distance between the centroids of two clusters, i.e., dist(Ki, Kj) = dist(Ci, Cj) Medoid: distance between the medoids of two clusters, i.e., dist(Ki, Kj) = dist(Mi, Mj) Medoid: a chosen, centrally located object in the cluster Centroid, Radius and Diameter of a Cluster (for numerical data sets) Centroid: the “middle” of a cluster Radius: square root of average distance from any point of the cluster to its centroid Diameter: square root of average mean squared distance between all pairs of points in the cluster Extensions to Hierarchical Clustering Major weakness of agglomerative clustering methods Can never undo what was done previously Do not scale well: time complexity of at least O(n2), where n is the number of total objects Integration of hierarchical & distance-based clustering BIRCH. ( 1996): uses CF-tree and incrementally adjusts the quality of sub-clusters CHAMELEON. ( 1999): hierarchical clustering using dynamic modeling BIRCH. ( Balanced Iterative Reducing and Clustering Using Hierarchies) Zhang, Ramakrishnan & Livny, SIGMODʼ96 Incrementally construct a CF (Clustering Feature) tree, a hierarchical data structure for multiphase clustering Phase 1: scan DB to build an initial in-memory CF tree (a multi-level compression of the data that tries to preserve the inherent clustering structure of the data) Phase 2: use an arbitrary clustering algorithm to cluster the leaf nodes of the CF-tree Scales linearly: finds a good clustering with a single scan and improves the quality with a few additional scans Weakness: handles only numeric data, and sensitive to the order of the data record Clustering Feature Vector in BIRCH. Clustering Feature (CF): CF = (N, LS, SS) N:. Number of data points LS: linear sum of N points: SS: square sum of N points CF-Tree in BIRCH. Clustering feature: Summary of the statistics for a given subcluster: the 0-th, 1st, and 2nd moments of the subcluster from the statistical point of view Registers crucial measurements for computing cluster and utilizes storage eﬀiciently A CF tree is a height-balanced tree that stores the clustering features for a hierarchical clustering A non-leaf node in a tree has descendants or “children” The non-leaf nodes store sums of the CFs of their children A CF tree has two parameters Branching factor: max # of children (B, for non-leaf nodes and L for leaf nodes) Threshold (T): max diameter of sub-clusters stored at the leaf nodes The CF Tree Structure Figure: T. Zhang et al. The Birch Algorithm Cluster Diameter For each point in the input (processed in the order they appear, sequentially) Find closest leaf entry Add point to leaf entry and update CF If entry diameter > max_diameter, then split leaf (take the two farthest points in the leaf cluster and seeds for the two new clusters and re-assign points to the closest seed. If splitting the leaf exceeds it branching factor (L), add a new non-leaf entry to the parent. If this exceeds the parents branching factor (B), then split the parent (and recursively split other non-leaf nodes (grand parents) on the path as needed) Algorithm is O(n) each data point is processed only once, sequentially Concerns Sensitive to insertion order of data points Since we fix the size of leaf nodes, so clusters may not be so natural Clusters tend to be spherical given the radius and diameter measures CHAMELEON:. Hierarchical Clustering Using Dynamic Modeling (1999) CHAMELEON:. G. Karypis, E. H. Han, and V. Kumar, 1999 Measures the similarity based on a dynamic model Two clusters are merged only if the interconnectivity and closeness (proximity) between two clusters are high relative to the internal interconnectivity of the clusters and closeness of items within the clusters Graph-based, and a two-phase algorithm Use a graph-partitioning algorithm: cluster objects into a large number of relatively small sub-clusters Use an agglomerative hierarchical clustering algorithm: find the genuine clusters by repeatedly combining these sub-clusters Overall Framework of CHAMELEON CHAMELEON. ( Clustering Complex Objects) Probabilistic Hierarchical Clustering Algorithmic hierarchical clustering Nontrivial to choose a good distance measure Hard to handle missing attribute values Optimization goal not clear: heuristic, local search Probabilistic hierarchical clustering Use probabilistic models to measure distances between clusters Generative model: Considers the set of data objects to be clustered as a sample of some underlying data generation mechanism. Easy to understand, same eﬀiciency as algorithmic agglomerative clustering method, can handle partially observed data. In practice, assume the generative models adopt common distributions functions, e.g., Gaussian distribution or Bernoulli distribution, governed by parameters Generative Model Given a set of 1-D points X = {x1, ..., xn} for clustering analysis & assuming they are generated by a Gaussian distribution: The probability that a point xi ∈ X is generated by the model The likelihood that X is generated by the model: The task of learning the generative model is to find the parameters µ and σ2 that maximizes the likelihood. A Probabilistic Hierarchical Clustering Algorithm For a set of objects partitioned into m clusters C1, . . ,Cm, the quality can be measured by, where P() is the maximum likelihood Distance between clusters Ci and Cj: 1. Cluster analysis: Basic concepts 2. Partitioning methods 3. Hierarchical methods 4. Density-based methods 5. Grid-based methods 6. Evaluation of clustering 7. Summary and outlook Next lecture 1. Cluster analysis: Basic concepts 2. Partitioning methods 3. Hierarchical methods 4. Density-based methods 5. Grid-based methods 6. Evaluation of clustering 7. Summary and outlook Density-based methods DBSCAN OPTICS DENCLUE. Density-Based Clustering Methods Clustering based on density (local cluster criterion), such as density-connected points Major features: Discover clusters of arbitrary shape Handle noise One scan Need density parameters Several interesting studies: DBSCAN:. Ester, et al. (KDDʼ96) OPTICS:. Ankerst, et al (SIGMODʼ99). DENCLUE:. Hinneburg & D. Keim (KDDʼ98) CLIQUE:. Agrawal, et al. (SIGMODʼ98) (more grid-based) Density-Based Clustering: Basic Concepts Two parameters: Eps: Maximum radius of the neighbourhood of some datapoint p. MinPts: Minimum number of points in the Eps-neighbourhood (NEps(p) ) of that data point p. NEps(p): {q belongs to D | dist(p,q) ≤ Eps} If |NEps(p)| ≥ MinPts, then p is called a core point. Directly density-reachable: A point p is directly density-reachable from a point q w.r.t. Eps, MinPts if p belongs to NEps(q) core point condition: |NEps (q)| ≥ MinPts Density-Reachable and Density-Connected Density-reachable: A point p is density-reachable from a point q w.r.t. Eps, MinPts if there is a chain of points p1, ..., pn, p1 = q, pn = p such that pi+1 is directly density-reachable from pi Density-connected A point p is density-connected to a point q w.r.t. Eps, MinPts if there is a point o such that both, p and q are density-reachable from o w.r.t. Eps and MinPts DBSCAN DBSCAN:. Density-Based Spatial Clustering of Applications with Noise Relies on a density-based notion of cluster: A cluster is defined as a maximal set of density-connected points Discovers clusters of arbitrary shape in spatial databases with noise DBSCAN:. The Algorithm Arbitrary select a point p If p is a core point, Retrieve all points density-reachable from p w.r.t. Eps and MinPts Form a cluster If p is a border point, no points are density-reachable from p and DBSCAN. visits the next point of the database Continue the process until all of the points have been processed DBSCAN:. Sensitive to Parameters OPTICS OPTICS:. A Cluster-Ordering Method (1999) OPTICS:. Ordering Points To Identify the Clustering Structure Ankerst, Breunig, Kriegel, and Sander (SIGMODʼ99) Produces a special order of the database w.r.t. its density-based clustering structure. This cluster-ordering contains information equivalent to the density-based clusterings corresponding to a broad range of parameter settings. Good for both automatic and interactive cluster analysis, including finding intrinsic clustering structure. Can be represented graphically or using visualization techniques. OPTICS:. Some Extension from DBSCAN. Core Distance: Minimum radius s.t. point is core (undefined if point is not core w.r.t. Eps and MinPts) Reachability Distance from point o to p. = Max (core-distance (o), d (o, p)) OPTICS:. Some Extension from DBSCAN. Core Distance: Minimum radius s.t. point is core (undefined if point is not core w.r.t. Eps and MinPts) Reachability Distance from point o to p. = Max (core-distance (o), d (o, p)) To retrieve the Eps-neighbourhood of a point o, a region query with o as the center and Eps as the radius is used. Uses a special index such as R*-tree, X-tree or M-tree to do this eﬀiciently. Complexity: O(NlogN) Otherwise it will be O(N2) OPTICS:. Main Steps Maintains a list (priority queue) called ordered_seeds, where objects are sorted in the ascending order of their reachability distance to their respective closest core point. Picks an arbitrary datapoint p from the Dataset. Retrieves the Eps_neighbourhood(p) Computes the core_distance(p) and sets reachability_distance(p) = “undefined”. Write p to output (cluster order of data objects). If p is not a core object: move to next object in ordered_seeds (or from the dataset if ordered_seeds is empty). If p is a core object: For each object q in Eps_neighbourhood(p): If q is not in ordered_seeds: Set reachability_distance(q)= reachability_distance(p,q) and add q to the appropriate location in ordered_seeds If q is in ordered_seeds: Set reachability_distance(q)= min{reachability_distance(p,q), reachability_distance(q)} move q to its correct place in ordered_seeds if reachability_distance(q) has decreased Output of OPTICS. Density-Based Clustering: OPTICS. & Its Applications DENCLUE DENCLUE:. Using Statistical Density Functions DENsity-based CLUstEring by Hinneburg & Keim (KDDʼ98) Uses statistical density functions: DENCLUE:. Using Statistical Density Functions DENsity-based CLUstEring by Hinneburg & Keim (KDDʼ98) Using statistical density functions: Major features Solid mathematical foundation Good for data sets with large amounts of noise Allows a compact mathematical description of arbitrarily shaped clusters in high-dimensional data sets Significant faster than existing algorithm (e.g., DBSCAN). But needs a large number of parameters Density Attractor Density attractors - local density maxima Density attractor of point x the local density maximum reached by a gradient ascend starting from x. Density Attractor Density attractor of point x is the local density maximum reached by a gradient accent starting from x. DENCLUE. - Clustering Center based clusters The set of density attractors X, forms the set of cluster centers. Each datapoint is assigned to the cluster represented by its density attractor Arbitrary shaped clusters Density attractors are connected to merge clusters. Two density attractors are connected if there's a high density path between them. Center-Deﬁned and Arbitrary Denclue: Technical Essence Influence function: describes the impact of a data point within its neighborhood Overall density of the data space can be calculated as the sum of the influence function of all data points Clusters can be determined mathematically by identifying density attractors Density attractors are local maximum of the overall density function Center defined clusters: assign to each density attractor the points density attracted to it Arbitrary shaped cluster: merge density attractors that are connected through paths of high density (> threshold) Uses grid cells but only keeps information about grid cells that do actually contain data points and manages these cells in a tree-based access structure Grid-based Clustering Dr. Thanuja Ambegoda Senior Lecturer Department of Computer Science and Engineering University of Moratuwa Based on the textbook and slides by Jiawei Han, Micheline Kamber, and Jian Pei University of Illinois at Urbana-Champaign & Simon Fraser University Grid-Based Clustering Method Using multi-resolution grid data structure Several interesting methods STING. ( a STatistical INformation Grid approach) by Wang, Yang and Muntz (1997) WaveCluster by Sheikholeslami, Chatterjee, and Zhang (VLDBʼ98) A multi-resolution clustering approach using wavelet method CLIQUE:. Agrawal, et al. (SIGMODʼ98) Both grid-based and subspace clustering STING:. A Statistical Information Grid Approach Wang, Yang and Muntz (VLDBʼ97) The spatial area is divided into rectangular cells There are several levels of cells corresponding to diﬀerent levels of resolution The STING. Clustering Method Each cell at a high level is partitioned into a number of smaller cells in the next lower level Statistical info of each cell is calculated and stored beforehand and is used to answer queries count, mean, stdv, min, max type of distribution-normal, uniform, etc. Statistics of higher-level cells can be easily calculated from those of lower-level cell Use a top-down approach to answer spatial data queries STING. Algorithm and Its Analysis Start from a pre-selected layer-typically with a small number of cells. For each cell in the current level, compute how relevant it is to the query. E.g., use a confidence interval. Remove the irrelevant cells from further consideration. Proceed to the next level using only the relevant cells. Repeat this process until the bottom layer is reached. If the query specifications are met, return the regions of the relevant cells in the bottom layer. If not, the data points in the relevant cells are retrieved for further processing. STING. Algorithm and Its Analysis Advantages: Query-independent, easy to parallelize, incremental update O(K), where K is the number of grid cells at the lowest level Disadvantages: All the cluster boundaries are either horizontal or vertical, and no diagonal (or arbitrary) boundaries are detected CLIQUE. ( Clustering In QUEst) Agrawal, Gehrke, Gunopulos, Raghavan (SIGMODʼ98) Automatically identifying subspaces of a high dimensional data space that allow better clustering than original space. CLIQUE. can be considered as both density-based and grid-based It partitions each dimension into the same number of equal length intervals It partitions a d-dimensional data space into non-overlapping rectangular cells A cell(also called a unit) is dense if the fraction of total data points contained in the cell exceeds the some threshold value A cluster is a maximal set of connected dense cells within a subspace CLIQUE:. The Major Steps Partition the d-dimensional data space into non-overlapping rectangular cells. Identify dense cells in all subspaces. I.e., cells with at least l number of data points in them. Uses the apriori principle: A k-dimensional cell is dense only if all of its (k-1)-dimensional projections are dense. Find all 1-dimensional dense cells in all dimensions Iteratively join k-dimensional dense cells to form candidate (k+1)-dimensional dense cells. If c1 and c2 are two dense cells in k-dimensions [d1, d2,...dk-1, a] and [d1, d2,...dk-1, b], and share the same intervals in their common dimensions, then c1 and c2 are joined to form a candidate for a dense cell in the (k+1) dimensional space [d1, d2,...dk-1, a, b]. CLIQUE:. The Major Steps Once the dense cells in each subspace of interest are found, form clusters by connecting them. A cluster is a maximal set of connected dense cells in k-dimensions. Two k-dimensional cells c1,c2 are connected if they have a common face or if there exists another k-dimensional cell c3 such that c1 is connected to c3 and c2 is connected to c3. This is known to be an NP-hard problem. CLIQUE. uses a greedy approach to tackle it. Strength and Weakness of CLIQUE. Strengths automatically finds subspaces of the highest dimensionality such that high density clusters exist in those subspaces insensitive to the order of records in input and does not presume some canonical data distribution scales linearly with the size of input and has good scalability as the number of dimensions in the data increases Weakness The accuracy of the clustering result depends heavily on the choice of the number and widths of the grid cells and the density threshold."
  },
  {
    "index": 4,
    "level": 1,
    "start_page": 212,
    "end_page": 266,
    "title": "07 Outlier Detection",
    "content": "Outlier Detection. Jian Pei Simon Fraser University Outlier Detection • “In 2017, an astronomical event occurred that was unlike any other: for the first time, we observed an object that we are certain originated from beyond our Solar System.” Jian Pei: Data Mining -- Outlier Detection 'Oumuamua Fraud detection • Credit card transaction fraud detection • Unusual amounts • Unusual locations/time • Fraud detection in stock markets • Example: unusual chain transactions by a small group of connected users • Fraud detection in healthcare insurance • A child and the parents frequently see the same medical doctor at the same time • A patient sees a medical doctor for flu every week Jian Pei: Data Mining -- Outlier Detection Outlier Analysis • “One person's noise is another person's signal” • Outliers: the objects considerably dissimilar from the remainder of the data Jian Pei: Data Mining -- Outlier Detection Outliers and Noise • Different from noise • Noise is random error or variance in a measured variable • Outliers are interesting: an outlier violates the mechanism that generates the normal data • Outlier detection vs. novelty detection • Early stage may be regarded as outliers • But later merged into the model Jian Pei: Data Mining -- Outlier Detection Types of Outliers • Three kinds: global, contextual and collective outliers • A data set may have multiple types of outlier • One object may belong to more than one type of outlier • Global outlier (or point anomaly) • An outlier object significantly deviates from the rest of the data set • challenge: find an appropriate measurement of deviation Jian Pei: Data Mining -- Outlier Detection Contextual Outliers • An outlier object deviates significantly based on a selected context • Ex. Is 10C in Vancouver an outlier? (depending on summer or winter?) • Attributes of data objects should be divided into two groups • Contextual attributes: defines the context, e.g., time & location • Behavioral attributes: characteristics of the object, used in outlier evaluation, e.g., temperature • A generalization of local outliers-whose density significantly deviates from its local area • Challenge: how to define or formulate meaningful context? Jian Pei: Data Mining -- Outlier Detection Collective Outliers • A subset of data objects collectively deviate significantly from the whole data set, even if the individual data objects may not be outliers • Application example: intrusion detection when a number of computers keep sending denial-of-service packages to each other • Detection of collective outliers • Consider not only behavior of individual objects, but also that of groups of objects • Need to have the background knowledge on the relationship among data objects, such as a distance or similarity measure on objects Jian Pei: Data Mining -- Outlier Detection Outlier Detection: Challenges • Modeling normal objects and outliers properly • Hard to enumerate all possible normal behaviors in an application • The border between normal and outlier objects is often a gray area • Application-specific outlier detection • Choice of distance measure among objects and the model of relationship among objects are often application-dependent • Example: in clinic data a small deviation could be an outlier; while in marketing analysis, larger fluctuations Jian Pei: Data Mining -- Outlier Detection Outlier Detection: Challenges • Handling noise in outlier detection • Noise may distort the normal objects and blur the distinction between normal objects and outliers • Noise may help hide outliers and reduce the effectiveness of outlier detection • Interpretability • Understand why these are outliers: Justification of the detection • Specify the degree of an outlier: the unlikelihood of the object being generated by a normal mechanism Jian Pei: Data Mining -- Outlier Detection Outlier Detection Methods • Whether user-labeled examples of outliers can be obtained • Supervised, semi-supervised, and unsupervised methods • Assumptions about normal data and outliers • Statistical, proximity-based, and clustering- based methods Jian Pei: Data Mining -- Outlier Detection Supervised Methods • Modeling outlier detection as a classification problem • Samples examined by domain experts used for training & testing • Methods for Learning a classifier for outlier detection effectively: • Model normal objects & report those not matching the model as outliers, or • Model outliers and treat those not matching the model as normal • Challenges • Imbalanced classes, i.e., outliers are rare: Boost the outlier class and make up some artificial outliers • Catch as many outliers as possible, i.e., recall is more important than accuracy (i.e., not mislabeling normal objects as outliers) Jian Pei: Data Mining -- Outlier Detection Unsupervised Methods • Assume the normal objects are somewhat ``clustered'' into multiple groups, each having some distinct features • An outlier is expected to be far away from any groups of normal objects • Weakness: Cannot detect collective outlier effectively • Normal objects may not share any strong patterns, but the collective outliers may share high similarity in a small area • Many clustering methods can be adapted for unsupervised methods • Find clusters, then outliers: not belonging to any cluster Jian Pei: Data Mining -- Outlier Detection Unsupervised Methods: Challenges Semi-Supervised Methods • In many applications, the number of labeled data is often small • Labels could be on outliers only, normal objects only, or both • If some labeled normal objects are available • Use the labeled examples and the proximate unlabeled objects to train a model for normal objects • Those not fitting the model of normal objects are detected as outliers • If only some labeled outliers are available, a small number of labeled outliers many not cover the possible outliers well • To improve the quality of outlier detection, one can get help from models for normal objects learned from unsupervised methods Jian Pei: Data Mining -- Outlier Detection Statistical/Model-based Methods • Make assumptions of data normality: normal data objects are generated by a statistical (stochastic) model, and that data not following the model are outliers • Effectiveness of statistical methods highly depends on whether the assumption of statistical model holds in the real data • There are many kinds of statistical models • Parametric vs. non-parametric Jian Pei: Data Mining -- Outlier Detection Proximity-based Methods • An object is an outlier if the nearest neighbors of the object are far away, i.e., the proximity of the object significantly deviates from the proximity of most of the other objects in the same data set • The effectiveness of proximity-based methods highly relies on the proximity measure • In some applications, proximity or distance measures cannot be obtained easily • Often have a difficulty in identifying a group of outliers that stay close to each other • Two major types of proximity-based outlier detection methods • Distance-based vs. density-based Jian Pei: Data Mining -- Outlier Detection Reconstruction-based Methods • Idea: sine the normal data samples often share certain similarities, they can often be represented in a more succinct way, compared with their original representation • Samples which cannot be well reconstructed by such alternative, succinct representation are regarded outliers • Two types of reconstruction-based outlier detection methods • Matrix-factorization based methods for numeric data • Pattern-based compression methods for categorical data Jian Pei: Data Mining -- Outlier Detection Statistical Approaches • Learn a generative model fitting the given data set, and then identify those objects in low-probability regions of the model as outliers • Two categories • A parametric method assumes that the normal data objects are generated by a parametric distribution with a finite number of parameters Θ • A nonparametric method tries to determine the model from the input data • A nonparametric method is not completely parameter-free Jian Pei: Data Mining -- Outlier Detection Detection of Univariate Outliers Based on Normal Distribution • Univariate outlier detection using maximum likelihood method • Example • A city's average temperature values in July in the last 10 years are, in value- ascending order, 24.0◦C, 28.9 ◦C, 28.9 ◦C, 29.0 ◦C, 29.1 ◦C, 29.1 ◦C, 29.2 ◦C, 29.2 ◦C, 29.3 ◦C, and 29.4 ◦C • Model assumption: the average tem- perature follows a normal distribution 𝜇𝜇, 𝜎𝜎! • Task: estimate the paramters 𝜇𝜇and 𝜎𝜎, that is, maximize the log-likelihood function Jian Pei: Data Mining -- Outlier Detection Maximum Likelihood Estimates • Results Jian Pei: Data Mining -- Outlier Detection Outlier Detection • The most deviating value, 24.0◦C, is 4.61◦C away from the estimated mean • Model assumption: μ ± 3σ region contains 99.7% data under the assumption of normal distribution • Because !.#$ $.%$ = 3.04 > 3, the probability that the value 24.0◦C is generated by the normal distribution is less than 0.15% -- it is an outlier Jian Pei: Data Mining -- Outlier Detection Boxplot • A five-number summary: the smallest nonoutlier value (Min), the lower quartile (Q1), the median (Q2), the upper quartile (Q3), and the largest nonoutlier value (Max) • The interquantile range (IQR) is defined as Q3 - Q1 • Any object that is more than 1.5 × IQR smaller than Q1 or 1.5 × IQR larger than Q3 is treated as an outlier because the region between Q1 - 1.5 × IQR and Q3 + 1.5 × IQR contains 99.3% of the objects Jian Pei: Data Mining -- Outlier Detection Grubb's Test (Maximum Normed Residual Test) • z-score 𝑧𝑧= |'()| • An object is an outlier if • where 𝑡𝑡⁄ # (!%),%(! is the value taken by a t-distribution at a significance level of α/(2n), and n is the number of objects in the data set Jian Pei: Data Mining -- Outlier Detection Detection of Multivariate Outliers Using the Mahalanobis Distance • Let ̅𝑜𝑜be the sample mean vector • For an object o, the squared Mahalanobis distance from o to ̅𝑜𝑜is Mdist o, ̅𝑜𝑜= 𝑜𝑜-̅𝑜𝑜+𝑆𝑆($(𝑜𝑜-̅𝑜𝑜), where S is the sample covariance matrix • Detect outliers using the univariate variable Mdist o, ̅𝑜𝑜 Jian Pei: Data Mining -- Outlier Detection Detection of Multivariate Outliers Using the χ2-statistic • For an object o, the χ2-statistic is 𝜒𝜒, = ∑-.$ 0!(1! \" • 𝑜𝑜) is the value of o on the i-th dimension • 𝐸𝐸) is the mean of the i-th dimension among all objects • n is the dimensionality • The larger the χ2-statistic, the more outlying the object Jian Pei: Data Mining -- Outlier Detection Detection Using a Mixture of Parametric Distributions • Model assumption: the normal data objects are generated by multiple normal distributions • Using EM (expectation-maximization) algorithm to learn the parameters Jian Pei: Data Mining -- Outlier Detection Non-parametric Method • Not assume an a-priori statistical model, instead, determine the model from the input data • Not completely parameter free but consider the number and nature of the parameters are flexible and not fixed in advance • Examples: histogram and kernel density estimation Jian Pei: Data Mining -- Outlier Detection Histogram • A transaction in the amount of $7,500 is an outlier, since only 0.2% transactions have an amount higher than $5,000 • Hard to choose an appropriate bin size for histogram • Too small bin size → normal objects in empty/rare bins, false positive • Too big bin size → outliers in some frequent bins, false negative Jian Pei: Data Mining -- Outlier Detection Kernel Density Estimation • Treat an observed object as an indicator of high probability density in the surrounding region • The probability density at a point depends on the distances from this point to the observed objects • Use a kernel function to model the influence of a sample point within its neighborhood • A kernel K() is a non-negative real-valued integrable function that satisfies two conditions • ∫!\" \" 𝐾𝐾𝑥𝑥𝑑𝑑𝑑𝑑= 1 • 𝐾𝐾-𝑥𝑥= 𝐾𝐾(𝑥𝑥) for any x Jian Pei: Data Mining -- Outlier Detection Kernel Density Estimation • Using a kernel function K, estimate by • A frequently used kernel is a standard Gaussian function with mean 0 and variance 1 Jian Pei: Data Mining -- Outlier Detection ˆf(x) = 1 i=1 ⇣x -Xi Proximity-based Outlier Detection • Objects far away from the others are outliers • The proximity of an outlier deviates significantly from that of most of the others in the data set • Distance-based outlier detection: An object o is an outlier if its neighborhood does not have enough other points • Density-based outlier detection: An object o is an outlier if its density is relatively much lower than that of its neighbors Jian Pei: Data Mining -- Outlier Detection Distance-based Outliers • A DB(p, D)-outlier is an object O in a dataset T such that at least a fraction p of the objects in T lie at a distance greater than distance D from O • The larger D, the more outlying • The larger p, the more outlying Jian Pei: Data Mining -- Outlier Detection Drawback of Distance- based Methods • Both o1 and o2 are outliers • Distance-based methods can detect o1, but not o2 Jian Pei: Data Mining -- Outlier Detection Density-based Methods: Intuition • Outliers comparing to their local neighborhoods, instead of the global data distribution • The density around an outlier object is significantly different from the density around its neighbors • Use the relative density of an object against its neighbors as the indicator of the degree of the object being outliers Jian Pei: Data Mining -- Outlier Detection K-Distance • The k-distance of p is the distance between p and its k-th nearest neighbor • In a set D of points, for any positive integer k, the k-distance of object p, denoted as k-distance(p), is the distance d(p, o) between p and an object o such that • For at least k objects o' Î D \\ {p}, d(p, o') £ d(p, o) • For at most (k-1) objects o' Î D \\ {p}, d(p, o') < d(p, o) Jian Pei: Data Mining -- Outlier Detection K-distance Neighborhood • Given the k-stance of p, the k-distance neighborhood of p contains every object whose distance from p is not greater than the k-distance • Nk-distance(p)(p) = {q Î D\\{p} | d(p, q) £ k-distance(p)} • Nk-distance(p)(p) can be written as Nk(p) Jian Pei: Data Mining -- Outlier Detection Jian Pei: Data Mining -- Outlier Detection Reachability Distance • The reachability distance of object p with respect to object o is reach- distk(p, o) = max{k-distance(o), d(p, o)} If p and o are close to each other, reach-dist(p, o) is the k-distance, otherwise, it is the real distance Local Reachability Density Jian Pei: Data Mining -- Outlier Detection Local outlier factor lrdk(o) = | Nk(o) | o02Nk(o) reachdistk(o0 o) Reconstruction-based Approaches • Find a succinct representation • Use the succinct representation to reconstruct the original data samples • Measure the quality (i.e., goodness) of reconstruction Jian Pei: Data Mining -- Outlier Detection Matrix Factorization Based Methods for Numeric Data Jian Pei: Data Mining -- Outlier Detection Singular Vector Decomposition (SVD) 𝑋𝑋≈𝑈𝑈Σ𝑉𝑉𝑉 Jian Pei: Data Mining -- Outlier Detection Clustering-based Outlier Detection • An object is an outlier if • It does not belong to any cluster; • There is a large distance between the object and its closest cluster ; or • It belongs to a small or sparse cluster Jian Pei: Data Mining -- Outlier Detection Classification-based Outlier Detection • Train a classification model that can distinguish “normal” data from outliers • A brute-force approach: Consider a training set that contains some samples labeled as “normal” and others labeled as “outlier” • A training set in practice is typically heavily biased: the number of “normal” samples likely far exceeds that of outlier samples • Cannot detect unseen anomaly Jian Pei: Data Mining -- Outlier Detection One-Class Model • A classifier is built to describe only the normal class • Learn the decision boundary of the normal class using classification methods such as SVM • Any samples that do not belong to the normal class (not within the decision boundary) are declared as outliers • Advantage: can detect new outliers that may not appear close to any outlier objects in the training set • Extension: Normal objects may belong to multiple classes Jian Pei: Data Mining -- Outlier Detection Semi-Supervised Learning Methods • Combine classification-based and clustering-based methods • Method • Use a clustering-based approach to find a large cluster, C, and a small cluster, C1 • Since some objects in C carry the label “normal”, treat all objects in C as normal • Use the one-class model of this cluster to identify normal objects in outlier detection • Since some objects in cluster C1 carry the label “outlier”, declare all objects in C1 as outliers • Any object that does not fall into the model for C (such as a) is considered an outlier as well Jian Pei: Data Mining -- Outlier Detection Contextual Outliers • An outlier object deviates significantly based on a selected context • Ex. Is 10C in Vancouver an outlier? (depending on summer or winter?) • Attributes of data objects should be divided into two groups • Contextual attributes: defines the context, e.g., time & location • Behavioral attributes: characteristics of the object, used in outlier evaluation, e.g., temperature • A generalization of local outliers-whose density significantly deviates from its local area • Challenge: how to define or formulate meaningful context? Jian Pei: Data Mining -- Outlier Detection Detection of Contextual Outliers • If the contexts can be clearly identified, transform it to conventional outlier detection • Identify the context of the object using the contextual attributes • Calculate the outlier score for the object in the context using a conventional outlier detection method Jian Pei: Data Mining -- Outlier Detection Example • Detect outlier customers in the context of customer groups • Contextual attributes: age group, postal code • Behavioral attributes: the number of transactions per year, annual total transaction amount • Method • Locate c's context; • Compare c with the other customers in the same group; and • Use a conventional outlier detection method Jian Pei: Data Mining -- Outlier Detection Modeling Normal Behavior • Model the “normal” behavior with respect to contexts • Use a training data set to train a model that predicts the expected behavior attribute values with respect to the contextual attribute values • An object is a contextual outlier if its behavior attribute values significantly deviate from the values predicted by the model • Use a prediction model to link the contexts and behavior • Avoid explicit identification of specific contexts • Some possible methods: regression, Markov Models, and Finite State Automaton ... Jian Pei: Data Mining -- Outlier Detection Collective Outliers • Objects as a group deviate significantly from the entire data • Examine the structure of the data set, i.e, the relationships between multiple data objects • The structures are often not explicitly defined and have to be discovered as part of the outlier detection process. Jian Pei: Data Mining -- Outlier Detection Detecting High Dimensional Outliers • Interpretability of outliers • Which subspaces manifest the outliers or an assessment regarding the “outlying-ness” of the objects • Data sparsity: data in high-D spaces are often sparse • The distance between objects becomes heavily dominated by noise as the dimensionality increases • Data subspaces • Local behavior and patterns of data • Scalability with respect to dimensionality • The number of subspaces increases exponentially Jian Pei: Data Mining -- Outlier Detection HilOut • Find distance-based outliers, but uses the ranks of distance instead of the absolute distance in outlier detection • For each object, o, find the k-nearest neighbors of o, denoted by nn1(o), . . , nnk(o), where k is an application-dependent parameter • The weight of o is • All objects are ranked in weight-descending order • The top-l objects in weight are outliers Jian Pei: Data Mining -- Outlier Detection Angle-based Outliers Jian Pei: Data Mining -- Outlier Detection Summary • Outlier detection and applications • Types of outliers • Statistical methods • Proximity-based methods • Clustering- and classification-based methods • Contextual outlier and collective outliers • Outlier detection for high-dimensional data Jian Pei: Data Mining -- Outlier Detection"
  },
  {
    "index": 5,
    "level": 1,
    "start_page": 267,
    "end_page": 314,
    "title": "09 NoSQL Examples",
    "content": "Class 7 Firestore Elements of Databases Mar 25, 2022 Slides Taken from https://www.cs.utexas.edu/~scohen/cs327e.html The \"NoSQL Movement\" Need for greater scalability Throughput Response time More expressive data models and schema flexibility Object-relational mismatch Preference for open-source software Source: schema.org Firestore Overview + Distributed system + Fully \"serverless\" + Simple APIs for reading and writing + Supports ACID transactions (uses Spanner behind the scenes) + Designed for mobile, web and IoT apps + Implements document model + Change data capture for documents + Inexpensive - Only runs on Google Cloud - Write throughput limits (10K writes/sec) Firestore's Document Model Firestore document == collection of typed <key, value> pairs Primitive types: String, Number, Bool, Timestamp Complex types: Array, Map, Geopoint Documents Concepts: grouped into collections same type documents can have different schemas assigned unique identifiers (id) store hierarchical data in subcollections Writing Single Documents Every document has unique identifier of String type The set method converts a Python dictionary into Firestore document A document write must also update any existing indexes on the collection Writing Nested Documents Subcollections are nested under documents Subcollections can be nested under other subcollections (max depth = 100) Writing Multiple Documents Write batches of documents in increments of 500 or less Batches can contain documents for multiple collections Reading Single Documents The get method fetches a single document The stream method fetches all documents in collection or subcollection stream + where methods filter documents in collection or subcollection order_by method for sorting results limit method for limiting number retrieved All reads require an index, query will fail if an index does not exist Reading Multiple Documents from google.cloud import firestore db= firestore.Client() authors_ref = db.collection('authors') query= authors_ref.where('seniority', results= query.stream() 8T for doc in results: , __ , -- , print('Document: ' + str(doc.to_dict())) from google.cloud import firestore db= firestore.Client() authors_ref = db.collection('authors') 'L3').order_by('last_name').limit(S) query= authors_ref.where( 'secondary_specialties', 'array_contains', 'Business') \\ .where('articles_to_date', '>', 100) results= query.stream() 9T for doc in results: print('Document: ' + str(doc.to_dict())) Updates and Deletes from google.cloud import firestore db= firestore.Client() author_ref = db.collection( 1 authors 1).document( 1sarah.asch 1 ) author _ref. update ( { 1 a rticles_to_date 1 : f irestore. Increment ( 1), 1 seniority 1 11 L411 }) from google.cloud import firestore db= firestore.Client() author_ref = db.collection( 1 authors 1 ).document( 1 sarah.asch 1 author_ref.update({ 1 articles_to_date 1 : firestore.DELETE_FIELD}) from google.cloud import firestore db= firestore.Client() db.collection( 1authors 1 ).document( 1 sarah.asch 1 ).delete() College Schema Convert this relational model to Firestore. Read access patterns: Get classes by cname Get students and their classes by sid Get instructor and their classes by tid College Schema Access patterns: Get classes by cname Get students and their classes by sid Get instructor and their classes by tid Converted relational model to Firestore. Design Guidelines for Document Databases Identify the expected access patterns against the database. For each access pattern, group entities into a hierarchy: top-level and lower-level types. Convert each top-level entity into a Firestore collection. Convert each lower-level entity into a Firestore subcollection nested in its parent collection. Construct a single unique identifier for each document by using the Primary Key column as is or concatenating multiple Primary Key columns. Exercise: Data Modeling Convert Shopify schema to Firestore. Access patterns: Get apps by category (Category.title) Get apps with highest review_count Get pricing plan details by app (Apps.id) Get key benefits by app (Apps.id) Class 7 MongoDB Elements of Databases Apr 1, 2022 Slides Taken from https://www.cs.utexas.edu/~scohen/cs327e.html The CAP Theorem Theorem: You can have at most two of these properties for any database system. Eric Brewer, PODC keynote, July 2000. MongoDB Overview Distributed database system Open-source software (sponsored by MongoDB Inc.) Designed for storing and processing web data Document-oriented data model \"Schemaless\" (schema-on-reads) Rich query language Secondary indexes Horizontal scaling through replication and sharding Runs on-premise and in cloud (Atlas offering) Primary datastore for many web applications Multi-document transaction support Sharding is not automatic Replication in MongoDB High-availability Redundancy Automatic failovers Load balancing reads Sharding in MongoDB shard key = one or more fields of a document which determine how documents get sliced Documents with the same shard key are assigned to the same chunk Chunks are assigned to a shard Key Range Chunk Shard 0...20 21...40 41...60 61...80 81...100 Sharding + Replication Each shard is deployed as a replica set Scales both reads and writes Widely used in prod environments Data Model MongoDB Document = BSON object Unordered key/value pairs with nesting Documents have unique identifiers (_id) Data types: String, Int, Double, Boolean, Date, Timestamp, Array, Object, ObjectId Documents are nested via Object type Max document size: 16 MB (including nested objects) Documents grouped into collections Collections grouped into databases \"_id\" : ObjectId(\"5f807ab092ea454d1100d13a\"), \"name\" : { \"first\" : \"Jim\", \"last\" : \"Gray\" \"nationality\" : \"American\", \"born\" : Date(\"1944-01-12\"), \"employers\" : [ \"Microsoft\", \"DEC\", \"Tandem\", \"IBM\" \"contributions\" : [ \"database transactions\", \"OLAP cube\" Inserts db.coll.insertOne(document) db.coll.insert([document1, document2, documentn]) db.coll.insertMany([document1, document2, documentn]) Legend: Document, Output Inserts Legend: Document, Output > doc2 = {\"company name\": \"Google Inc.\", \"exchange\": \"NASDAQ\",. \" symbol\": \"GOOG\", \"summary\": {\"date\": 20211022, \"open\": 2807.02, \"high\": 2831.17, \"low\": 2743.41}} \"company name\" : \"Google Inc.\", \"exchange\" : \"NASDAQ\",. \" symbol\" : \"GOOG\", \"summary\" : { \"date\" 20211022, \"open\" 2807.02, \"high\" 2831.17, \"low\" : 2743.41 > doc3 = {\"company name\": \"Google Inc.\", \"symbol\": \"GOOG\", \"exchange\": \"NASDAQ\",. \" summary\": [{\"date\": 20201007, \"open\" 1464.29, \"high\": 1468.96, \"low\": 1461.47}, {\"date\": 20201006, \"open\": 1476.89, \"high\": 1480.93, \"low\": 1453.44}]} \"company name\" : \"Google Inc.\", \"symbol\" : \"GOOG\", \"exchange\" : \"NASDAQ\",. \" summary\" : [ \"date\" 20201007, ''open'' 1464.29, \"high\" 1468.96, \"low\" : 1461.47 > db.rnarket.insertMany([doc2, doc3)) \"acknowledged\" : true, } ' \"date\" 20201006, ''open'' 1476.89, \"high\" 1480.93, \"insertedids\" Objectid(\"624744a17dbfdcf5f676721c\"), Objectid(\"624744a17dbfdcf5f676721d\") \"low\" : 1453.44 Reads db.coll.findOne(selection, projection) db.coll.find(selection, projection) Legend: Selection, Projection, Output Nested Queries Legend: Selection, Projection, Output >selection= {\"summary.date\": 20211022} { \"summary.date\" : 20211022 } > projection = {\"summary. date\" : 1, \"summary. open\" : 1, \"summary. high\" : { \"summary.date\" : 1, \"summary.open\" : 1, \"summary.high\" : 1, \" id\" > db.market.find(selection, proJection) { \"summary\" : { \"date\" : 20211022, \"open\" : 2807.02, \"high\" : 2831.17 > selection = {\"summary.date\": 20211022, \"symbol\": \"GOOG\"} { \"summary.date\" : 20211022, \"symbol\" : \"GOOG\" } > proJec ion summary. a e : summary.open : summary. 1, _id:0} 0 } } } { \"summary.date\" : 1, \"summary.open\" : 1, \"summary.high\" : 1, \" id\" 0 } ection, proJection).pretty() \"summary\" : \"date\" \"open\" \"high\" 20211022, 2807.02, 2831.17 Or Queries Boolean Operators: $or $and Legend: Selection, Projection, Output Range Queries Range operators: $lt $gt $lte $gte Legend: Selection, Projection, Output Updates db.coll.update(selection, update) db.coll.updateMany(selection, update) Legend: Selection, Update Deletes db.coll.deleteOne(selection) db.coll.deleteMany(selection) Legend: Selection, Output Class 7 Neo4j Elements of Databases Apr 8, 2022 Slides Taken from https://www.cs.utexas.edu/~scohen/cs327e.html Neo4j Overview Labeled property graph database Highly connected data Declarative, SQL-inspired query language (Cypher) Open-source, sponsored by Neo4j Inc. Rich plugin and extension language (similar to Postgres) ACID-compliant transactions Distributed architecture for scaling reads Visualization tools (Neo4j Browser, Bloom) Optimized for graph traversals Available as a cloud offering (Aura) Limited scalability for writes (no sharding) “Hello World” in Cypher 1 CREATE ();. 2 CREATE (:. Person); 3 CREATE (:. Place); 5 MATCH(. n) RETURN. n; 7 CREATE (:. Person {name: 11Ethan11})-[:LIVES_IN]->(:Place {city: 11Austin11 }); 8 CREATE (:. Person {name: 11Sasha11})-[:LIVES_IN]->(:Place {city: 11New York11 }); 10 MATCH. ( p)-[r]->(c) 11 RETURN. p, type(r), c; 13 MATCH ()-[. r]->() 14 RETURN. type(r), COUNT(. r); 16 MATCH. ( p)-[r:LIVES_IN]->(c) 17 WHERE. p.name = 11Ethan11 18 AND c. city = 11Austin1 19 RETURN. p, r, c; IAM model: a labeled property graph example name: Ethan HAS_ROLE name: Project Owner HAS_PERMISSION name: storage.list name: storage.create name: storage.delete IN_GROUP name: Data Engineer name: DB Editor HAS_PERMISSION name: jobs.get name: jobs.create Creating the IAM Nodes Creating the Relationships 1 MATCH. ( p:Person {name: 1 1Ethan11 2 MATCH. ( r:Role {name: 11Owner11 3 CREATE. ( p)-[:HAS_ROLE]->(r); 5 MATCH. ( p:Person {name: 1 1Ethan11 6 MATCH. ( g:Group {name: 11Data Engineer11 7 CREATE. ( p}-[:IN_GROUP]->(g); 9 MATCH. ( g: Group {name: 11 Data Eng inee r11 10 MATCH. ( r:Role {name: 11DB Editor11 11 CREATE. ( g)-[:HAS_ROLE]->(r); 13 MATCH. ( p)-[h]->(r) RETURN. p, h, r; \"'-\"-R <;)·-OS -· ' \"\u000e\u000f name:Owner V name: Data Engineer name: storage.create +-------------------------------------------------------------------------------------------------------------------------+ I P I h I r +-------------------------------------------------------------------------------------------------------------------------+ I (:Person {name: \"Ethan\", email: \"ethan@utexas.edu\"}) I [:IN_GROUPJ I (:Group {owner: \"Alex\", name: \"Data Engineer\"}) I (:Person {name: \"Ethan\", email: \"ethan@utexas.edu\"}) I [:HAS_ROLEJ I (:Role {name: \"Owner\", resource: \"Project\"}) I (:Group {owner: \"Alex\", name: \"Data Engineer\"}) I [:HAS_ROLEJ I (:Role {name: \"DB Editor\", resource: \"Cloud SQL\"}) +-------------------------------------------------------------------------------------------------------------------------+ Creating the Relationships (cont.) 16 MATCH. ( r:Role {resource: 11Project11 17 MATCH. ( p:Permission {name: 11storage. list11 18 CREATE. ( r)-[:HAS_PERMISSION]->(p); 20 MATCH. ( r:Role {resource: 11Project11 21 MATCH. ( p:Permission {name: 11storage.create11 22 CREATE. ( r)-[:HAS_PERMISSION]->(p); 24 MATCH. ( r: Ro le {name: 110wne r11 25 MATCH. ( p:Permission {name: 1 1storage.delete11 26 CREATE. ( r)-[:HAS_PERMISSION]->(p); 28 MATCH. ( r:Role)-[h]->(p:Permission) 29 WHERE. r. resource = 11 Project II OR r. name 30 RETURN. r, h, p; 110wner11 -,-R fJ--,mm =1= ,\u0010,-\u0011\" name: Owner Role name: Data Engineer HAS_PE lSSION name: storage.list T-----------------------------------------------------------------------------------------------------------T I r I h I P +-----------------------------------------------------------------------------------------------------------+ I ( :Role {name: \"Owner\", resource: \"Project\"}) I [ :HAS_PERMISSIONJ I ( :Permission {name: \"storage.delete\"}) I I ( :Role {name: \"Owner\", resource: \"Project\"}) I [ :HAS_PERMISSIONJ I ( :Permission {name: \"storage.create\"}) I I ( :Role {name: \"Owner\", resource: \"Project\"}) I ( :HAS_PERMISSIONJ I ( :Permission {name: \"storage.list\"}) +-----------------------------------------------------------------------------------------------------------+ Creating the Relationships (cont.) MATCH. { r:Role {name: \"DB Editor\"}) MATCH. ( p:Permission {name: \"jobs.list\"}) CREATE. { r)-[:HAS_PERMISSION]->{p); MATCH. ( r:Role {name: \"DB Editor\"}) MATCH. ( p:Permission {name: \"jobs.get\"}) CREATE. ( r)-[:HAS_PERMISSION]->{p); MATCH. ( r:Role {name: \"DB Editor\"}) MATCH. { p:Permission {name: \"jobs.create\"}) CREATE. { r)-[:HAS_PERMISSION]->{p); =•'·MR HAS_ OLE IN_GRO Group name: Owner Role name: Data Engineer HAS_PERMISSION name: storage.create name: storage.delete 45 MATCH. { r:Role)-[h:HAS_PERMISSION]->{p:Permission) 46 WHERE. r.name = \"DB Editor\" 47 RETURN. r, h, p; name: DB Editor HAS_PERMISSION +--------------------------------------------------------------------------------------------------------------+ I r I h I P +--------------------------------------------------------------------------------------------------------------+ I (:Role {name: \"DB Editor\", resource: \"Cloud SQL\"}) I [:HAS_PERMISSION] I (:Permission {name: \"jobs.create\"}) I (:Role {name: \"DB Editor\", resource: \"Cloud SQL\"}) I [:HAS_PERMISSION] I (:Permission {name: \"jobs.get\"}) I ( :Role {name: \"DB Editor\", resource: \"Cloud SQL\"}) I [ :HAS_PERMISSION] I ( :Permission {name: \"jobs.list\"}) +--------------------------------------------------------------------------------------------------------------+ Visualizing the Graph C D CD localhost:7474/browser/ neo4j$ ,.rL MATCH. ( n) RETURN. n LIMIT. 25 Parmission(6) El ■1113;\\•i'i@i HAS PERMISSION(. 6) Table Code Permission <id>: 6 name: jobs.create c!J * e .. , • f; □ □ jobs.ere .. Counting Nodes and Relationships Querying the Graph 1 MATCH. ( p:Person)-[r*]->(m:Permission) 2 WHERE. p. name = 11Ethan11 3 RETURN. r, m.name 4 ORDER BY. m; +------------------------------------------------------------------+ I r I m.name +------------------------------------------------------------------+ [ [ :IN_GROUP], [ :HAS_ROLE], [ :HAS_PERMISSION] l \"jobs.list\" [ [ :IN_GROUP], [ :HAS_ROLE], [ :HAS_PERMISSION] l \"jobs.get\" [ [:IN_GROUPJ , [:HAS_ROLEJ , [:HAS_PERMISSION]] \"jobs.create\" [ [ :HAS_ROLE], [ :HAS_PERMISSION]] \"storage.list\" [ [ :HAS_ROLE], [ :HAS_PERMISSION] l \"storage.create\" [ [ :HAS_ROLE], [ :HAS_PERMISSION]] \"storage.delete\" +------------------------------------------------------------------+ 6 MATCH. ( p:Person)-[r*]->(m:Permission) 7 WHERE. p.name = \"Ethan\" 8 WITH distinct m.name as distinct_perms 9 RETURN. distinct_perms 10 ORDER BY. distinct_perms; ,-.,\u0011\"R ,=o~, •4c•5 name: Data Engineer HAS_PERMISSION +------------------+ I distinct_perms +------------------+ \"jobs.create\" \"jobs.get\" \"jobs.list\" \"storage.create\" \"storage.delete\" \"storage.list\" +------------------+ Querying the Graph 12 MATCH. ( p:Person)-[r*l]->(m:Permission) 13 WHERE. p. name = 11 Ethan 11 14 RETURN. r, m.name 15 ORDER BY. m; +------------+ I r Im.name I +------------+ +------------+ MATCH WHERE. ( p:Person)-[r*l .. 2]->(m:Permission) p.name = \"Ethan\" RETURN. r, m.name ORDER BY. m; +-----------------------------------------------------+ I r I m.name +-----------------------------------------------------+ I [ [:HAS_ROLEJ, [:HAS_PERMISSION] ] I \"storage.list\" I [ [ :HAS_ROLE] , [ :HAS_PERMISSIONJ) I \"storage.create\" I I [ [ :HAS_ROLEJ, [ :HAS_PERMISSIONJ J I \"storage.delete\" I +-----------------------------------------------------+ \u0012···,\u0013R ,-.\u0012\"\" \u0003 \u0003-\u0004· name: Data Engineer HAS_PERMISSION Updating Nodes Adding node properties: Adding node labels: Updating Relationships Adding and updating relationship properties: \"Renaming\" a relationship type: Deleting Relationships and Nodes Drop the relationships connected to nodes labeled Person: Drop nodes labeled Person: Drop all the nodes and relationships in the current database:"
  },
  {
    "index": 6,
    "level": 1,
    "start_page": 315,
    "end_page": 346,
    "title": "11 Data Cube Technology",
    "content": "Slides from text book PPTs by Prof. Jiawei Han & other online sources Data Mining Data Cube Technology. Chapter S: Data Cube Technolog- data cell □ Data Cube Computation: Basic Concepts □ Data Cube Computation Methods time □ Processing Advanced Queries with Data Cube Technology □ Multid\"mensional Data Analysis in Cube Space □ Summary time,item time,item,location Data Cube: A Lattice of Cuboids all 0-D(apex) cuboid supplier 1-D cuboids ocation,supplier 2-D cuboids 3-D cuboids · em,location,supplier 4-D(base) cuboid time, item, location, supplierc time time,item,I Data Cube: A Lattice of Cuboids all ,supp tion supplier O-D(apex) cuboid D Base vs. aggregate cells Ancestor vs. descendant cells 1-D cuboids □ Parent vs. child cells location,supplier □ {*,*,*) time, item, location, supplier 2-D cuboids 3-D cuboids 4-D(base) cuboid □ {*, milk, *, *) □ {*, milk, Urbana, *) □ {*, milk, Chicago, *) □ {9/15, milk, Urbana, *) □ {9/15, milk, Urbana, Dairy_land) □ Fu II cube vs. iceberg cu be compute cube sales iceberg as select month, city, customer group, count(*) from sale,slnfo cube by month, city, customer group iceberg having count(*)>= min support <;=J _c_o_n_d_it_io_1n_ □ Compute only the cells whose measure satisfies the iceberg ,con1dition □ Only a small portion of cells may be \"above the water\" in a sparse cube □ Ex.: Show only those cells whose count is no less than 100 Whó Iceberg Cube? □ Advantages of computing iceberg cubes □ No need to save nor show those cells whose value is below the threshold (iceberg condition) □ Efficient methods may even avoid computing the un-needed, intermediate cells □ Avoid explosive growth □ Example: A cube with 100 dimensions □ Suppose it contains only 2 base cells: {(a1, a2, a3, •••• , a100), (a1, a2, b3, ••• , b100)} □ How many aggregate cells if \"having count >= 1\"? □ Answer: 2101 - □ (Why?!) □ What about the iceberg cells, (i,e., with condition: \"having count>= 2\")? □ Answer: 4 (Why?!) s Iceberg Cube Good Enough? Closed Cube & Cube Shell □ Let cube P have only 2 base cells: {(a1, a2, a3 ••• , a100):10, (a1, a2, b3, ••• , b100):10} How many cells will the iceberg cube contain if \"having count(*)E 10\"? □ Answer: 2101 - 4 (still too bigl) □ Cose cube: A cell c is closed if there exists no cell d, such that dis a descendant of c, and d has the same measure value as c -□rn Ex. P has only 3 closed cells: ------, : 20 2 a3 • • • , a100):10, (a1, a2, b3, • • • , b100):10} , I ,a, a, A closed cube is a cube consisting of only closed cells □ Cube She Cube She I: The cuboids involving a small# of dimensions, e.g., 3 Idea: Only compute cube shells, other dimension combinations can be computed on the fly tr--1\\ -----v Q: For (A11 A2, ••• A10), how many comb1nat1ons to compute? □ Da a Cube Computa ion: Basic Co I cepts □ Da a Cube Computa, ion Methods time □ Processing Adva · ced Que ies with Data Cu I e Technology □ Mult.dimensional Data Analysis in Cube Space □ Summary Roadm for Efficient Computation □ General computation heuristics {Agarwal et al.'96) □ Computing full/iceberg cubes: 3 methodologies Bottom-Up: Multi-Way array aggregation {Zhao, Deshpande & Naughton, SIGMOD'. 97) □ Top-down: D BUC {Beyer & Ramarkrishnan, SIGMOD'. 99) Integrating Top-Down and Bottom-Up: □ Star-cubing algorithm {Xin, Han, Li & Wah: VLDB'03) □ High-dimensional OLAP: A Shell-Fragment Approach {Li, et al. VLDB'04) D Computing alternative kinds of cubes: Partial cube, closed cube, approximate cube, ..... □ Sorting, hashing, and grouping operations are applied to the dimension attributes in order to reorder and cluster related tuples all □ Aggregates may be computed from previously computed aggregates, rather than from the base fact table □ Sma lest-child: computing a cuboid from the smallest, prod,da ·. 1 try previously computed cuboid country . ate, count\u0019 Cache resu ts: caching results of a cuboid from which other cuboids are computed to reduce disk I/Os prod, date, country □ Amortize-sea s: computing as many as possible cuboids at the same time to amortize disk reads Share-sorts: sharing sorting costs cross multiple cuboids when sort-based meth1od is used □ Share-parff ons: sharing the partitioning cost across multiple cuboids when hash-based algorithms are used S. Agarwal, R. Agrawal, P. M. Deshpande, A. Gupta, J. F. Naughton, R. Ramakrishnan, S. Sarawagi. On the computation of multidimensional aggregates. VLDB'96 □ Array based \"bottom up\" algorithm (from ABC to AB, ... ) □ Using multi-dimensional chunks □ Simultaneous aggregation on multiple dimensions □ Intermediate aggregate values are re-used for computing ancestor cuboids □ Cannot do Apriori pruning: No iceberg optimization □ Comments on the method All □ Efficient for computing the full cube for a small number of dimensions □ If there are a large number of dimensions, \"top-down\" computation and iceberg cube computation methods (e.g., BUC) should be used Cube Computation: Multi-Wa& Ana& Aggregation (MQLAP). □ Partition arrays into chunks {a small subcube which fits in memory). □ Compressed sparse array addressing: {chunk_id, offset) □ Compute aggregates in \"multiway\" by visiting cube cells in the order which minimizes the# of times to visit each cell, and reduces memory access and storage cost What is the best traversing order to do multi-way aggregation? ti-way Ar ay Aggregatican ta2• ='° □ How to minimizes the memory requirement and reduc_ I/Os? One chunk of BC plane Entire AB plane A:40,B:400,C:4000 □ Keep the smallest plane in main One column of AC plane memory, fetch and compute only one chunk at a time for the largest plane □ The planes should be sorted and computed according to their size in ascending order □ Same methodology for computing 2-D and 1-D planes All 'X' * * * * 'X' Cube Computatio : Computing in Reverse Orde, □ BUC {Beyer & Ramakrishnan, SIGMOD'. 99) BUC: acronym of Bottom-Up {cube) Computation {Note: It is \"top-down\" in our view since we put Apex cuboid on the top!) □ Divides dimensions into partitions and facilitates iceberg pruning □ If a partition does not satisfy min_sup, its descendants can be pruned □ If minsup = 1 D compute full CUBE! □ No simultaneous aggregation 3AB 4ABC all ABD ABCD. 1 all e · B D 14 C 9 AD 11 BC 13 BD 15 CD 6ABD 8ACD 12BCD 5ABCD □ Usually, entire data set cannot fit in main memory □ Sort distinct values □ partition into blocks that fit □ Continue processing □ Optimizations Partitioning □ External Sorting, Hashing, Counting Sort Ordering dimensions to encourage pruning □ Cardinality, Skew, Correlation Collapsing duplicates □ Cannot do holistic aggregates anymore! Curse f Dimensionali y □ High-D OLAP:. Needed in many applications □ Science and engineering analysis Bio-data analysis: thousands of genes □ Statistical surveys: hundreds of variables 1.000 □ None of the previous cubing method can handle high dimensionalityl \u000f======t=--_-----1,____ _ _____L __ _____J___ _ __J ceberg cube and compressed cubes: only delay the inevitable explosion Full materialization: still significant overhead in accessing results on disk □ A shell-fragment approach: X. Li, J. Han, and H. Gonzalez, High-Dimensional OLAP: A Minimal Cubing Approach, VLDB104 n ; ,o A curse of dimensionality: A database of 600k t1uples. Each dimension has cardinality of 100 and zip/ of 2. Fast □ Observation: OLAP occurs only on a small subset of dimensions at a time □ Semi-On ine Computational odel Partition the set of dimensions into s ell fragments Compute data cubes for each shell fragment while retaining inverted ind·ces or value-list i d·ces □ Given the pre-computed f agment cubes, dynamically compute cube cells of the high-dimensional data cube online Major idea: Tradeoff between the amount of pre-computation and the speed of online computation Reducing computing high-dimensional cube into precomputing a set of lower dimensional cub,es Online re-construction of original high-dimensional space □ Lossless reduction □ Example: Let the cube aggregation function b·e counit tid □ Divide the 5-D table into 2 shell fragments: {A, B, C) and {D, E) □ Build traditional invert index or R D list Attribute TID llist L'ist Value s·ze □ Generalize the :1 D inverted indice.s to mu dimensional ones in1 th1e data cube sen.se lti □ Compute all cuboids for data cubes ABC a n1d ID1E wh1ile retaining the inverted indices □ Ex. shell fragment cube ABC contains 7 □ A, B, C; AB, AC, BC; ABC. cub 1o'id,s: □ This completes the offline computation Shelll-fragment AB Mlea.sure I ab 1le □ l·f measures other than1 ti·d count sum count a re present, store in 2 1---1----1----I ID_measure table separate 3 1--t----l----l from the she I I fragments 1---1----1----1 3,0 Cell al bl al 2 a2 1 a2 2 Attfbute· TIID List List Value Size 1 4 .5 1234.5 Intersection TID List List Size 1 23n 1 4 5 1 23n2.3 4Sn 1 4 5 4 5 .4 5 n 2 ,3 t1> □ Given a database of T tuples, D dimensions, and F shell fragment size, the fragment cubes' space requirement is: For F < 5, the growth is sub-linear □ Shell fragments do not have to be disjoint □ Fragment groupings can be arbitrary to allow for maximum online performance Known common combinations (e.g.,<city, state>) should be grouped together □ Shell fragment sizes can be adjusted for optimal balance between offline and online computation Cell al bl al b2 a2 bl a2 b2 Attribute TIO List List Vallue Size Intersection TID List List Size 123nl45 123n23 45n145 45 n 2 3 D'imensi'ons . •·• \\..._ ___ '\\...._ ___ Y-- ---- ABC Cube EF Cuboid DE Cuboid Cell Tuple-ID List dl ,el {1, 3, 8, 9} d •e2 {2, 4, 6, 7} d2 •el {5, 10} i i - DEF Cube A B C D E F /. ' Instantiated Base Table G H I J K L. M Online Cube ■■I■ On ine Oue,ô Computation with S e •Fragments A query has the general form: <a1, a2, ••• , an: M> -ach ai has 3 possible values (e.g., <3, ?, ?, *, 1: count> returns a 2-D data cube) ln1stan1tiated value Aggregate* function □ Inquire ? Function Method: Given the fragment cubes, process a query as follows Divide the query into fragment, same as the shell-fragment Fetch the corresponding T D list for each fragment from the fragment cube □ Intersect the TID lists from each fragment to construct· sta tiated ase table Compute the data cube using the base table with any cubing algorithm bperimenl: Size s. Dimensicanalitv (SO an 100 carclinalil.) 000 ---- -- ------------,------------ - 0-C - 00-C - Ex er·ments on real-world data □ UCI Forest CoverType data set □ 54 dimensions, 581K tuples (1) Shell fragments of size 2 took 33 seconds and 325MB to compute (1) ,+-l 2 0 o---------_. _________ _. 3-D subquery with 1 instantiate D: 85ms\"'l.4 sec. □ Longitudinal Study of Vocational Rehab. - 0 Data: 24 dimensions, 8818 tuples Shell fragments of size 3 took 0.9 seconds and 60MB to compute {50-C): 106 tuples, 0 skew, 50 cardinality, fragment size 3 {100-C): 106 tuples, 2 skew, 100 cardinality, fragment size 2 □ 5-D query with O instantiated D: 227ms\"'2.6 sec. Chapter : Data Cube 1echna oa, □ Data Cube Computation: Basic Concepts □ Data Cube Computation Methods □ Processing Advanced Q eries with Data Cube Tech no ogy □ M ltidimensional Data Ana ysis in Cube Space □ Summary time • • ta Cube □ Data Cube Computation: Basic Concepts □ Data Cube Computation Methods echncalcag. time □ Processing Advanced Q. eries with Data Cube Techno ogy □ M tidimensiona Data Analysis ·n Cube Space □ Summary Data Mining in Cube S ace □ Data cube greatly increases the analysis bandwidth □ Four ways to interact OLAP-styled analysis and data mining □ Using cube space to define data space for mining □ Using OLAP queries to generate features and targets for mining, e.g., multi-feature cube □ Using data-mining models as building blocks in a multi-step mining process, e.g., prediction cube □ Using data-cube computation techniques to speed up repeated model construction □ Cube-space data mining may require building a model for each candidate data space □ Sharing computation across model-construction for different candidates may lead to efficient mining lex Aggregation at Multiple Granularities: Mu ti-Feature Cubes □ Multi-feature cubes (Ross, et al. 1998): Compute complex queries involving multiple dependent aggregates at multiple granularities □ Ex. Grouping by all subsets of {item, region, month}, find the maximum price in 2010 for each group, and the total sales among all maximum price tuples select item, region, month, max(price), sum(R.sales) from purchases where year = 2010 cube by item, region, month: R such that R.price = max(price) □ Continuing the last example, among the max price tuples, find the min and max shelf live, and find the fraction of the total sales due to tuple that have min shelf life within the set of all max price tuples õ iscoue,ö•Driuen Exp o,ation of ·÷ ata Cubes □ Discovery-driven exploration of huge cube space {Sarawagi, et al.'98) Effective navigation of large OLA data cubes □ pre-compute measures indicating exceptions, guide user in the data analysis, at all levels of aggregation Exception: significantly different from the value anticipated, based on a statistical model □ Visual cues such as background color are used to reflect the degree of exception of each cell □ Kinds of exceptions □ SelfExp: surprise of cell relative to other cells at same level of aggregation □ lnExp: surprise beneath the cell □ Path Exp: surprise beneath cell for each drill-down path □ Computation of exception indicator can be overlapped with cube construction Exceptions can be stored, indexed and retrieved like precomputed aggregates Exa les: Disca i cem rn o n :1 Jan 1Feb MfaC' ApC' Tota] 1 % - 1 % mie n 1'1 Ja- Feb \"Mac- ApL p r-1 ntec:- - % [\" pt' p 1 J 1 % HP 1c o p i n - [' 10% b o rne c o1mp1uter- 1 % -2% 1l ap1to1p. ic o mputec 1%, 0 lb 11b.a IL .b ]I I Lo 1te -- l, Er-g10 -wa S 10 L, rn 1c: 1 p 0 mou:s,e m ou se rnp 15 Ir 1 % 1 % 1Q% 3,% -2'% p c - 1 - 1 - 1 % lvf .a-- Ju n 1 % 3'% - 1 a-c- Ju n - u ] Au - - 1 3 % - 1 % 1Q% - 1 % -7 % 3 % 3'% - 1 0% , %, - 1 I 1 % - 1 I % 0,% -2% -2% I % 6'% - 1 1 % s --p O ct - 1 % S,ep O ct 4 ' 1 % -5 % 1 % - 1 3'% - 1 % 2'% 1 % Dec 1 % - 1 % 3 % 7 - - 1 % □ Data Cube Computation: Basic Concepts □ Data Cube Computation Methods time □ Processing Advanced Queries with Data Cube Technology □ Mu tidimensional Data Analysis in Cube Space □ Summary -/ ata Cube Techno agy: Summary □ Data Cube Computation: Prel im ·nary Concepts Data Cube Computation Methods M ultiWay Array Aggregation BUC High-Dimensional OLAP with Shel l- ragments Multidimensional Data Analysis in Cube Space M ulti-feature Cubes Discovery-Driven Exploration of Data Cubes"
  }
]