[
  {
    "index": 0,
    "title": "L01 Introduction to Neural Networks",
    "content": "Neural networks are fascinating systems inspired by the way our brains work. At their core, they are networks made up of many interconnected units called neurons, much like the nerve cells in our own nervous system. The human brain, for example, contains about a trillion neurons, each connected to thousands of others, creating an incredibly complex web that allows us to think, learn, and perceive the world around us.\n\nTo understand neural networks, it helps to start with the biological neuron. A neuron is a specialized cell that receives electrical signals from other neurons through structures called dendrites. These signals cross tiny gaps known as synapses before reaching the neuron's cell body. Each synapse has a certain strength, or weight, which determines how much of the incoming signal actually influences the neuron. The neuron sums all these weighted inputs, and if the total signal is strong enough to reach a certain threshold, the neuron fires an electrical pulse called an action potential. This pulse then travels down the axon to communicate with other neurons. Importantly, synapses can be excitatory, encouraging the neuron to fire, or inhibitory, discouraging it. In models, excitatory synapses are represented by positive weights, while inhibitory ones have negative weights.\n\nArtificial neural networks borrow this idea but simplify it into mathematical models. Instead of biological cells, we have artificial neurons that take multiple inputs, multiply each by a weight, sum them up, and then apply an activation function to decide the output. There are different types of artificial neurons. The simplest is the binary threshold neuron, which outputs a 1 if the weighted sum exceeds a threshold and 0 otherwise. Another common type is the rectified linear neuron, which outputs zero for negative inputs and the input itself if positive. Then there’s the sigmoid neuron, which uses a smooth, S-shaped function to produce outputs between 0 and 1. This smoothness is helpful because it allows the network to learn more effectively.\n\nWhen we connect many of these artificial neurons together, we get a neural network. Typically, these networks have layers: an input layer that receives data, one or more hidden layers that process the data, and an output layer that produces the final result. For example, a simple feed-forward network might take an image as input, process it through hidden layers, and output a prediction about what the image contains.\n\nYou might wonder why we study neural networks. One reason is to better understand how the brain might work, given its complexity. But more practically, artificial neural networks have proven to be powerful tools for solving real-world problems. They excel at tasks like recognizing objects in images, reading handwritten digits, understanding speech, and more generally, classifying and predicting data.\n\nTo make this more concrete, imagine a simple 3x3 grid that can display patterns, including letters like T and L. Each cell in the grid can be either shaded or not, represented by a 1 or 0. We can design a small neural network that takes these nine inputs and learns to recognize whether the pattern forms a T or an L. The network uses weighted connections and thresholds to decide which letter is shown.\n\nOne of the earliest and simplest types of neural networks is the perceptron. It’s a binary classifier that tries to separate data into two classes by finding a line (or hyperplane) that divides them. The perceptron learns by adjusting its weights based on misclassified examples: if it wrongly classifies a positive example as negative, it adjusts the weights to be more like that example, and vice versa. This process repeats until the perceptron correctly classifies all examples, assuming the data is linearly separable. However, if the data cannot be separated by a straight line, the perceptron won’t converge. To handle such cases, the pocket algorithm keeps track of the best solution found so far, even if it’s not perfect, improving the perceptron’s performance on more complex data.\n\nNeural networks have shown remarkable success in machine learning challenges. For instance, in image recognition tasks like the ImageNet competition, deep neural networks have drastically reduced error rates, outperforming earlier methods by a significant margin. Similarly, in speech recognition, deep networks have replaced older models, improving accuracy and making voice-controlled systems more reliable.\n\nTraining these networks involves adjusting the weights to minimize the difference between the network’s predictions and the actual desired outputs. This difference is measured by an error function, often the squared difference between predicted and actual values. Because neural networks can be complex and nonlinear, it’s not practical to solve for the best weights analytically. Instead, we use numerical methods like gradient descent, which iteratively tweaks the weights in the direction that reduces the error the most.\n\nTo perform gradient descent, we need to know how the error changes with respect to each weight, which means calculating partial derivatives. This is where back propagation comes in. Back propagation is an efficient algorithm that applies the chain rule of calculus to propagate the error backward through the network, layer by layer, computing gradients for all weights. These gradients tell us how to adjust each weight to reduce the error.\n\nBack propagation can be thought of as a modular process: first, compute the error at the output, then calculate how this error changes with the output, and finally propagate these changes backward through the network’s layers. This approach allows us to train networks with multiple hidden layers, enabling them to learn complex patterns without manual feature engineering.\n\nIn summary, neural networks are powerful tools inspired by the brain’s structure, capable of learning from data to solve a wide range of problems. Starting from simple models like the perceptron to deep networks trained with back propagation and gradient descent, they have transformed fields like computer vision and speech recognition. As you explore further, you’ll find that understanding these foundational concepts opens the door to many exciting applications and advances in artificial intelligence."
  },
  {
    "index": 1,
    "title": "L02 Loss Functions Regularization",
    "content": "When we talk about training machine learning models, especially for classification tasks, one of the most important ideas to understand is the concept of a loss function. Think of a loss function as a way to measure how far off our model’s predictions are from the actual answers we want. The whole goal of training is to adjust the model so that this loss gets as small as possible, meaning the model’s predictions get closer and closer to the truth.\n\nLet’s start with a simple case: binary classification. This is when we have just two classes, often labeled 0 and 1. For each example in our data, there’s a true label, which we call the target, and it’s either 0 or 1. Our model tries to predict the probability that the example belongs to class 1. So, instead of just saying “this is class 1” or “this is class 0,” the model outputs a number between 0 and 1, which we interpret as the chance that the example is in class 1. This is where the sigmoid function comes in. The sigmoid squashes any number into a value between 0 and 1, making it perfect for representing probabilities.\n\nNow, how do we measure how good or bad these probability predictions are? One straightforward way is to use something called squared error, which is just the square of the difference between the predicted probability and the actual label. It’s simple and intuitive—if the prediction is close to the true label, the error is small; if it’s far, the error is big. But in practice, squared error doesn’t work very well for classification. It treats the problem more like a regression task, and it’s sensitive to outliers, which can throw off the training.\n\nA much better choice is something called cross entropy loss. Cross entropy comes from information theory and measures the difference between two probability distributions. In our case, one distribution is the true label (which is either 0 or 1), and the other is the predicted probability from the model. Cross entropy is low when these two distributions are similar and high when they are very different. What’s neat about cross entropy is that it penalizes wrong predictions more heavily when the model is confident but wrong. For example, if the true label is 1 but the model predicts a probability close to 0, the loss will be very large, pushing the model to correct itself strongly. This makes cross entropy a natural and effective loss function for classification problems.\n\nWhen we move beyond two classes to multi-class classification, things get a bit more complex. Instead of just predicting the probability of one class, the model now has to assign probabilities to multiple classes, and these probabilities must add up to 1. To do this, we use a function called softmax. Softmax takes the raw scores the model produces for each class and converts them into probabilities that sum to one. Then, we use cross entropy loss again, but this time it compares the true class label (which is one-hot encoded, meaning it’s 1 for the correct class and 0 for the others) with the predicted probabilities from softmax. This combination of softmax and cross entropy is the standard approach for multi-class classification.\n\nNow, even with the right loss function, there’s another challenge: making sure our model doesn’t just memorize the training data but actually learns patterns that generalize well to new, unseen data. This is where regularization comes in. Regularization is a set of techniques designed to improve a model’s ability to generalize by preventing it from becoming too complex or overfitting the training data. Overfitting happens when a model learns not only the underlying patterns but also the noise and random quirks in the training data, which hurts its performance on new data.\n\nRegularization works by adding a penalty term to the loss function. Instead of just minimizing the error on the training data, we minimize the error plus this penalty, which depends on the model’s parameters, usually the weights. The penalty discourages the model from having very large or complicated weights, effectively pushing it to find simpler solutions that are less likely to overfit.\n\nThere are two common types of regularization: L2 and L1. L2 regularization adds a penalty proportional to the sum of the squares of the weights. This tends to shrink the weights smoothly, making them smaller but rarely zero. It’s especially useful when you have many features but not a lot of data, as it helps stabilize the training process. On the other hand, L1 regularization adds a penalty proportional to the sum of the absolute values of the weights. This has the interesting effect of pushing some weights exactly to zero, effectively removing those features from the model. This leads to a sparse model, which can be easier to interpret and sometimes more efficient.\n\nTo visualize this, imagine fitting a curve to some data points. Without regularization, the model might fit a very wiggly curve that passes through every point perfectly but behaves wildly between points—this is overfitting. With regularization, the model is encouraged to find a smoother curve that captures the general trend without fitting the noise. This balance is crucial for building models that perform well not just on the data they’ve seen but also on new data.\n\nIn summary, choosing the right loss function and applying regularization are key steps in building effective classification models. Cross entropy loss combined with sigmoid or softmax activations provides a solid foundation for binary and multi-class classification, respectively. Regularization techniques like L1 and L2 help ensure that the model generalizes well by controlling complexity and preventing overfitting. Understanding these concepts will give you a strong base for developing models that are both accurate and reliable in real-world applications."
  },
  {
    "index": 2,
    "title": "L03 Convolutional Neural Networks",
    "content": "Today, we’re going to explore one of the most exciting and powerful tools in modern machine learning: Convolutional Neural Networks, or CNNs. These networks have revolutionized how computers understand images, enabling everything from facial recognition on your phone to self-driving cars identifying pedestrians. But what exactly makes CNNs so special, and how do they work? Let’s break it down step by step.\n\nImagine you have a picture, like a simple black-and-white image made up of tiny squares called pixels. Each pixel has a value representing how light or dark it is. Now, if you wanted a computer to recognize patterns in this image—say, edges or shapes—you could try to look at every pixel individually, but that quickly becomes overwhelming, especially for large images. This is where the idea of a convolutional layer comes in.\n\nA convolutional layer uses something called a filter, which you can think of as a small window or patch that slides over the image. This filter is a small grid of numbers, or weights, that multiply with the pixel values underneath it. At each position, the filter calculates a weighted sum of the pixels it covers, producing a single number. When you slide this filter across the entire image, you end up with a new image called a feature map. This feature map highlights where certain patterns or features appear in the original image, like edges or textures.\n\nOne important detail is that images often have multiple channels—like red, green, and blue in color images—so instead of a flat 2D grid, the image is more like a 3D block. The filter matches this depth, meaning it looks at all color channels at once, combining information across them. Also, instead of using just one filter, CNNs use many filters simultaneously, each designed to detect different features. This results in multiple feature maps stacked together, giving the network a rich understanding of the image’s content.\n\nAfter the convolutional layer extracts these features, the network uses a pooling layer to simplify the information. Pooling reduces the size of the feature maps by summarizing small regions into single values. The most common method is max pooling, where the highest value in each small region is kept, and the rest are discarded. This step helps the network focus on the most important features and makes it more robust to small changes or shifts in the image, like if an object moves slightly.\n\nBy stacking many convolutional and pooling layers, the network builds a hierarchy of features. Early layers might detect simple edges or colors, while deeper layers combine these into more complex shapes or even entire objects. At the end of this process, the network usually has one or more fully connected layers, similar to traditional neural networks, which take all the extracted features and make a final decision, like classifying the image as a cat or a dog.\n\nYou might wonder why we don’t just use fully connected networks for images. The problem is that fully connected layers require an enormous number of weights when dealing with images, making them inefficient and prone to overfitting. CNNs solve this by sharing the same filter weights across the entire image, drastically reducing the number of parameters. This weight sharing not only makes CNNs more efficient but also allows them to learn features that are useful regardless of where they appear in the image.\n\nTraining a CNN involves adjusting the filter weights so the network gets better at recognizing patterns. This is done through backpropagation, where the network calculates how much each weight contributed to the error in its prediction and updates them accordingly. For convolutional layers, this means carefully computing gradients that tell the network how to tweak each filter. For pooling layers, especially max pooling, the gradient flows back only through the pixels that had the maximum value during the forward pass.\n\nOver the years, researchers have developed many CNN architectures that push the boundaries of what these networks can do. For example, ResNet introduced a way to train very deep networks with over a hundred layers by adding shortcut connections, helping the network learn better and faster. GoogleNet used a clever design called the Inception module to process multiple filter sizes in parallel, improving efficiency. VGG showed that stacking many small filters can also lead to excellent performance. These architectures have won major competitions and set the standard for image recognition tasks.\n\nIn summary, CNNs are powerful because they mimic how we might look at images—focusing on local patterns, combining them into bigger features, and learning which features matter most. Their clever use of filters and pooling layers makes them efficient and effective, especially for images. Understanding how convolution, pooling, and backpropagation work together gives you a solid foundation to explore more advanced topics in deep learning and computer vision. If you’re curious, there are plenty of resources and tutorials that can help you dive deeper and even start building your own CNN models."
  }
]